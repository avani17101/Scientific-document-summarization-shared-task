{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 10:29:36.346336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:/usr/local/apps/cuDNN/7.6.5-cuda-10.2/lib64\n",
      "2021-12-08 10:29:36.346397: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import ast\n",
    "import scipy\n",
    "# from argparse import ArgumentParser\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument(\"-multi_lb_classi\", default=1, type=int, help=\"whether train a multi-lable classifier or individual classifiers for each class\")\n",
    "# opt = parser.parse_args()\n",
    "multi_lb_classi = 0\n",
    "\n",
    "\n",
    "def preprocess(example_sent):\n",
    "    global stop_words\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975049bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_2018(files, folder):\n",
    "    \n",
    "    pairs_lis = []\n",
    "    prev_corpus = []\n",
    "    docs_wise = {}\n",
    "    \n",
    "    for z,f in enumerate(files):\n",
    "        cit_text_lis = []\n",
    "        ref_text_lis = []\n",
    "        cit_off_lis = []\n",
    "        ref_off_lis = []\n",
    "        new_corpus = []\n",
    "\n",
    "        try:\n",
    "            a = folder+f+\"/Reference_XML/\"+f+\".xml\"\n",
    "            tree = ET.parse(a)\n",
    "            root = tree.getroot()\n",
    "            final =[]\n",
    "            total = len(root)\n",
    "            for a in root:\n",
    "                for b in a:\n",
    "                    final.append(b.text)\n",
    "            d={'col1':final}\n",
    "            rp = pd.DataFrame(data=d)\n",
    "            corpus = rp.col1\n",
    "            new_corpus = corpus.apply(lambda x: preprocess(x))\n",
    "\n",
    "            data = None\n",
    "            ann = None\n",
    "            a_folder = folder+f+\"/annotation/\"\n",
    "            file = os.listdir(a_folder)[0]\n",
    "\n",
    "            ann = a_folder+file\n",
    "            with open(ann,\"r\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "                cit_text = re.findall(\"Citation Text:\\s+([^|]*)\", data)\n",
    "                pattern = r'\\<.*?\\>'\n",
    "                pattern2 = r'\\(.*?\\)'\n",
    "                for c in cit_text:\n",
    "                    c = re.sub(pattern2,'',re.sub(pattern, '', c))\n",
    "                    c = preprocess(c)\n",
    "                    cit_text_lis.append(c)\n",
    "\n",
    "\n",
    "                ref_text = re.findall(\"Reference Text:\\s+([^|]*)\", data)\n",
    "                pattern = r'\\<.*?\\>'\n",
    "                pattern2 = r'\\(.*?\\)'\n",
    "                for ref in ref_text:\n",
    "                    ref = re.sub(pattern2,'',re.sub(pattern, '', ref))\n",
    "                    ref = preprocess(ref)\n",
    "                    ref_text_lis.append(ref)\n",
    "                    \n",
    "\n",
    "    #                 cit_off = re.findall(\"Citation Offset:\\s+([^|]*)\", data)\n",
    "    #                 for c in cit_off:\n",
    "    #                     c = ast.literal_eval(c)\n",
    "    #                     cit_off_lis.append(c)\n",
    "\n",
    "\n",
    "                ref_off = re.findall(\"Reference Offset:\\s+([^|]*)\", data)\n",
    "                for r in ref_off:\n",
    "                    r = ast.literal_eval(r)\n",
    "                    ref_off_lis.append(r)\n",
    "\n",
    "            for i in range(len(cit_text_lis)):\n",
    "                for j in ref_off_lis[i]:\n",
    "                    if int(j) < len(new_corpus):\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[int(j)]],label=1.0)) #positive pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3)) #negative pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "                        pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "                        pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "\n",
    "            if (z!=0 and len(new_corpus)!=0 and len(prev_corpus)!=0):\n",
    "                pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "                pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "                pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "                pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "                \n",
    "            docs_wise[f] = {'corpus':new_corpus.values,  'cite_text':cit_text_lis, 'ref_off':ref_off_lis}\n",
    "            prev_corpus = new_corpus\n",
    "        except Exception as e: \n",
    "            print(f,e)\n",
    "#             exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "#             fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "#             print(exc_type, fname, exc_tb.tb_lineno)\n",
    "#             prev_corpus = []\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    return pairs_lis, docs_wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a47334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = os.listdir(\"./scisumm-2018/Training\")\n",
    "# random.shuffle(docs)\n",
    "train_e = int(0.7*len(docs))\n",
    "val_e = int(0.1*len(docs)) + train_e\n",
    "train_rps = docs[:train_e]\n",
    "val_rps = docs[train_e:val_e]\n",
    "test_rps = docs[val_e:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41bc4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J98-2005 not well-formed (invalid token): line 36, column 116\n",
      "P98-1081 not well-formed (invalid token): line 147, column 30\n",
      "E03-1020 not well-formed (invalid token): line 34, column 62\n",
      "H89-2014 not well-formed (invalid token): line 82, column 106\n",
      "X96-1048 not well-formed (invalid token): line 14, column 117\n",
      "N01-1011 not well-formed (invalid token): line 13, column 100\n",
      "J00-3003 not well-formed (invalid token): line 12, column 25\n",
      "H05-1115 not well-formed (invalid token): line 6, column 109\n",
      "C94-2154 not well-formed (invalid token): line 10, column 202\n",
      "N09-1001 invalid syntax (<unknown>, line 1)\n",
      "W06-3909 invalid syntax (<unknown>, line 1)\n"
     ]
    }
   ],
   "source": [
    "# test_docs  = os.listdir(\"./scisumm-2018/Test\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_data, train_docs = get_dataset_2018(train_rps,\"scisumm-2018/Training/\")\n",
    "val_data, val_docs = get_dataset_2018(val_rps,\"scisumm-2018/Training/\")\n",
    "test_data, test_docs = get_dataset_2018(test_rps,\"scisumm-2018/Training/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit test passed\n",
      "unit test passed\n",
      "unit test passed\n"
     ]
    }
   ],
   "source": [
    "def test_if_valid_data(docs):\n",
    "    for k in docs:\n",
    "        try:\n",
    "            assert(len(docs[k]['ref_off']) == len(docs[k]['cite_text']))\n",
    "        except:\n",
    "            print(\"test failed: length of queries and gt not equal\")\n",
    "            return\n",
    "    print(\"unit test passed\")\n",
    "test_if_valid_data(train_docs)\n",
    "test_if_valid_data(val_docs)\n",
    "test_if_valid_data(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eec5bd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3912, 746, 1056, 22, 3, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data),len(test_data), len(train_docs), len(val_docs),len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b03fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_data, shuffle=True, batch_size=1)\n",
    "test_dataloader = DataLoader(test_data, shuffle=True, batch_size=1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e558f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0510ca7c46394167935b3115f500c20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721b2c5594854d278660754bed8123dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b193873f49f455d8140f928cfcccc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86221d2c0a4d43c091c46b0723446dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ffc7d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model,'bert2018_fine_tuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "58b10468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('bert2018_fine_tuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0a957f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84eed00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ignite.metrics import Rouge  #https://pypi.org/project/pytorch-ignite/\n",
    "from rouge_score import rouge_scorer  #https://pypi.org/project/rouge-score/\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "from pythonrouge.pythonrouge import Pythonrouge\n",
    "\n",
    "\n",
    "def get_matching_sentences(model,corpus,queries,gt, thresh=0.6,topk=3,cs_based=True):\n",
    "    #f1 score will be same as precision and recall in our case: since documents\n",
    "    \n",
    "    # Get a vector for each headline (sentence) in the corpus\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "    corpus_embeddings = corpus_embeddings/np.linalg.norm(corpus_embeddings,axis=0).reshape(-1)\n",
    "    # Define search queries and embed them to vectors as well\n",
    "\n",
    "    query_embeddings = model.encode(queries)\n",
    "    # For each search term return 3 closest sentences\n",
    "    total_nums_correct = 0\n",
    "    total_retrieved = 0\n",
    "    total_relevent = 0\n",
    "    rouge1 = []\n",
    "    rouge2 = []\n",
    "    rouge_su4 = []\n",
    "    for i in range(len(queries)):\n",
    "        query, query_embedding  = queries[i], query_embeddings[i]\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "        \n",
    "        distances = np.array(distances)\n",
    "        distances = 1 - distances\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: -x[1])\n",
    "#         print(results)\n",
    "        retrieved = []\n",
    "        if cs_based:\n",
    "            for k,dist in results:\n",
    "                if dist >= thresh:\n",
    "                    retrieved.append(k)\n",
    "        else: #retrieve topk most matching sentences\n",
    "            indexes = results[0:topk]\n",
    "            retrieved = []\n",
    "            for l,k in indexes:\n",
    "                retrieved.append(l)\n",
    "    \n",
    "        nums_correct = len(np.intersect1d(retrieved,gt[i]))\n",
    "        total_nums_correct += nums_correct\n",
    "        total_relevent += len(gt[i])\n",
    "        total_retrieved += len(retrieved)\n",
    "        for idx in retrieved:\n",
    "            scores = scorer.score(corpus[idx].strip(), query)\n",
    "            rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=[[corpus[idx].strip()]], reference=[[[query]]],\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=True, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "    \n",
    "            score = rouge.calc_score()\n",
    "            rouge1.append(score['ROUGE-1'])\n",
    "            rouge2.append(score['ROUGE-2'])\n",
    "            rouge_su4.append(score['ROUGE-SU4'])\n",
    "\n",
    "            \n",
    "    return total_nums_correct, total_relevent,total_retrieved, np.mean(rouge1), np.mean(rouge2), np.mean(rouge_su4)\n",
    "    \n",
    "                    \n",
    "#             print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e61cc555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P06-2124',\n",
       " 'E09-2008',\n",
       " 'D10-1083',\n",
       " 'W04-0213',\n",
       " 'C10-1045',\n",
       " 'C90-2039',\n",
       " 'P00-1025',\n",
       " 'D09-1023',\n",
       " 'C98-1097',\n",
       " 'W95-0104',\n",
       " 'N04-1038',\n",
       " 'N09-1025',\n",
       " 'P07-1040',\n",
       " 'W11-0815',\n",
       " 'D10-1058',\n",
       " 'C02-1025',\n",
       " 'W03-0410',\n",
       " 'W09-0621',\n",
       " 'P98-1046',\n",
       " 'P05-1053',\n",
       " 'N06-2049',\n",
       " 'W08-2222']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_keys_train = list(train_docs.keys())\n",
    "docs_keys_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c5cd9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C08-1098', 'P05-1004', 'J96-3004', 'C00-2123'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f8e36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n",
      "13 13\n",
      "64 64\n",
      "18 18\n"
     ]
    }
   ],
   "source": [
    "for k in test_docs:\n",
    "    print(len(test_docs[k]['ref_off']), len(test_docs[k]['cite_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f61e0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']= '0'\n",
    "def evaluate(model, docs, thresh=0.6,topk=3,cs_based=True):\n",
    "    rouge1_lis = []\n",
    "    rouge2_lis = []\n",
    "    rouge_su4_lis = []\n",
    "    tp_big = 0\n",
    "    tot_relevent = 0\n",
    "    tot_retrieved = 0\n",
    "\n",
    "    for k in docs:\n",
    "        tp, rele,ret, rouge1, rouge2, rouge_su4 = get_matching_sentences(model,docs[k]['corpus'], docs[k]['cite_text'], docs[k]['ref_off'],thresh,topk,cs_based)\n",
    "        tp_big += tp\n",
    "        tot_relevent += rele\n",
    "        tot_retrieved += ret\n",
    "        rouge1_lis.append(rouge1)\n",
    "        rouge2_lis.append(rouge2)\n",
    "        rouge_su4_lis.append(rouge_su4)\n",
    "    recall = tp_big/tot_relevent\n",
    "    precision = tp_big/tot_retrieved\n",
    "    f1 = 2*recall*precision/(recall+precision+1e-10)\n",
    "    return recall,precision,f1, np.mean(rouge1_lis), np.mean(rouge2_lis), np.mean(rouge_su4_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3069ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in train: recall 0.3609375, precision 0.1763358778625954, f1-score 0.23692307687897962, rouge1 0.20644336908995095, rouge2 0.05007303001994657, rouge_su4 0.0692603719023074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,train_docs,thresh=0.65)   \n",
    "print(\"metrics obtained in train: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce53e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: recall 0.005747126436781609, precision 0.004424778761061947, f1-score 0.004999999950845001, rouge1 0.20585767107793065, rouge2 0.047522497145947244, rouge_su4 0.05414208794237818\n"
     ]
    }
   ],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.65)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1d8c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: recall 0.017241379310344827, precision 0.005119453924914676, f1-score 0.007894736806799169, rouge1 0.1754443025850619, rouge2 0.03489439339590088, rouge_su4 0.04993661360795343\n"
     ]
    }
   ],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.6)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cb10c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: recall 0.040229885057471264, precision 0.004989308624376337, f1-score 0.008877615706429738, rouge1 0.14744871211600644, rouge2 0.0235749694857687, rouge_su4 0.04140865457807666\n"
     ]
    }
   ],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.55)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5ed7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: recall 0.034482758620689655, precision 0.016260162601626018, f1-score 0.02209944747026037, rouge1 0.15032574190132786, rouge2 0.02408980974320818, rouge_su4 0.04102838252314815\n"
     ]
    }
   ],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,cs_based=False)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
