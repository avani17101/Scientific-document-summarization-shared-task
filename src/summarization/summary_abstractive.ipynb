{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## Scientific document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input 1: abstract\n",
    "* input 2: citance text\n",
    "* target: gold standard summary\n",
    "* bad data in standard corpus, no abstract summary: C92-1025, H94-1048, etc. Therefore take abstract from paper directly\n",
    "* asbtract direct copy, rest rank order from citing text till 250 words.\n",
    "* extractive summarization, can be seen from examples given.\n",
    "* use abstractive summary approaches like PEGASUS\n",
    "* dataset downloaded from https://cs.stanford.edu/~myasu/projects/scisumm_net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/W02-1039', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P96-1021', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/N06-1020', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C86-1016', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C92-2070', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/D08-1082', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/J01-2002', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C02-1114', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P85-1018']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "ds_folder = glob.glob(\"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/*\")\n",
    "print(ds_folder[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P83-1020\n"
     ]
    }
   ],
   "source": [
    "ds_paper_num = ds_folder[0][-8:]\n",
    "print(ds_paper_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/Documents_xml/P83-1020.xml\n"
     ]
    }
   ],
   "source": [
    "ds_paper_xml = ds_folder[0] + '/Documents_xml/' + ds_paper_num + '.xml'\n",
    "print(ds_paper_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Theory: Talking About Talking About Trees\n",
      "\n",
      "Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call theory theory While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(ds_paper_xml)\n",
    "root = tree.getroot()\n",
    "title = root[0].text\n",
    "abstract = ' '.join([a.text for a in root[1]])\n",
    "print(title)\n",
    "print()\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt\n"
     ]
    }
   ],
   "source": [
    "ds_summary_txt = ds_folder[0] + '/summary/' + ds_paper_num + '.gold.txt'\n",
    "print(ds_summary_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Theory: Talking About Talking About Trees\n",
      "Linguists, including computational linguists, have always been fond of talking about trees.\n",
      "In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory).\n",
      "While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural language in a manner which is intrinsically computational.\n",
      "This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.\n",
      "Our D-theory model is powerful in that it allows the right-most daughter of a node to be lowered under a sibling node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(ds_summary_txt, 'r') as handle:\n",
    "    summary_gold = handle.read()\n",
    "print(summary_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_citance_json = ds_folder[0] + \"/citing_sentences_annotated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "citance_json = pd.read_json(ds_citance_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citance_No</th>\n",
       "      <th>citing_paper_id</th>\n",
       "      <th>citing_paper_authority</th>\n",
       "      <th>citing_paper_authors</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>keep_for_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>This&amp; quot; core parser&amp; quot; has been the su...</td>\n",
       "      <td>Description theory (henceforth, D-theory) (Mar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>This model is interesting in that it does not ...</td>\n",
       "      <td>This model is interesting in that it does not ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>The original D-theory model (Marcus et al (198...</td>\n",
       "      <td>The original D-theory model (Marcus et al (198...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>P10-1053</td>\n",
       "      <td>0</td>\n",
       "      <td>Sylvain, Schmitz</td>\n",
       "      <td>More on Dominance Links Dominance links are qu...</td>\n",
       "      <td>More on Dominance Links Dominance links are qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>E95-1017</td>\n",
       "      <td>3</td>\n",
       "      <td>David, Milward</td>\n",
       "      <td>S /npvp Mary/ V S th inks /npvp^ John M&amp; amp; ...</td>\n",
       "      <td>S /npvp Mary/ V S th inks /npvp^ John M&amp;C sugg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citance_No citing_paper_id  citing_paper_authority citing_paper_authors  \\\n",
       "0           1        E95-1043                       0       Patrick, Sturt   \n",
       "1           2        E95-1043                       0       Patrick, Sturt   \n",
       "2           3        E95-1043                       0       Patrick, Sturt   \n",
       "3           4        P10-1053                       0     Sylvain, Schmitz   \n",
       "4           5        E95-1017                       3       David, Milward   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  This& quot; core parser& quot; has been the su...   \n",
       "1  This model is interesting in that it does not ...   \n",
       "2  The original D-theory model (Marcus et al (198...   \n",
       "3  More on Dominance Links Dominance links are qu...   \n",
       "4  S /npvp Mary/ V S th inks /npvp^ John M& amp; ...   \n",
       "\n",
       "                                          clean_text  keep_for_gold  \n",
       "0  Description theory (henceforth, D-theory) (Mar...              0  \n",
       "1  This model is interesting in that it does not ...              0  \n",
       "2  The original D-theory model (Marcus et al (198...              1  \n",
       "3  More on Dominance Links Dominance links are qu...              0  \n",
       "4  S /npvp Mary/ V S th inks /npvp^ John M&C sugg...              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citance_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Description theory (henceforth, D-theory) (Marcus et al (1983)).', 'This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)).', 'The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node.', 'More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992).', 'S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).']\n"
     ]
    }
   ],
   "source": [
    "citance = citance_json['clean_text'].to_list()\n",
    "print(citance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Description theory (henceforth, D-theory) (Marcus et al (1983)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)). The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992). S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(citance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process(sent):\n",
    "    sent = sent.encode(\"ascii\", \"ignore\").decode()\n",
    "    sent = re.sub(r\"\\n\", \" \", sent)\n",
    "    sent = re.sub(r\"\\^\", \"\", sent)\n",
    "    sent = re.sub(r\"\\/\", \"\", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, root, tokenizer, max_len):\n",
    "        self.ds_folder = glob.glob(root + \"*\")\n",
    "        self.len = len(self.ds_folder)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        ds_paper_num = self.ds_folder[i][-8:]\n",
    "        ds_paper_xml = self.ds_folder[i] + '/Documents_xml/' + ds_paper_num + '.xml'\n",
    "        tree = ET.parse(ds_paper_xml)\n",
    "        root = tree.getroot()\n",
    "        title = root[0].text\n",
    "        abstract = ' '.join([a.text for a in root[1]])\n",
    "        \n",
    "        ds_citance_json = self.ds_folder[i] + \"/citing_sentences_annotated.json\"\n",
    "        citance_json = pd.read_json(ds_citance_json)\n",
    "        citance = citance_json['clean_text'].to_list()\n",
    "        citance = ' '.join(citance)\n",
    "        \n",
    "        ds_summary_txt = self.ds_folder[i] + '/summary/' + ds_paper_num + '.gold.txt'\n",
    "        with open(ds_summary_txt, 'r') as handle:\n",
    "            summary_gold = handle.read()\n",
    "            \n",
    "        try:\n",
    "            X = pre_process(title + '. ' + abstract + citance)\n",
    "        except:\n",
    "            X = pre_process(abstract + citance)\n",
    "        \n",
    "        y = pre_process(summary_gold)\n",
    "        \n",
    "        return X+' </s>', y+' </s>'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "root = \"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"A Polynomial-Time Algorithm For Statistical Machine Translation. Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3ROIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minuteusually substantially lesswith no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion transduction model. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996). Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation. To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar (Wu, 1996) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases. Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model. The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. To do bigram-integrated decoding, we need to augment each chart item (X,i, j) with two target-language boundary words u and v to produce a bigram-item like u  v Xi j, following the dynamic programming algorithm of Wu (1996). NP (1) VPP-VP (2), NP (1) VPP-VP (2) VPP-VP? VP (1) PP (2), PP (2) VP (1) In this case m-gram integrated decoding can bedone in O (|w|3+4 (m? 1)) time which is much lower order polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. See Wu (1996) or Melamed (2004) for a detailed exposition. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al, 1996) and (Wu, 1996). (Wu,1996) propose using contiguity restrictions on the reordering. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework ,including information extraction (Miller et al, 2000) and syntax-based machine translation (Wu, 1996). To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above. </s>\",\n",
       " 'A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. We test our algorithm on Chinese-English translation.  </s>')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Triage(root, None, None)\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2962\n"
     ]
    }
   ],
   "source": [
    "max_ = 0\n",
    "for i in data:\n",
    "    max_ = max(max_, len(i[1]))\n",
    "print(max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "document = []\n",
    "summary = []\n",
    "for d, s in data:\n",
    "    document.append(d)\n",
    "    summary.append(s)\n",
    "ds_dict = {\n",
    "    'document': document,\n",
    "    'summary': summary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_dict(ds_dict)\n",
    "raw_datasets = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 908\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': \"Transition-based Dependency Parsing with Rich Non-local Features. Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010).   Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. Second, at a similar tokssec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. Here we use the identical trainingvalidationevaluation splits and experimental set-up as Zhang and Nivre (2011). Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. For reference, Zhang and Nivre (2011) report 86.084.4, which is previously the best result reported on this data set. Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. </s>\",\n",
       " 'summary': 'Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transition-based parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available. We develop the feature template for the arc-eager model.  </s>'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each predictions\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_agregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine tune transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.5\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 2500\n",
    "max_target_length = 250\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c1c4dd482f48febc5965c09c1165ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334881f881db4296b0fba35f2c557a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'document', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 908\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'document', 'input_ids', 'labels', 'summary'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /ssd_scratch/cvit/aryamaan/\n",
    "!mkdir /ssd_scratch/cvit/aryamaan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model_name = '/ssd_scratch/cvit/aryamaan/' + model_checkpoint.split()[-1]\n",
    "# model_name = model_checkpoint.split()[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'input_ids', 'labels', 'summary'],\n",
       "    num_rows: 908\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running training *****\n",
      "  Num examples = 908\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14528\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14528' max='14528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14528/14528 32:29, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.538200</td>\n",
       "      <td>0.830664</td>\n",
       "      <td>15.441200</td>\n",
       "      <td>13.798700</td>\n",
       "      <td>15.322400</td>\n",
       "      <td>15.395800</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>0.784953</td>\n",
       "      <td>15.378100</td>\n",
       "      <td>13.752500</td>\n",
       "      <td>15.279800</td>\n",
       "      <td>15.319100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.757614</td>\n",
       "      <td>15.354600</td>\n",
       "      <td>13.703000</td>\n",
       "      <td>15.252300</td>\n",
       "      <td>15.289900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.708500</td>\n",
       "      <td>0.754053</td>\n",
       "      <td>15.419900</td>\n",
       "      <td>13.739000</td>\n",
       "      <td>15.330400</td>\n",
       "      <td>15.360200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.740059</td>\n",
       "      <td>15.419900</td>\n",
       "      <td>13.739000</td>\n",
       "      <td>15.330400</td>\n",
       "      <td>15.360200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.731700</td>\n",
       "      <td>0.732737</td>\n",
       "      <td>15.419900</td>\n",
       "      <td>13.739000</td>\n",
       "      <td>15.330400</td>\n",
       "      <td>15.360200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.723029</td>\n",
       "      <td>15.421500</td>\n",
       "      <td>13.739200</td>\n",
       "      <td>15.331300</td>\n",
       "      <td>15.360700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.658600</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>15.421500</td>\n",
       "      <td>13.739200</td>\n",
       "      <td>15.331300</td>\n",
       "      <td>15.360700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.721198</td>\n",
       "      <td>15.421500</td>\n",
       "      <td>13.739200</td>\n",
       "      <td>15.331300</td>\n",
       "      <td>15.360700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.719866</td>\n",
       "      <td>15.421500</td>\n",
       "      <td>13.739200</td>\n",
       "      <td>15.331300</td>\n",
       "      <td>15.360700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.719607</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>0.714351</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.713616</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>0.715628</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>0.714531</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.714354</td>\n",
       "      <td>15.410000</td>\n",
       "      <td>13.709400</td>\n",
       "      <td>15.318400</td>\n",
       "      <td>15.341200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-6500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14500\n",
      "Configuration saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14500/config.json\n",
      "Model weights saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in /ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [/ssd_scratch/cvit/aryamaan/t5-small/checkpoint-14000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14528, training_loss=0.7150875595172597, metrics={'train_runtime': 1949.7096, 'train_samples_per_second': 7.451, 'train_steps_per_second': 7.451, 'total_flos': 4407608777637888.0, 'train_loss': 0.7150875595172597, 'epoch': 16.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Dependency-Based Compositional Semantics Compositional question answering begins by mapping\n",
      "Learning Dependency-Based Compositional Semantics. Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning. In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments. More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011).  Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world. It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011). One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al 2011) or even a binary correctincorrect signal (Clarke et al 2010). For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011). DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1). In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable. Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available. WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011). See Liang et al (2011) for work in representing lambda calculus expressions with trees. </s>\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "inputs = prefix + raw_datasets['train']['document'][index]\n",
    "input_ids = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors='pt').input_ids.cuda()\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(raw_datasets['train']['document'][index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
