{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## Scientific document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input 1: abstract\n",
    "* input 2: citance text\n",
    "* target: gold standard summary\n",
    "* dataset downloaded from https://cs.stanford.edu/~myasu/projects/scisumm_net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/W02-1039', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P96-1021', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/N06-1020', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C86-1016', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C92-2070', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/D08-1082', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/J01-2002', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/C02-1114', '/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P85-1018']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "ds_folder = glob.glob(\"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/*\")\n",
    "print(ds_folder[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P83-1020\n"
     ]
    }
   ],
   "source": [
    "ds_paper_num = ds_folder[0][-8:]\n",
    "print(ds_paper_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/Documents_xml/P83-1020.xml\n"
     ]
    }
   ],
   "source": [
    "ds_paper_xml = ds_folder[0] + '/Documents_xml/' + ds_paper_num + '.xml'\n",
    "print(ds_paper_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Theory: Talking About Talking About Trees\n",
      "\n",
      "Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call theory theory While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(ds_paper_xml)\n",
    "root = tree.getroot()\n",
    "title = root[0].text\n",
    "abstract = ' '.join([a.text for a in root[1]])\n",
    "print(title)\n",
    "print()\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/P83-1020/summary/P83-1020.gold.txt\n"
     ]
    }
   ],
   "source": [
    "ds_summary_txt = ds_folder[0] + '/summary/' + ds_paper_num + '.gold.txt'\n",
    "print(ds_summary_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Theory: Talking About Talking About Trees\n",
      "Linguists, including computational linguists, have always been fond of talking about trees.\n",
      "In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory).\n",
      "While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural language in a manner which is intrinsically computational.\n",
      "This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.\n",
      "Our D-theory model is powerful in that it allows the right-most daughter of a node to be lowered under a sibling node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(ds_summary_txt, 'r') as handle:\n",
    "    summary_gold = handle.read()\n",
    "print(summary_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_citance_json = ds_folder[0] + \"/citing_sentences_annotated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "citance_json = pd.read_json(ds_citance_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citance_No</th>\n",
       "      <th>citing_paper_id</th>\n",
       "      <th>citing_paper_authority</th>\n",
       "      <th>citing_paper_authors</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>keep_for_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>This&amp; quot; core parser&amp; quot; has been the su...</td>\n",
       "      <td>Description theory (henceforth, D-theory) (Mar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>This model is interesting in that it does not ...</td>\n",
       "      <td>This model is interesting in that it does not ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>E95-1043</td>\n",
       "      <td>0</td>\n",
       "      <td>Patrick, Sturt</td>\n",
       "      <td>The original D-theory model (Marcus et al (198...</td>\n",
       "      <td>The original D-theory model (Marcus et al (198...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>P10-1053</td>\n",
       "      <td>0</td>\n",
       "      <td>Sylvain, Schmitz</td>\n",
       "      <td>More on Dominance Links Dominance links are qu...</td>\n",
       "      <td>More on Dominance Links Dominance links are qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>E95-1017</td>\n",
       "      <td>3</td>\n",
       "      <td>David, Milward</td>\n",
       "      <td>S /npvp Mary/ V S th inks /npvp^ John M&amp; amp; ...</td>\n",
       "      <td>S /npvp Mary/ V S th inks /npvp^ John M&amp;C sugg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citance_No citing_paper_id  citing_paper_authority citing_paper_authors  \\\n",
       "0           1        E95-1043                       0       Patrick, Sturt   \n",
       "1           2        E95-1043                       0       Patrick, Sturt   \n",
       "2           3        E95-1043                       0       Patrick, Sturt   \n",
       "3           4        P10-1053                       0     Sylvain, Schmitz   \n",
       "4           5        E95-1017                       3       David, Milward   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  This& quot; core parser& quot; has been the su...   \n",
       "1  This model is interesting in that it does not ...   \n",
       "2  The original D-theory model (Marcus et al (198...   \n",
       "3  More on Dominance Links Dominance links are qu...   \n",
       "4  S /npvp Mary/ V S th inks /npvp^ John M& amp; ...   \n",
       "\n",
       "                                          clean_text  keep_for_gold  \n",
       "0  Description theory (henceforth, D-theory) (Mar...              0  \n",
       "1  This model is interesting in that it does not ...              0  \n",
       "2  The original D-theory model (Marcus et al (198...              1  \n",
       "3  More on Dominance Links Dominance links are qu...              0  \n",
       "4  S /npvp Mary/ V S th inks /npvp^ John M&C sugg...              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citance_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Description theory (henceforth, D-theory) (Marcus et al (1983)).', 'This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)).', 'The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node.', 'More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992).', 'S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).']\n"
     ]
    }
   ],
   "source": [
    "citance = citance_json['clean_text'].to_list()\n",
    "print(citance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Description theory (henceforth, D-theory) (Marcus et al (1983)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)). The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992). S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(citance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process(sent):\n",
    "    sent = sent.encode(\"ascii\", \"ignore\").decode()\n",
    "    sent = re.sub(r\"\\n\", \" \", sent)\n",
    "    sent = re.sub(r\"\\^\", \"\", sent)\n",
    "    sent = re.sub(r\"\\/\", \"\", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, root, tokenizer, max_len):\n",
    "        self.ds_folder = glob.glob(root + \"*\")\n",
    "        self.len = len(self.ds_folder)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        ds_paper_num = self.ds_folder[i][-8:]\n",
    "        ds_paper_xml = self.ds_folder[i] + '/Documents_xml/' + ds_paper_num + '.xml'\n",
    "        tree = ET.parse(ds_paper_xml)\n",
    "        root = tree.getroot()\n",
    "        title = root[0].text\n",
    "        abstract = ' '.join([a.text for a in root[1]])\n",
    "        \n",
    "        ds_citance_json = self.ds_folder[i] + \"/citing_sentences_annotated.json\"\n",
    "        citance_json = pd.read_json(ds_citance_json)\n",
    "        citance = citance_json['clean_text'].to_list()\n",
    "        citance = ' '.join(citance)\n",
    "        \n",
    "        ds_summary_txt = self.ds_folder[i] + '/summary/' + ds_paper_num + '.gold.txt'\n",
    "        with open(ds_summary_txt, 'r') as handle:\n",
    "            summary_gold = handle.read()\n",
    "            \n",
    "        try:\n",
    "            return pre_process(title + '. ' + abstract), pre_process(citance), pre_process(summary_gold)\n",
    "        except:\n",
    "            return pre_process(               abstract), pre_process(citance), pre_process(summary_gold)        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "root = \"/home/aryamaanjain/nlp_project/scisummnet_release1.1__20190413/top1000_complete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"A Polynomial-Time Algorithm For Statistical Machine Translation. Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3ROIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minuteusually substantially lesswith no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance.\",\n",
       " 'As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion transduction model. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996). Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation. To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar (Wu, 1996) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases. Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model. The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. To do bigram-integrated decoding, we need to augment each chart item (X,i, j) with two target-language boundary words u and v to produce a bigram-item like u  v Xi j, following the dynamic programming algorithm of Wu (1996). NP (1) VPP-VP (2), NP (1) VPP-VP (2) VPP-VP? VP (1) PP (2), PP (2) VP (1) In this case m-gram integrated decoding can bedone in O (|w|3+4 (m? 1)) time which is much lower order polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. See Wu (1996) or Melamed (2004) for a detailed exposition. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al, 1996) and (Wu, 1996). (Wu,1996) propose using contiguity restrictions on the reordering. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework ,including information extraction (Miller et al, 2000) and syntax-based machine translation (Wu, 1996). To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above.',\n",
       " 'A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. We test our algorithm on Chinese-English translation. ')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Triage(root, None, None)\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sentence likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "              nn.Linear(input_dim, hidden_dim),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.3),\n",
    "              nn.Linear(hidden_dim, 1),\n",
    "              nn.Sigmoid()\n",
    "          )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "loss_function = nn.BCELoss()\n",
    "model = Network(768, 64).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.2377017187729239\n",
      "Epoch: 1\n",
      "Loss: 0.11358868230106624\n",
      "Epoch: 2\n",
      "Loss: 0.08993537564394918\n",
      "Epoch: 3\n",
      "Loss: 0.08618419579579756\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:', epoch)\n",
    "    total_loss = 0\n",
    "    for i, dp in enumerate(data):\n",
    "        if i % 100 == 0:\n",
    "            print(round(i/len(data)*100, 2), end='\\r')\n",
    "        neg = torch.Tensor(embedding_model.encode(dp[1])).cuda()\n",
    "        pos = torch.Tensor(embedding_model.encode(dp[2])).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(neg)\n",
    "        loss = loss_function(output, torch.Tensor([0]).cuda())\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        output = model(pos)\n",
    "        loss = loss_function(output, torch.Tensor([1]).cuda())\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        optimizer.step()\n",
    "    print('Loss:', total_loss / (2*len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "abstract, citance, gold = data[index]\n",
    "pre_summary = abstract[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clist = citance.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0008860226953402162, 0.00874951109290123, 0.29783767461776733, 4.519195863394998e-05, 8.593596945161153e-09, 0.8060272932052612, 0.19032022356987, 7.479471241822466e-05, 0.0026635006070137024, 0.020017080008983612, 0.0005736013408750296, 0.0002440696262056008, 6.778553142794408e-06, 2.8225438200024655e-06, 0.001961392816156149, 0.1124313622713089, 6.103876512497663e-05, 8.708559107617475e-06, 0.0007500247447751462, 0.45174703001976013, 0.656055212020874, 0.03744475543498993, 0.00459548644721508, 9.789203431864735e-06]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for c in clist:\n",
    "    input = torch.Tensor(embedding_model.encode(c)).cuda()\n",
    "    score = model(input)\n",
    "    scores.append(score.item())\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrasal Cohesion And Statistical Machine Translation. There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pre_summary += clist[np.argmax(scores)]\n",
    "\n",
    "print(pre_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce to word limit using abstractive methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_abs = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"summarize: \" + pre_summary, return_tensors=\"pt\", max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_abs.generate(\n",
    "    inputs[\"input_ids\"], max_length=250, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(outputs[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrasal movement has been used to improve statistical machine translation. we compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. lexical translation models structural changes and changes to feature values.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.11\r"
     ]
    }
   ],
   "source": [
    "rscores_list = []\n",
    "\n",
    "for i, dp in enumerate(data):\n",
    "    if i % 100 == 0:\n",
    "        print(round(i/len(data)*100, 2), end='\\r')\n",
    "    abstract, citance, gold = dp\n",
    "    pre_summary = abstract[:]\n",
    "\n",
    "    clist = citance.split('.')\n",
    "    scores = []\n",
    "    for c in clist:\n",
    "        input = torch.Tensor(embedding_model.encode(c)).cuda()\n",
    "        score = model(input)\n",
    "        scores.append(score.item())\n",
    "\n",
    "    pre_summary += clist[np.argmax(scores)]\n",
    "    \n",
    "    rscores = scorer.score(gold, pre_summary)\n",
    "    rscores_list.append(rscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616219811940567 0.73298061299218 0.719980626362649\n"
     ]
    }
   ],
   "source": [
    "# rouge 1\n",
    "tp = 0\n",
    "tr = 0\n",
    "tf = 0\n",
    "\n",
    "for rr in rscores_list:\n",
    "    p, r, f = rr['rouge1']\n",
    "    tp += p\n",
    "    tr += r\n",
    "    tf += f\n",
    "tp /= len(rscores_list)\n",
    "tr /= len(rscores_list)\n",
    "tf /= len(rscores_list)\n",
    "    \n",
    "print(tp, tr, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939648897214218 0.6644611185483289 0.6608730307168373\n"
     ]
    }
   ],
   "source": [
    "# rouge 2\n",
    "tp = 0\n",
    "tr = 0\n",
    "tf = 0\n",
    "\n",
    "for rr in rscores_list:\n",
    "    p, r, f = rr['rouge2']\n",
    "    tp += p\n",
    "    tr += r\n",
    "    tf += f\n",
    "tp /= len(rscores_list)\n",
    "tr /= len(rscores_list)\n",
    "tf /= len(rscores_list)\n",
    "    \n",
    "print(tp, tr, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7393056533289155 0.7058958277728171 0.7000450739699114\n"
     ]
    }
   ],
   "source": [
    "# rouge L\n",
    "tp = 0\n",
    "tr = 0\n",
    "tf = 0\n",
    "\n",
    "for rr in rscores_list:\n",
    "    p, r, f = rr['rougeL']\n",
    "    tp += p\n",
    "    tr += r\n",
    "    tf += f\n",
    "tp /= len(rscores_list)\n",
    "tr /= len(rscores_list)\n",
    "tf /= len(rscores_list)\n",
    "    \n",
    "print(tp, tr, tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
