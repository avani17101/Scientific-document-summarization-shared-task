{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c2f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import ast\n",
    "# from argparse import ArgumentParser\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument(\"-multi_lb_classi\", default=1, type=int, help=\"whether train a multi-lable classifier or individual classifiers for each class\")\n",
    "# opt = parser.parse_args()\n",
    "multi_lb_classi = 0\n",
    "docs = os.listdir(\"./scisumm-2018/Training\")\n",
    "train_e = int(0.8*len(docs))\n",
    "train_docs = docs[:train_e]\n",
    "test_docs = docs[train_e:]\n",
    "# test_docs  = os.listdir(\"./scisumm-2018/Test\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(example_sent):\n",
    "    global stop_words\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d01b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f9331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bea1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files, folder):\n",
    "    data_lis = []\n",
    "    labels = []\n",
    "    sid_lis = []\n",
    "    ssid_lis = []\n",
    "    for z,f in enumerate(files):\n",
    "        data = None\n",
    "        ann = None\n",
    "        a_folder = folder+f+\"/annotation/\"\n",
    "        file = os.listdir(a_folder)[0]\n",
    "\n",
    "        ann = a_folder+file\n",
    "        with open(ann,\"r\") as f:\n",
    "            data = f.read()\n",
    "#         print(f)\n",
    "            facet = re.findall(\"Discourse Facet:\\s+([^|]*)\", data)\n",
    "            for f in facet:\n",
    "                if '[' in f:\n",
    "                    f = ast.literal_eval(f)\n",
    "                    f_lis = []\n",
    "                    for a in f:\n",
    "                        t_lb = a.strip().lower()\n",
    "                        if 'results' in t_lb:\n",
    "                            f_lis.append(t_lb.replace('results','result'))\n",
    "                        else:\n",
    "                            f_lis.append(t_lb)\n",
    "                    labels.append(f_lis)\n",
    "                        \n",
    "                else:\n",
    "                    t_lb = f.strip().lower()\n",
    "                    if 'results' in t_lb:\n",
    "                        t_lb = t_lb.replace('results','result')\n",
    "                    labels.append([t_lb])\n",
    "\n",
    "            \n",
    "            ref_text = re.findall(\"Reference Text:\\s+([^|]*)\", data)\n",
    "            pattern = r'\\<.*?\\>'\n",
    "            pattern2 = r'\\(.*?\\)'\n",
    "            \n",
    "            \n",
    "            for ref in ref_text:\n",
    "                ssid_per_citant = []\n",
    "                sid_per_citant = []\n",
    "                temp = re.findall(r'\\<S.*?\\>',ref)\n",
    "                sids = re.findall(r'<S sid =.*?\\s',ref)\n",
    "                ssids = re.findall(r'ssid =.*?>',ref)\n",
    "                for i,sid in enumerate(sids):\n",
    "                    sid = sid.split('=')[1]\n",
    "                    \n",
    "                    try:\n",
    "                        sid_per_citant.append(int(sid.replace('\"','').strip()))\n",
    "                    except:\n",
    "#                         print(sid)\n",
    "                        sid = sid.split('>')[0]\n",
    "                        sid_per_citant.append(int(sid.replace('\"','').strip()))\n",
    "                    try:\n",
    "                        ssid = ssids[i].split('=')[1].replace('>',\"\")\n",
    "                    except:\n",
    "                        ssid = sid\n",
    "                    ssid_per_citant.append(int(ssid.replace('\"','').strip()))\n",
    "                sid_lis.append(sid_per_citant)\n",
    "                ssid_lis.append(ssid_per_citant)\n",
    "                ref = re.sub(pattern2,'',re.sub(pattern, '', ref))\n",
    "                ref = preprocess(ref)\n",
    "                data_lis.append(ref)\n",
    "\n",
    "            \n",
    "    return data_lis, sid_lis, ssid_lis,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2885f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_dataset(test_docs):\n",
    "#     '''\n",
    "#     gives citance and labels\n",
    "#     '''\n",
    "#     data_lis = []\n",
    "#     labels = []\n",
    "#     for z,f in enumerate(files):\n",
    "#         df = folder+f+\"/annotation/\"+f+\".csv\"\n",
    "#         data_lis.append()\n",
    "\n",
    "# test_data,test_labels = get_dataset(test_docs[:2],folder='scisumm-2018/Test/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1be2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_sid, train_ssid, train_labels = get_dataset(train_docs,\"scisumm-2018/Training/\")\n",
    "test_data, test_sid, test_ssid, test_labels = get_dataset(test_docs,\"scisumm-2018/Training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d5c7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 590, 590, 590)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_labels), len(train_sid), len(train_ssid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690ed2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 163)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a11670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15],\n",
       " [2, 15],\n",
       " [21, 7],\n",
       " [25],\n",
       " [34, 67],\n",
       " [25, 26, 27],\n",
       " [2, 3, 21],\n",
       " [7, 10],\n",
       " [10, 44],\n",
       " [10, 13],\n",
       " [21],\n",
       " [17, 21],\n",
       " [21],\n",
       " [21, 9, 6, 65, 76],\n",
       " [26, 29, 30],\n",
       " [5],\n",
       " [6],\n",
       " [9],\n",
       " [1],\n",
       " [9],\n",
       " [3],\n",
       " [3],\n",
       " [15],\n",
       " [4],\n",
       " [22],\n",
       " [100],\n",
       " [1],\n",
       " [3],\n",
       " [6],\n",
       " [9],\n",
       " [1],\n",
       " [1],\n",
       " [104],\n",
       " [104],\n",
       " [27, 34, 2],\n",
       " [16],\n",
       " [97],\n",
       " [20, 21],\n",
       " [97],\n",
       " [6],\n",
       " [3],\n",
       " [1, 2, 10],\n",
       " [52],\n",
       " [3],\n",
       " [26],\n",
       " [22],\n",
       " [5, 6, 7],\n",
       " [3],\n",
       " [3, 4],\n",
       " [2, 6],\n",
       " [6, 1, 2, 52],\n",
       " [28],\n",
       " [8],\n",
       " [1],\n",
       " [17],\n",
       " [9],\n",
       " [17],\n",
       " [16, 17],\n",
       " [63],\n",
       " [63],\n",
       " [68],\n",
       " [63],\n",
       " [6],\n",
       " [24],\n",
       " [24, 82, 101],\n",
       " [11],\n",
       " [21, 22, 27, 8],\n",
       " [38],\n",
       " [4, 5],\n",
       " [16],\n",
       " [1],\n",
       " [94],\n",
       " [11],\n",
       " [11],\n",
       " [11],\n",
       " [46],\n",
       " [21],\n",
       " [46],\n",
       " [70, 72],\n",
       " [0],\n",
       " [24],\n",
       " [0],\n",
       " [70, 72, 24],\n",
       " [20],\n",
       " [94],\n",
       " [94],\n",
       " [33, 34],\n",
       " [21],\n",
       " [12],\n",
       " [21, 22, 23],\n",
       " [4, 5],\n",
       " [3],\n",
       " [8, 9, 10],\n",
       " [39, 40, 22],\n",
       " [23, 4],\n",
       " [4],\n",
       " [11, 14],\n",
       " [2, 22, 23, 24],\n",
       " [22, 23, 24],\n",
       " [23, 24],\n",
       " [23, 2, 3],\n",
       " [24],\n",
       " [12],\n",
       " [50],\n",
       " [130],\n",
       " [2],\n",
       " [46],\n",
       " [16],\n",
       " [23],\n",
       " [58],\n",
       " [129],\n",
       " [50],\n",
       " [33],\n",
       " [132],\n",
       " [37, 38],\n",
       " [1, 10],\n",
       " [37, 38, 45],\n",
       " [21, 23, 37, 39],\n",
       " [21],\n",
       " [79],\n",
       " [22, 23],\n",
       " [66, 67, 68],\n",
       " [22, 23],\n",
       " [51, 58],\n",
       " [37, 38],\n",
       " [15, 22],\n",
       " [50],\n",
       " [57, 66, 67, 68, 82],\n",
       " [13, 11, 57],\n",
       " [3, 4, 46],\n",
       " [37, 38, 48, 49, 55],\n",
       " [45, 55],\n",
       " [1],\n",
       " [3, 28],\n",
       " [51, 55, 58, 59],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [2],\n",
       " [14],\n",
       " [1],\n",
       " [26],\n",
       " [11],\n",
       " [64],\n",
       " [1],\n",
       " [1],\n",
       " [23],\n",
       " [1],\n",
       " [1],\n",
       " [20, 1],\n",
       " [41, 3, 6],\n",
       " [31, 32, 33],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [7, 8],\n",
       " [36, 37],\n",
       " [4, 25],\n",
       " [25],\n",
       " [12],\n",
       " [17, 25],\n",
       " [18, 24],\n",
       " [25],\n",
       " [12, 7, 8],\n",
       " [18, 19],\n",
       " [18, 5, 6, 7],\n",
       " [34, 35, 36, 37],\n",
       " [17, 19, 4, 5],\n",
       " [25],\n",
       " [18],\n",
       " [18, 24],\n",
       " [12],\n",
       " [19, 7],\n",
       " [7, 8],\n",
       " [18],\n",
       " [7],\n",
       " [18, 111, 112],\n",
       " [4, 5],\n",
       " [38, 41],\n",
       " [25],\n",
       " [18],\n",
       " [19],\n",
       " [24],\n",
       " [18, 19],\n",
       " [17],\n",
       " [18],\n",
       " [19, 25],\n",
       " [19, 7],\n",
       " [19],\n",
       " [165],\n",
       " [4],\n",
       " [19, 25],\n",
       " [19, 25],\n",
       " [19],\n",
       " [10],\n",
       " [3, 10],\n",
       " [3],\n",
       " [3],\n",
       " [10],\n",
       " [2, 3],\n",
       " [7],\n",
       " [3],\n",
       " [2],\n",
       " [10],\n",
       " [28],\n",
       " [4],\n",
       " [10, 2, 3],\n",
       " [8],\n",
       " [18, 19, 139],\n",
       " [6],\n",
       " [18, 19, 20],\n",
       " [96],\n",
       " [20, 1],\n",
       " [2],\n",
       " [1, 8, 8, 14, 15, 75],\n",
       " [9],\n",
       " [6, 25],\n",
       " [6],\n",
       " [8],\n",
       " [96],\n",
       " [2],\n",
       " [6],\n",
       " [3, 139],\n",
       " [2],\n",
       " [2, 3],\n",
       " [8],\n",
       " [38, 39],\n",
       " [8],\n",
       " [2, 89, 94, 160, 161],\n",
       " [9],\n",
       " [9],\n",
       " [9],\n",
       " [24],\n",
       " [5],\n",
       " [2],\n",
       " [24],\n",
       " [14],\n",
       " [9],\n",
       " [9],\n",
       " [14],\n",
       " [24],\n",
       " [14],\n",
       " [24],\n",
       " [14],\n",
       " [38],\n",
       " [9],\n",
       " [2],\n",
       " [9],\n",
       " [9],\n",
       " [9],\n",
       " [24],\n",
       " [38],\n",
       " [24],\n",
       " [2],\n",
       " [24],\n",
       " [2],\n",
       " [9],\n",
       " [5],\n",
       " [9],\n",
       " [9],\n",
       " [5],\n",
       " [27],\n",
       " [124, 125],\n",
       " [87, 88, 89],\n",
       " [1],\n",
       " [88, 90],\n",
       " [29, 30],\n",
       " [69, 70],\n",
       " [2],\n",
       " [145],\n",
       " [29, 30],\n",
       " [36],\n",
       " [36],\n",
       " [36],\n",
       " [10],\n",
       " [36],\n",
       " [3, 4, 5],\n",
       " [1, 2, 3],\n",
       " [15, 16, 17],\n",
       " [36],\n",
       " [4, 5],\n",
       " [15, 21],\n",
       " [6, 6, 7, 8],\n",
       " [14],\n",
       " [36],\n",
       " [36],\n",
       " [15, 16, 17],\n",
       " [36],\n",
       " [36],\n",
       " [7, 8, 9, 10, 11],\n",
       " [2],\n",
       " [15, 21],\n",
       " [7],\n",
       " [2, 3, 4],\n",
       " [40],\n",
       " [16, 17],\n",
       " [2],\n",
       " [1],\n",
       " [18],\n",
       " [1],\n",
       " [10],\n",
       " [6],\n",
       " [21],\n",
       " [15],\n",
       " [17],\n",
       " [4],\n",
       " [25, 27],\n",
       " [3, 4, 6],\n",
       " [18, 26, 33, 34],\n",
       " [6, 46],\n",
       " [1],\n",
       " [3, 5],\n",
       " [4],\n",
       " [7, 8, 9],\n",
       " [14, 26, 27],\n",
       " [29, 30],\n",
       " [29, 30, 1, 2],\n",
       " [3],\n",
       " [3, 46],\n",
       " [1],\n",
       " [6, 46],\n",
       " [3, 5],\n",
       " [4],\n",
       " [3, 4, 5, 6],\n",
       " [46],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [26],\n",
       " [6],\n",
       " [2, 3],\n",
       " [3],\n",
       " [3],\n",
       " [4],\n",
       " [42, 43, 44],\n",
       " [1, 2, 3],\n",
       " [26],\n",
       " [1, 2, 3],\n",
       " [76, 77],\n",
       " [11],\n",
       " [3],\n",
       " [4],\n",
       " [11],\n",
       " [15],\n",
       " [17],\n",
       " [2],\n",
       " [1],\n",
       " [73],\n",
       " [1, 67],\n",
       " [8],\n",
       " [27],\n",
       " [28],\n",
       " [64],\n",
       " [15],\n",
       " [67, 67, 68],\n",
       " [15, 28],\n",
       " [33],\n",
       " [11],\n",
       " [23],\n",
       " [1],\n",
       " [2, 3],\n",
       " [15],\n",
       " [17],\n",
       " [158],\n",
       " [4, 113],\n",
       " [129, 132, 133],\n",
       " [13],\n",
       " [174],\n",
       " [27, 28, 31, 28],\n",
       " [13, 14],\n",
       " [54, 57],\n",
       " [72],\n",
       " [46, 47],\n",
       " [1, 3, 3],\n",
       " [18, 19, 35],\n",
       " [1],\n",
       " [1],\n",
       " [14, 16],\n",
       " [18, 3, 11],\n",
       " [3],\n",
       " [13],\n",
       " [3],\n",
       " [10],\n",
       " [24],\n",
       " [27],\n",
       " [44],\n",
       " [5],\n",
       " [52],\n",
       " [41],\n",
       " [1],\n",
       " [30],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [4, 1],\n",
       " [4],\n",
       " [21],\n",
       " [19, 1],\n",
       " [21],\n",
       " [19],\n",
       " [24, 8],\n",
       " [9],\n",
       " [1],\n",
       " [9],\n",
       " [3],\n",
       " [19],\n",
       " [4],\n",
       " [9],\n",
       " [19],\n",
       " [3],\n",
       " [19],\n",
       " [1],\n",
       " [75],\n",
       " [7, 8],\n",
       " [17],\n",
       " [11],\n",
       " [17, 20],\n",
       " [78, 79],\n",
       " [7, 8],\n",
       " [5],\n",
       " [4],\n",
       " [3, 17],\n",
       " [3, 4],\n",
       " [5],\n",
       " [17],\n",
       " [17, 20],\n",
       " [8, 11],\n",
       " [17],\n",
       " [17],\n",
       " [75],\n",
       " [17],\n",
       " [5, 11],\n",
       " [75],\n",
       " [5, 11],\n",
       " [3],\n",
       " [5],\n",
       " [11],\n",
       " [10, 11],\n",
       " [36],\n",
       " [20],\n",
       " [7],\n",
       " [9],\n",
       " [3],\n",
       " [9, 3],\n",
       " [12],\n",
       " [3, 1, 2],\n",
       " [11],\n",
       " [11],\n",
       " [18, 37],\n",
       " [18],\n",
       " [17],\n",
       " [17],\n",
       " [17, 20],\n",
       " [11],\n",
       " [11, 8],\n",
       " [1],\n",
       " [2],\n",
       " [10],\n",
       " [17],\n",
       " [12],\n",
       " [17, 8],\n",
       " [9],\n",
       " [4],\n",
       " [75],\n",
       " [1],\n",
       " [11],\n",
       " [11],\n",
       " [12],\n",
       " [3, 33],\n",
       " [3],\n",
       " [36],\n",
       " [33],\n",
       " [44, 45],\n",
       " [44, 45, 33, 34],\n",
       " [8],\n",
       " [3],\n",
       " [8],\n",
       " [8],\n",
       " [3],\n",
       " [8],\n",
       " [8, 3, 1],\n",
       " [1, 1],\n",
       " [7, 8],\n",
       " [8],\n",
       " [1, 2],\n",
       " [8],\n",
       " [8],\n",
       " [16, 17],\n",
       " [3, 37],\n",
       " [5],\n",
       " [5],\n",
       " [5, 11],\n",
       " [11, 10],\n",
       " [5, 11],\n",
       " [10],\n",
       " [2],\n",
       " [37, 89, 123, 125],\n",
       " [68, 71],\n",
       " [11, 15],\n",
       " [64, 68],\n",
       " [75, 76],\n",
       " [38],\n",
       " [22, 27],\n",
       " [64, 67, 1],\n",
       " [66, 37, 38],\n",
       " [32, 33],\n",
       " [1, 2, 3],\n",
       " [1],\n",
       " [1],\n",
       " [1, 4],\n",
       " [2, 11],\n",
       " [5, 33],\n",
       " [1],\n",
       " [44],\n",
       " [9, 10],\n",
       " [10, 11],\n",
       " [1],\n",
       " [1, 2],\n",
       " [3],\n",
       " [2],\n",
       " [1],\n",
       " [1],\n",
       " [18, 38, 39, 40],\n",
       " [2],\n",
       " [19],\n",
       " [8],\n",
       " [1, 2],\n",
       " [1, 2, 1, 2],\n",
       " [1],\n",
       " [1, 2, 3],\n",
       " [2],\n",
       " [3, 6],\n",
       " [1],\n",
       " [1],\n",
       " [3, 5],\n",
       " [1],\n",
       " [3, 5],\n",
       " [11],\n",
       " [1],\n",
       " [13],\n",
       " [11],\n",
       " [2],\n",
       " [1, 2],\n",
       " [11, 50, 51],\n",
       " [1, 2],\n",
       " [1, 2],\n",
       " [1],\n",
       " [49, 50, 51],\n",
       " [10, 64],\n",
       " [1, 2],\n",
       " [1, 2],\n",
       " [3, 5],\n",
       " [11],\n",
       " [23, 24],\n",
       " [12, 13],\n",
       " [1],\n",
       " [13, 30, 22],\n",
       " [22],\n",
       " [22, 16],\n",
       " [16, 17],\n",
       " [31, 14],\n",
       " [5, 6],\n",
       " [14],\n",
       " [14],\n",
       " [18, 20],\n",
       " [22, 23],\n",
       " [24],\n",
       " [11],\n",
       " [18],\n",
       " [38, 41],\n",
       " [8, 9],\n",
       " [22, 23],\n",
       " [30, 7],\n",
       " [63],\n",
       " [10, 11],\n",
       " [9],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [18],\n",
       " [7],\n",
       " [17],\n",
       " [35],\n",
       " [17],\n",
       " [7],\n",
       " [63]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ssid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b0329",
   "metadata": {},
   "source": [
    "## multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0717ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(train_labels)\n",
    "# train_labels = le.transform(train_labels)\n",
    "# # val_labels = le.transform(val_labels)\n",
    "# test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0bcde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['method_citation'],\n",
       " ['hypothesis_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['implication_citation', 'method_citation'],\n",
       " ['hypothesis_citation', 'method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['hypothesis_citation', 'aim_citation'],\n",
       " ['hypothesis_citation', 'aim_citation'],\n",
       " ['hypothesis_citation', 'aim_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['aim_citation'],\n",
       " ['result_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['aim_citation'],\n",
       " ['aim_citation'],\n",
       " ['aim_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['aim_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['hypothesis_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation', 'aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['aim_citation', 'result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'aim_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation', 'result_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation', 'implication_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation', 'aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation', 'method_citation'],\n",
       " ['result_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['implication_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['implication_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['implication_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation', 'implication_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['result_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['hypothesis_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'implication_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation'],\n",
       " ['aim_citation'],\n",
       " ['method_citation'],\n",
       " ['result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['aim_citation', 'result_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation', 'result_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['implication_citation'],\n",
       " ['implication_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['method_citation'],\n",
       " ['aim_citation', 'method_citation'],\n",
       " ['implication_citation'],\n",
       " ['method_citation']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0c15c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 163)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3c208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avani.gupta/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:876: UserWarning: unknown class(es) ['method citation'] will be ignored\n",
      "  \"unknown class(es) {0} will be ignored\".format(sorted(unknown, key=str))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels_lb = mlb.fit_transform(train_labels)\n",
    "test_labels_lb = mlb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6578594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7a0f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36885d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aim_citation', 'hypothesis_citation', 'implication_citation',\n",
       "       'method_citation', 'result_citation'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6afdafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 590)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_labels_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "261bae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(pred_lis, gt_lis):\n",
    "    acc = accuracy_score(pred_lis, gt_lis)\n",
    "    recall = recall_score(pred_lis, gt_lis,average='micro')\n",
    "    prec = precision_score(pred_lis, gt_lis,average='micro')\n",
    "    f1 = f1_score(pred_lis, gt_lis,average='micro')\n",
    "    print(\"metrics obtained in test: accuracy {} recall {}, precision {}, f1-score {}\".format(acc,recall,prec,f1))\n",
    "    \n",
    "# def get_integer_mapping(le):\n",
    "#     '''\n",
    "#     Return a dict mapping labels to their integer values\n",
    "#     from an SKlearn LabelEncoder\n",
    "#     le = a fitted SKlearn LabelEncoder\n",
    "#     '''\n",
    "#     res = {}\n",
    "#     for cl in le.classes_:\n",
    "#         res.update({le.transform([cl])[0]:cl})\n",
    "\n",
    "#     return res\n",
    "\n",
    "\n",
    "# mapping = get_integer_mapping(mlb)\n",
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd858f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data,val_labels = get_dataset(val_docs,folder='From-ScisummNet-2019')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "# val_encodings = tokenizer(val_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "test_encodings = tokenizer(test_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d8b8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings_orig = tokenizer(test_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "121ec9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 590, 590)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings),len(train_labels),len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b48792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([590, 512]), 590)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_ids'].shape, len(train_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "582771b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15],\n",
       " [2, 15],\n",
       " [21, 37],\n",
       " [152],\n",
       " [124, 194],\n",
       " [115, 116, 117],\n",
       " [2, 3, 21],\n",
       " [7, 10],\n",
       " [10, 83],\n",
       " [10, 13],\n",
       " [21],\n",
       " [17, 21],\n",
       " [21],\n",
       " [21, 39, 45, 192, 203],\n",
       " [116, 119, 120],\n",
       " [5],\n",
       " [6],\n",
       " [9],\n",
       " [48],\n",
       " [9],\n",
       " [3],\n",
       " [3],\n",
       " [15],\n",
       " [99],\n",
       " [22],\n",
       " [239],\n",
       " [52],\n",
       " [112],\n",
       " [6],\n",
       " [9],\n",
       " [52],\n",
       " [52],\n",
       " [243],\n",
       " [243],\n",
       " [27, 85, 97],\n",
       " [155],\n",
       " [236],\n",
       " [20, 21],\n",
       " [236],\n",
       " [6],\n",
       " [3],\n",
       " [17, 18, 113],\n",
       " [68],\n",
       " [3],\n",
       " [42],\n",
       " [125],\n",
       " [5, 6, 7],\n",
       " [3],\n",
       " [3, 4],\n",
       " [2, 6],\n",
       " [6, 17, 18, 68],\n",
       " [44],\n",
       " [8],\n",
       " [1],\n",
       " [17],\n",
       " [45],\n",
       " [17],\n",
       " [16, 17],\n",
       " [270],\n",
       " [270],\n",
       " [275],\n",
       " [270],\n",
       " [6],\n",
       " [24],\n",
       " [24, 289, 308],\n",
       " [11],\n",
       " [21, 22, 53, 141],\n",
       " [64],\n",
       " [116, 117],\n",
       " [128],\n",
       " [312],\n",
       " [301],\n",
       " [11],\n",
       " [11],\n",
       " [11],\n",
       " [72],\n",
       " [21],\n",
       " [72],\n",
       " [277, 279],\n",
       " [0],\n",
       " [24],\n",
       " [0],\n",
       " [277, 279, 24],\n",
       " [153],\n",
       " [301],\n",
       " [301],\n",
       " [166, 167],\n",
       " [154],\n",
       " [12],\n",
       " [21, 22, 23],\n",
       " [205, 206],\n",
       " [3],\n",
       " [186, 187, 188],\n",
       " [39, 40, 78],\n",
       " [23, 205],\n",
       " [205],\n",
       " [11, 14],\n",
       " [203, 22, 23, 24],\n",
       " [22, 23, 24],\n",
       " [23, 24],\n",
       " [23, 141, 142],\n",
       " [24],\n",
       " [29],\n",
       " [123],\n",
       " [203],\n",
       " [62],\n",
       " [119],\n",
       " [16],\n",
       " [96],\n",
       " [131],\n",
       " [202],\n",
       " [123],\n",
       " [50],\n",
       " [205],\n",
       " [116, 117],\n",
       " [80, 89],\n",
       " [116, 117, 124],\n",
       " [100, 102, 116, 118],\n",
       " [100],\n",
       " [158],\n",
       " [101, 102],\n",
       " [145, 146, 147],\n",
       " [101, 102],\n",
       " [130, 137],\n",
       " [116, 117],\n",
       " [94, 101],\n",
       " [129],\n",
       " [136, 145, 146, 147, 161],\n",
       " [75, 90, 136],\n",
       " [3, 4, 125],\n",
       " [116, 117, 127, 128, 134],\n",
       " [124, 134],\n",
       " [63],\n",
       " [3, 107],\n",
       " [130, 134, 137, 138],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [260],\n",
       " [204],\n",
       " [1],\n",
       " [123],\n",
       " [11],\n",
       " [254],\n",
       " [191],\n",
       " [45],\n",
       " [115],\n",
       " [45],\n",
       " [45],\n",
       " [20, 45],\n",
       " [41, 47, 50],\n",
       " [123, 124, 125],\n",
       " [62],\n",
       " [62],\n",
       " [45],\n",
       " [7, 8],\n",
       " [36, 37],\n",
       " [32, 25],\n",
       " [25],\n",
       " [63],\n",
       " [17, 25],\n",
       " [18, 24],\n",
       " [25],\n",
       " [63, 58, 59],\n",
       " [18, 19],\n",
       " [18, 33, 34, 35],\n",
       " [85, 86, 87, 88],\n",
       " [17, 19, 333, 334],\n",
       " [25],\n",
       " [18],\n",
       " [18, 24],\n",
       " [63],\n",
       " [19, 58],\n",
       " [58, 59],\n",
       " [18],\n",
       " [35],\n",
       " [18, 162, 163],\n",
       " [333, 334],\n",
       " [322, 325],\n",
       " [25],\n",
       " [18],\n",
       " [19],\n",
       " [24],\n",
       " [18, 19],\n",
       " [17],\n",
       " [18],\n",
       " [19, 25],\n",
       " [19, 58],\n",
       " [19],\n",
       " [216],\n",
       " [32],\n",
       " [19, 25],\n",
       " [19, 25],\n",
       " [19],\n",
       " [37],\n",
       " [20, 37],\n",
       " [20],\n",
       " [20],\n",
       " [37],\n",
       " [2, 3],\n",
       " [93],\n",
       " [20],\n",
       " [19],\n",
       " [37],\n",
       " [72],\n",
       " [90],\n",
       " [37, 2, 3],\n",
       " [8],\n",
       " [18, 19, 201],\n",
       " [68],\n",
       " [18, 19, 20],\n",
       " [158],\n",
       " [49, 63],\n",
       " [2],\n",
       " [1, 8, 70, 76, 77, 137],\n",
       " [9],\n",
       " [68, 87],\n",
       " [68],\n",
       " [8],\n",
       " [158],\n",
       " [2],\n",
       " [68],\n",
       " [3, 201],\n",
       " [2],\n",
       " [2, 3],\n",
       " [8],\n",
       " [100, 101],\n",
       " [8],\n",
       " [64, 151, 156, 222, 223],\n",
       " [23],\n",
       " [9],\n",
       " [23],\n",
       " [51],\n",
       " [32],\n",
       " [29],\n",
       " [51],\n",
       " [14],\n",
       " [9],\n",
       " [9],\n",
       " [14],\n",
       " [51],\n",
       " [14],\n",
       " [51],\n",
       " [14],\n",
       " [65],\n",
       " [23],\n",
       " [29],\n",
       " [9],\n",
       " [9],\n",
       " [9],\n",
       " [51],\n",
       " [65],\n",
       " [51],\n",
       " [29],\n",
       " [51],\n",
       " [29],\n",
       " [23],\n",
       " [32],\n",
       " [9],\n",
       " [23],\n",
       " [32],\n",
       " [27],\n",
       " [124, 125],\n",
       " [87, 88, 89],\n",
       " [1],\n",
       " [88, 90],\n",
       " [29, 30],\n",
       " [69, 70],\n",
       " [2],\n",
       " [145],\n",
       " [29, 30],\n",
       " [36],\n",
       " [36],\n",
       " [36],\n",
       " [89],\n",
       " [36],\n",
       " [124, 125, 126],\n",
       " [80, 81, 82],\n",
       " [119, 120, 121],\n",
       " [36],\n",
       " [196, 197],\n",
       " [64, 70],\n",
       " [6, 144, 145, 146],\n",
       " [93],\n",
       " [36],\n",
       " [36],\n",
       " [119, 120, 121],\n",
       " [36],\n",
       " [36],\n",
       " [199, 200, 201, 202, 203],\n",
       " [140],\n",
       " [64, 70],\n",
       " [86],\n",
       " [123, 124, 125],\n",
       " [40],\n",
       " [95, 96],\n",
       " [140],\n",
       " [217],\n",
       " [18],\n",
       " [1],\n",
       " [47],\n",
       " [6],\n",
       " [21],\n",
       " [15],\n",
       " [17],\n",
       " [30],\n",
       " [62, 64],\n",
       " [3, 4, 6],\n",
       " [18, 26, 33, 34],\n",
       " [6, 46],\n",
       " [86],\n",
       " [88, 90],\n",
       " [110],\n",
       " [113, 114, 115],\n",
       " [14, 26, 27],\n",
       " [29, 30],\n",
       " [29, 30, 107, 108],\n",
       " [3],\n",
       " [3, 46],\n",
       " [86],\n",
       " [6, 46],\n",
       " [88, 90],\n",
       " [110],\n",
       " [3, 4, 5, 6],\n",
       " [46],\n",
       " [63],\n",
       " [4],\n",
       " [4],\n",
       " [196],\n",
       " [86],\n",
       " [6],\n",
       " [2, 3],\n",
       " [63],\n",
       " [63],\n",
       " [4],\n",
       " [102, 103, 104],\n",
       " [61, 62, 63],\n",
       " [86],\n",
       " [61, 62, 63],\n",
       " [136, 137],\n",
       " [11],\n",
       " [63],\n",
       " [4],\n",
       " [11],\n",
       " [15],\n",
       " [17],\n",
       " [131],\n",
       " [34],\n",
       " [123],\n",
       " [34, 117],\n",
       " [137],\n",
       " [27],\n",
       " [28],\n",
       " [114],\n",
       " [144],\n",
       " [117, 229, 230],\n",
       " [144, 157],\n",
       " [195],\n",
       " [11],\n",
       " [23],\n",
       " [261],\n",
       " [35, 254],\n",
       " [144],\n",
       " [17],\n",
       " [357],\n",
       " [4, 113],\n",
       " [129, 132, 133],\n",
       " [13],\n",
       " [174],\n",
       " [27, 28, 31, 227],\n",
       " [212, 213],\n",
       " [54, 57],\n",
       " [72],\n",
       " [245, 246],\n",
       " [84, 101, 161],\n",
       " [18, 19, 61],\n",
       " [189],\n",
       " [189],\n",
       " [40, 42],\n",
       " [18, 29, 37],\n",
       " [3],\n",
       " [13],\n",
       " [3],\n",
       " [10],\n",
       " [40],\n",
       " [43],\n",
       " [60],\n",
       " [81],\n",
       " [68],\n",
       " [57],\n",
       " [1],\n",
       " [46],\n",
       " [47],\n",
       " [47],\n",
       " [47],\n",
       " [4, 47],\n",
       " [4],\n",
       " [40],\n",
       " [38, 47],\n",
       " [40],\n",
       " [65],\n",
       " [70, 94],\n",
       " [28],\n",
       " [47],\n",
       " [164],\n",
       " [158],\n",
       " [65],\n",
       " [4],\n",
       " [28],\n",
       " [65],\n",
       " [3],\n",
       " [65],\n",
       " [20],\n",
       " [193],\n",
       " [51, 52],\n",
       " [17],\n",
       " [40],\n",
       " [17, 20],\n",
       " [196, 197],\n",
       " [51, 52],\n",
       " [34],\n",
       " [33],\n",
       " [60, 17],\n",
       " [121, 122],\n",
       " [34],\n",
       " [17],\n",
       " [17, 20],\n",
       " [37, 40],\n",
       " [17],\n",
       " [17],\n",
       " [193],\n",
       " [17],\n",
       " [34, 40],\n",
       " [193],\n",
       " [34, 40],\n",
       " [110],\n",
       " [34],\n",
       " [40],\n",
       " [67, 68],\n",
       " [93],\n",
       " [20],\n",
       " [51],\n",
       " [53],\n",
       " [121],\n",
       " [53, 121],\n",
       " [130],\n",
       " [110, 58, 59],\n",
       " [40],\n",
       " [40],\n",
       " [18, 155],\n",
       " [18],\n",
       " [17],\n",
       " [17],\n",
       " [17, 20],\n",
       " [40],\n",
       " [40, 52],\n",
       " [108],\n",
       " [59],\n",
       " [128],\n",
       " [17],\n",
       " [130],\n",
       " [17, 126],\n",
       " [53],\n",
       " [4],\n",
       " [193],\n",
       " [30],\n",
       " [40],\n",
       " [40],\n",
       " [12],\n",
       " [25, 104],\n",
       " [3],\n",
       " [58],\n",
       " [104],\n",
       " [66, 67],\n",
       " [66, 67, 104, 105],\n",
       " [8],\n",
       " [25],\n",
       " [8],\n",
       " [8],\n",
       " [25],\n",
       " [8],\n",
       " [8, 153, 72],\n",
       " [151, 72],\n",
       " [78, 79],\n",
       " [8],\n",
       " [1, 2],\n",
       " [8],\n",
       " [8],\n",
       " [38, 39],\n",
       " [3, 144],\n",
       " [17],\n",
       " [17],\n",
       " [17, 11],\n",
       " [11, 164],\n",
       " [17, 23],\n",
       " [10],\n",
       " [156],\n",
       " [214, 358, 392, 394],\n",
       " [68, 71],\n",
       " [188, 192],\n",
       " [64, 68],\n",
       " [155, 156],\n",
       " [38],\n",
       " [22, 27],\n",
       " [64, 67, 81],\n",
       " [66, 214, 215],\n",
       " [32, 33],\n",
       " [154, 155, 156],\n",
       " [15],\n",
       " [15],\n",
       " [15, 18],\n",
       " [16, 25],\n",
       " [19, 47],\n",
       " [15],\n",
       " [58],\n",
       " [23, 24],\n",
       " [24, 25],\n",
       " [15],\n",
       " [15, 16],\n",
       " [17],\n",
       " [16],\n",
       " [154],\n",
       " [154],\n",
       " [93, 113, 114, 115],\n",
       " [16],\n",
       " [94],\n",
       " [8],\n",
       " [154, 155],\n",
       " [1, 2, 154, 155],\n",
       " [154],\n",
       " [15, 155, 156],\n",
       " [16],\n",
       " [3, 6],\n",
       " [15],\n",
       " [154],\n",
       " [3, 5],\n",
       " [154],\n",
       " [3, 5],\n",
       " [25],\n",
       " [15],\n",
       " [13],\n",
       " [25],\n",
       " [16],\n",
       " [15, 16],\n",
       " [25, 64, 65],\n",
       " [15, 16],\n",
       " [154, 155],\n",
       " [154],\n",
       " [63, 64, 65],\n",
       " [85, 139],\n",
       " [15, 16],\n",
       " [15, 16],\n",
       " [3, 5],\n",
       " [25],\n",
       " [98, 99],\n",
       " [12, 13],\n",
       " [204],\n",
       " [13, 203, 22],\n",
       " [22],\n",
       " [22, 40],\n",
       " [40, 41],\n",
       " [110, 167],\n",
       " [29, 30],\n",
       " [38],\n",
       " [38],\n",
       " [18, 20],\n",
       " [22, 23],\n",
       " [197],\n",
       " [164],\n",
       " [18],\n",
       " [62, 65],\n",
       " [32, 33],\n",
       " [22, 23],\n",
       " [53, 61],\n",
       " [135],\n",
       " [10, 11],\n",
       " [32],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [17],\n",
       " [90],\n",
       " [30],\n",
       " [17],\n",
       " [107],\n",
       " [17],\n",
       " [7],\n",
       " [135]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fff72bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avani.gupta/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/avani.gupta/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_sid_= np.array(train_sid)\n",
    "train_sid_ = np.expand_dims(train_sid_, axis=1)\n",
    "\n",
    "test_sid_= np.array(test_sid)\n",
    "test_sid_ = np.expand_dims(test_sid_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ef9cb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sid_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "271032f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all = np.concatenate((train_encodings['input_ids'],train_sid_),axis=1)\n",
    "test_data_all = np.concatenate((test_encodings['input_ids'],test_sid_),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fc5a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "184ab8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0e41594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "\n",
    "def classify(classifier):\n",
    "    classifier.fit(train_encodings['input_ids'], train_labels_lb)\n",
    "    pred_lis = classifier.predict(test_encodings['input_ids'])\n",
    "    calc_metrics(pred_lis, test_labels_lb)\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "# classify(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2824321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "classifier = ClassifierChain(GaussianNB())\n",
    "# classify(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ebd4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = LabelPowerset(GaussianNB())\n",
    "# classify(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1091aae",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd0f7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_two_labels(data, labels):\n",
    "    '''\n",
    "    split two labels in 2 data-points\n",
    "    '''\n",
    "    labels_new = []\n",
    "    data_new = []\n",
    "    for i,lb in enumerate(labels):\n",
    "        if type(lb) == list: #needs to be split\n",
    "            for l in lb:\n",
    "                l = l.lower()\n",
    "                if '_' in l:\n",
    "                    l = l.split('_')[0]\n",
    "                else:\n",
    "                    l = l.split()[0]\n",
    "                if l=='results':\n",
    "                    l = 'result'\n",
    "                labels_new.append(l)\n",
    "                data_new.append(data[i])\n",
    "        else:\n",
    "            l = lb.lower()\n",
    "            if '_' in l:\n",
    "                    l = l.split('_')[0]\n",
    "            else:\n",
    "                l = l.split()[0]\n",
    "            if l=='results':\n",
    "                l = 'result'\n",
    "            labels_new.append(l)\n",
    "            data_new.append(data[i])\n",
    "    return data_new, labels_new\n",
    "\n",
    "train_data, train_labels = split_two_labels(train_data, train_labels)\n",
    "test_data, test_labels  = split_two_labels(test_data, test_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5018e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "# val_labels = le.transform(val_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f89ae6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 639)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a2b488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data,val_labels = get_dataset(val_docs,folder='From-ScisummNet-2019')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "# val_encodings = tokenizer(val_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "test_encodings = tokenizer(test_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9993322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels))\n",
    "test_dataset = IMDbDataset(test_encodings, torch.from_numpy(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d3022e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aim', 1: 'hypothesis', 2: 'implication', 3: 'method', 4: 'result'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_integer_mapping(le):\n",
    "    '''\n",
    "    Return a dict mapping labels to their integer values\n",
    "    from an SKlearn LabelEncoder\n",
    "    le = a fitted SKlearn LabelEncoder\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({le.transform([cl])[0]:cl})\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "mapping = get_integer_mapping(le)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0ec9a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aim': 0, 'hypothesis': 1, 'implication': 2, 'method': 3, 'result': 4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_mapping =  dict([(value, key) for key, value in mapping.items()])\n",
    "inv_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9518b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sep_class_classifiers():\n",
    "    for cl in mapping:\n",
    "#     cl = 2\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/avani.gupta/results'+str(cl),          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,              # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/avani.gupta/logs'+str(cl),            # directory for storing logs\n",
    "        logging_steps=20000,\n",
    "        save_steps = 30000\n",
    "        )\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        train_ind = np.where(train_labels==cl)[0].astype(int)\n",
    "        new_train_labels = np.zeros(train_labels.shape).astype(int)\n",
    "        new_train_labels[train_ind] = 1\n",
    "\n",
    "        # val_ind = np.where(val_labels==cl)[0].astype(int)\n",
    "        test_ind = np.where(test_labels==cl)[0].astype(int)\n",
    "        new_test_labels = np.zeros(test_labels.shape).astype(int)\n",
    "        new_test_labels[test_ind] = 1\n",
    "\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, torch.from_numpy(new_train_labels))\n",
    "        test_dataset = IMDbDataset(test_encodings, torch.from_numpy(new_test_labels))    \n",
    "\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated  Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset         # training dataset\n",
    "        )\n",
    "        trainer.train()\n",
    "        # eval\n",
    "        model.eval()\n",
    "        torch.save(model, \"bert_single_lb_classi\"+str(cl)+\".pth\")\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        pred_lis = []\n",
    "        gt_lis = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            pred_lis.extend(predictions.cpu().numpy())\n",
    "            gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "# train_sep_class_classifiers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26b1ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping  =  {0: 'aim', 1: 'hypothesis', 2: 'implication', 3: 'method', 4: 'result'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9538918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 22073,  8454,  ...,     0,     0,     0],\n",
       "        [  101,  6415,  4590,  ...,     0,     0,     0],\n",
       "        [  101,  2795,  2358,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3120,  2773,  ...,     0,     0,     0],\n",
       "        [  101,  6611,  7504,  ...,     0,     0,     0],\n",
       "        [  101,  3259,  6235,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3b68047",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_lis = test_encodings_orig['input_ids']\n",
    "token_type_ids_lis = test_encodings_orig['token_type_ids']\n",
    "attention_mask_lis = test_encodings_orig['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "544fc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for cl in mapping:\n",
    "    preds[cl] = []\n",
    "    device = 'cuda'\n",
    "    model = torch.load(\"bert_single_lb_classi\"+str(cl)+\".pth\").eval()\n",
    "    for i in range(len(input_ids_lis)):\n",
    "        model.to(device)\n",
    "        outputs = model(input_ids = input_ids_lis[i].unsqueeze(0).to(device),token_type_ids=token_type_ids_lis[i].unsqueeze(0).to(device),attention_mask=attention_mask_lis[i].unsqueeze(0).to(device))\n",
    "        preds[cl].append(torch.argmax(outputs.logits.cpu().detach(), dim=-1).numpy()[0])\n",
    "\n",
    "test_pred_lb= np.concatenate((np.expand_dims(np.array(preds[0]),1),np.expand_dims(np.array(preds[1]),1),np.expand_dims(np.array(preds[2]),1),np.expand_dims(np.array(preds[3]),1),np.expand_dims(np.array(preds[4]),1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4a497df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.6073619631901841 recall 0.6590909090909091, precision 0.7341772151898734, f1-score 0.6946107784431138\n"
     ]
    }
   ],
   "source": [
    "y = np.where((test_pred_lb == [0, 0, 0, 0, 0]).all(axis = 1))\n",
    "test_pred_lb[y]  = [0,0,0,1,0]\n",
    "calc_metrics(test_pred_lb,test_labels_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0aac3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels_lb))\n",
    "test_dataset = IMDbDataset(test_encodings, torch.from_numpy(test_labels_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07aa40d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bce74d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_classes=5):\n",
    "        super(Net,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',return_dict=True)\n",
    "        self.classifier=nn.Linear(self.bert.config.hidden_size,n_classes) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,input_ids, token_type_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "952b023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baa6b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdfd8720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "580b0ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56627af0d974deeaebf7c28ce2356a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avani.gupta/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/avani.gupta/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 197.2358829677105\n",
      "epoch 1 loss 197.27312152087688\n",
      "epoch 2 loss 197.20827310532331\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss = nn.BCELoss()\n",
    "model.train()\n",
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        ep_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'],batch['token_type_ids'], batch['attention_mask'])\n",
    "            l = loss(outputs,batch['labels'].float())\n",
    "            l.backward()\n",
    "            ep_loss += l.item()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        print(\"epoch {} loss {}\".format(epoch,ep_loss))\n",
    "        \n",
    "    torch.save(model,'multi_lb_bert.pth')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83e2b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('multi_lb_bert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce322a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "device = 'cuda'\n",
    "model.eval()\n",
    "for i in range(len(input_ids_lis)):\n",
    "    model.to(device)\n",
    "    outputs = model(input_ids = input_ids_lis[i].unsqueeze(0).to(device),token_type_ids=token_type_ids_lis[i].unsqueeze(0).to(device),attention_mask=attention_mask_lis[i].unsqueeze(0).to(device))\n",
    "    preds.append(outputs.cpu().detach().numpy()[0])\n",
    "\n",
    "# test_pred_lb= np.concatenate((np.expand_dims(np.array(preds[0]),1),np.expand_dims(np.array(preds[1]),1),np.expand_dims(np.array(preds[2]),1),np.expand_dims(np.array(preds[3]),1),np.expand_dims(np.array(preds[4]),1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0dce575",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf73775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[preds >= 0.6] = 1\n",
    "preds[preds < 0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c7ce0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cdc744b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.6441717791411042 recall 0.6748466257668712, precision 0.6962025316455697, f1-score 0.6853582554517134\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(preds,test_labels_lb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
