{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import ast\n",
    "import scipy\n",
    "from nltk import ngrams\n",
    "# from argparse import ArgumentParser\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument(\"-multi_lb_classi\", default=1, type=int, help=\"whether train a multi-lable classifier or individual classifiers for each class\")\n",
    "# opt = parser.parse_args()\n",
    "multi_lb_classi = 0\n",
    "\n",
    "\n",
    "def preprocess(example_sent):\n",
    "    global stop_words\n",
    "    word_tokens = word_tokenize(str(example_sent).lower())\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa06a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sent):\n",
    "    return list(ngrams(sent.split(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "975049bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_2018(files, folder):\n",
    "    pairs_lis = []\n",
    "    prev_corpus = []\n",
    "    docs_wise = {}\n",
    "    \n",
    "    for z,f in enumerate(files):\n",
    "        cit_text_lis = []\n",
    "        ref_text_lis = []\n",
    "        cit_off_lis = []\n",
    "        ref_off_lis = []\n",
    "        new_corpus = []\n",
    "\n",
    "        try:\n",
    "            a = folder+f+\"/Reference_XML/\"+f+\".xml\"\n",
    "            tree = ET.parse(a)\n",
    "            root = tree.getroot()\n",
    "            final =[]\n",
    "            total = len(root)\n",
    "            for a in root:\n",
    "#                 if a.text!='':\n",
    "#                     final.append(a.text)\n",
    "                for b in a:\n",
    "                    if b.text!='':\n",
    "                        final.append(b.text)\n",
    "                    \n",
    "            print(final)            \n",
    "            d={'col1':final}\n",
    "            rp = pd.DataFrame(data=d)\n",
    "            corpus = rp.col1\n",
    "            new_corpus = corpus.apply(lambda x: preprocess(x))\n",
    "\n",
    "            data = None\n",
    "            ann = None\n",
    "            a_folder = folder+f+\"/annotation/\"\n",
    "            file = os.listdir(a_folder)[0]\n",
    "\n",
    "            ann = a_folder+file\n",
    "            with open(ann,\"r\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "                cit_text = re.findall(\"Citation Text:\\s+([^|]*)\", data)\n",
    "                pattern = r'\\<.*?\\>'\n",
    "                pattern2 = r'\\(.*?\\)'\n",
    "                for c in cit_text:\n",
    "                    c = re.sub(pattern2,'',re.sub(pattern, '', c))\n",
    "                    c = preprocess(c)\n",
    "                    cit_text_lis.append(c)\n",
    "\n",
    "\n",
    "                ref_text = re.findall(\"Reference Text:\\s+([^|]*)\", data)\n",
    "                pattern = r'\\<.*?\\>'\n",
    "                pattern2 = r'\\(.*?\\)'\n",
    "                for ref in ref_text:\n",
    "                    ref = re.sub(pattern2,'',re.sub(pattern, '', ref))\n",
    "                    ref = preprocess(ref)\n",
    "                    ref_text_lis.append(ref)\n",
    "\n",
    "\n",
    "    #                 cit_off = re.findall(\"Citation Offset:\\s+([^|]*)\", data)\n",
    "    #                 for c in cit_off:\n",
    "    #                     c = ast.literal_eval(c)\n",
    "    #                     cit_off_lis.append(c)\n",
    "\n",
    "\n",
    "                ref_off = re.findall(\"Reference Offset:\\s+([^|]*)\", data)\n",
    "                for r in ref_off:\n",
    "                    r = ast.literal_eval(r)\n",
    "                    ref_off_lis.append(r)\n",
    "\n",
    "            \n",
    "#             new_corpus = np.array(new_corpus)\n",
    "#             updated_corpus = []\n",
    "#             for sent in new_corpus:\n",
    "#                 if sent!='':\n",
    "#                     updated_corpus.append(sent)\n",
    "                    \n",
    "          \n",
    "#             new_corpus = updated_corpus\n",
    "#             del updated_corpus\n",
    "            for i in range(len(cit_text_lis)):\n",
    "                temp = cit_text_lis[i]\n",
    "                citant_ngrams = get_ngrams(temp)\n",
    "    #                 reff_ = [int(i) for i in ref_off_lis[i]]\n",
    "    #                 cited_text_spans = new_corpus[reff_]\n",
    "    #                 cited_text_spans = pd.Series(cited_text_spans)\n",
    "    #                 cited_text_ngrams = cited_text_spans.apply(lambda x: get_ngrams(x))\n",
    "    #                 cited_text_ngrams_lis = []\n",
    "    #                 for i in range(len(cited_text_ngrams)):\n",
    "    #                     cited_text_ngrams_lis.extend(cited_text_ngrams[i])\n",
    "                for j in ref_off_lis[i]:\n",
    "                    j = int(j)\n",
    "                    if j< len(new_corpus):\n",
    "                        cited_text_spans = new_corpus[j]\n",
    "                        cited_text_ngrams =  get_ngrams(cited_text_spans)\n",
    "                        lis = np.intersect1d(list(citant_ngrams), list(cited_text_ngrams))\n",
    "                        lisone = np.intersect1d(temp, list(cited_text_spans))\n",
    "                        lis = \" \".join(lis) + \" \" + \" \".join(lisone)\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i]+\" sey \"+lis,new_corpus[int(j)]],label=1.0)) #positive pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i]+\" sey \"+lis,new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3)) #negative pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i]+\" sey \"+lis,new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i]+\" sey \"+lis,new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "    #                         pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "    #                         pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "\n",
    "            if (z!=0 and len(new_corpus)!=0 and len(prev_corpus)!=0):\n",
    "                pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "    #                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "    #                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "    #                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "\n",
    "            docs_wise[f] = {'corpus':new_corpus,  'cite_text':cit_text_lis, 'ref_off':ref_off_lis}\n",
    "            prev_corpus = new_corpus\n",
    "        except Exception as e: \n",
    "            print(f,e)\n",
    "#             exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "#             fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "#             print(exc_type, fname, exc_tb.tb_lineno)\n",
    "#             prev_corpus = []\n",
    "    return pairs_lis, docs_wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a47334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = os.listdir(\"/ssd_scratch/cvit/dhawals1939/scisumm-2018/Training\")\n",
    "\n",
    "# random.shuffle(docs)\n",
    "# train_e = int(docs)\n",
    "# val_e = int(0.1*len(docs)) + train_e\n",
    "train_rps = docs\n",
    "# val_rps = docs[train_e:val_e]\n",
    "# test_rps = docs[val_e:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "41bc4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A word in one language can be translated to zero, one, or several words in other languages.', 'Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation.', 'We built a fertility hidden Markov model by adding fertility to the hidden Markov model.', 'This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.', 'It is similar in some ways to IBM Model 4, but is much easier to understand.', 'We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.', 'IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).', 'There are three kinds of important information for word alignment models: lexicality, locality and fertility.', 'IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago.', 'Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4.', 'Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language.', 'However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box.', 'The complexity in IBM Model 4 makes it hard to understand and to improve.', 'Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.', 'We also want it to be accurate and computationally efficient.', 'There have been many years of research on word alignment.', 'Our work is different from others in essential ways.', 'Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.', 'Directly modeling fertility makes our model fundamentally different from others.', 'Most models have limited ability to model fertility.', 'Liang et al.', '(2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1.', 'ITG models (Wu, 1997) assume the fertility to be either zero or one.', 'It can model phrases, but the phrase has to be contiguous.', 'There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly.', 'Our model is a coherent generative model that combines the HMM and IBM Model 4.', 'It is easier to understand than IBM Model 4 (see Section 3).', 'Our model also removes several undesired properties in IBM Model 4.', 'We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 911 October 2010.', 'Qc 2010 Association for Computational Linguistics estimation.', 'Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993).', 'Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable.', 'Our model is much faster than IBM Model 4.', 'In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.', 'Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility.', 'Brown et al.', '(1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility.', 'If the optimal alignment is not in those neighbors, this method will not be able find the opti total of I + 1 empty words for the HMM model1.', 'Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1.', 'After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j → i, i = aj where j = 1, 2, . . .', ', J and i = 1, 2, . . .', ', 2I + 1.', 'Words from position I + 1 to 2I + 1 in the target sentence are all empty words.', 'We allow each source word to align with exactly one target word, but each target word may align with multiple source words.', 'The fertility φi of a word ei at position i is defined as the number of aligned source words: J mal alignment.', 'We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees.', 'DeNero et al.', '(2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.', 'where δ is the Kronecker delta function: ( 1 if x = y δ(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility.', 'the target sentence is \"£2I +1 \"£2I +1 φi. We define φǫ ≡ 2I +1 i=I +1 φi. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . .', ', fJ and a f J , we have \"£I φi + φǫ = J . target sentence eI 1 = e1, e2, . . .', ', eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions.', 'Following Brown et al.', '(1993), we assume that each source word is aligned to exactly one target word.', 'get sentence are a set Bi, such that each element in Bi is aligned with i, and all alignments of i are in Bi.', 'Inverted alignments are explicitly used in IBM Models 3, 4 and 5, but not in our model, which is We denote as aJ = a1, a2, . . .', ', aJ the alignments one reason that our model is easier to understand.', 'between f J and eI . When a word fj is not aligned 1 1 with any word e, aj is 0.', 'For convenience, we add an empty word ǫ to the target sentence at position 0 (i.e., e0 = ǫ).', 'However, as we will see, we have to add more than one empty word for the HMM.', '2.2 IBM Model 1 and HMM.', 'IBM Model 1 and the HMM are both generative models, and both start by defining the probability of alignments and source sentence given the In order to compute the “jump probability” in the target sentence: P (aJJ 1 ); the data likeli HMM model, we need to know the position of the 1 , f1 |e2I +1 hood can be computed by summing over alignments: aligned target word for the previous source word.', 'If the previous source word aligns to an empty word, 1 If fj.', '−1 does not align with an empty word and fj alignswe could use the position of the empty word to indi with an empty word, we want to record the position of the target word that fj−1 aligns with.', 'There are I + 1 possibilities: fj is cate the nearest previous source word that does not align to an empty word.', 'For this reason, we use a the first word in the source sentence, or fj the target word.', '−1 aligns with one ofP (f J |e2I +1) = \"£ J P (aJ , f J |e2I +1).', 'The alignwhere the first two equations imply that the proba 1 1 a1 1 1 1 ments aJ are the hidden variables.', 'The expectation maximization algorithm is used to learn the parameters such that the data likelihood is maximized.', 'Without loss of generality, P (aJ , f J |e2I +1) can bility of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a nonempty word is the same as the probability of jumping from the corespondent empty 1 1 1 be decomposed into length probabilities, distortion probabilities (also called alignment probabilities), and lexical probabilities (also called translation probabilities): P (aJ , f J |e2I +1) 1 1 1 J word.', 'The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): = P (J |e2I +1) n P (aj , fj |f j−1, aj−1, e2I +1) c(i − i′) 1 j=1 1 1 1 P (i|i′, I ) = \"£ i′′ c(i′′ − i′) J = P (J |e2I +1) n P (aj |f j−1, aj−1, e2I +1) × where c( ) is the count of jumps of a given distance.', '1 j=1 1 1 1 In IBM Model 1, the word order does not mat ter.', 'The HMM is more likely to align a source P (fj |f j−1, aj , e2I +1)l 1 1 1 where P (J |e2I +1) is a length probability, word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form (aj |f j−1, aj−1 2I +1P 1 1 , e1 ) is a distortion prob phrases.', 'ability and P (fj |f j probability.', '−1, aj , e 2I +1 1 ) is a lexical For these two models, in theory, the fertility for a target word can be as large as the length of the IBM Model 1 assumes a uniform distortion probability, a length probability that depends only on the length of the target sentence, and a lexical probability that depends only on the aligned target word: J source sentence.', 'In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004).', 'The HMM is P (aJ , f J |e2I +1) = P (J |I ) n P (f |e ) less likely to have this garbage collector problem be 1 1 1 (2I + 1)J j=1 j aj cause of the alignment probability constraint.', 'However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |aj−1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words.', 'This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models.', 'Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1.', 'Throughout the paper, “our model” refers to the fertility hidden Markov model.', 'Due to space constraints, we are unable to provide details for IBM Models 3, 4 and 5; see Brown et al.', '(1993) and Och and Ney (2003).', 'But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4.', 'IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy.', 'Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (φI , φǫ, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability.', 'IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn.', 'Our model has only one parameter for each target word, which can be learned more reliably.', 'In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (φI , φǫ, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I φi 1 λ(ei ) summing over fertilities and alignments: n λ(ei) e− × P (f J |e2I +1) = \"£ I J P (φI , φǫ, aJ , f J |e2I +1).', 'i=1 φi! 1 1 φ1 ,φǫ ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable φi, and we assume φi follows a Poisson distribution Poisson(φi; λ(ei)).', 'The sum of the fer (I λ(ǫ))φǫ e−(I λ(ǫ)) φǫ!', '× J tilities of all the empty words (φǫ) grows with the length of the target sentence.', 'Therefore, we assume that φǫ follows a Poisson distribution with parameter I λ(ǫ).', 'Now P (φI , φǫ, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (φI , φǫ, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (φI |e2I +1)P (φǫ|φI , e2I +1) × 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f j−1, aj−1, e2I +1, φI , φǫ) j=1 1 1 1 1 P (φI , φǫ, aJ , f J |e2I +1) = n λ(ei) e−λ(ei ) 1 1 1 I φ 1 λ(e ) φi! × = n λ(ei) i e− i i=1 (I λ(ǫ))φǫ e−I λ(ǫ) φǫ!', '× φ i=1 (I λ(ǫ))φǫ ! × e−(I λ(ǫ)) J n P (aj |f j−1, aj−1, e2I +1 I φǫ!', '× J j=1 1 1 1 , φ1 , φǫ) × n P (aj | j=1 aj−1 , I )P (fj | eaj ) (2) P (fj |f j−1, aj , e2I +1, φI , φǫ)l 1 1 1 1 Superficially, we only try to model the length 1 |e2I +1probability more accurately.', 'However, we also en When we compute P (f J 1 ), we only sum force the fertility for the same target word across the corpus to be consistent.', 'The expected fertility for a nonempty word ei is λ(ei), and the expected fertil over fertilities that agree with the alignments: ity for all empty words is I λ(ǫ).', 'Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), λ(e), ξ1(e) , ξ2(a )) 1 1 1 = P (φI , φǫ, aJ , f J |e2I +1) = P˜ ′ aJ e 2I +1, f J ) log ′ P (aJ , f J | e2I +1) 1 1 ,φǫ 1 1 1 1 1 1 J 1 1 1 1 ≈ P (φI , φǫ, aJ , f J |e2I +1) × − ξ1(e)( P (f |e) − 1) 1 1 1 1 I \\uf8eb J \\uf8f6 e f n δ \\uf8ed i=1 j=1 δ(aj , i), φi\\uf8f8 × − ξ2(a′)( a′ a P (a|a′) − 1) \\uf8eb 2I +1 J \\uf8f6 Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 δ \\uf8ed i=I +1 j=1 δ(aj , i), φǫ\\uf8f8 (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, φǫ and each P (f |e) = \"£s c (f |e; f (s), e(s)) (4) φi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments.', 'Because we only sum over fer tilities that are consistent with the alignments, we P (a|a′) = \"£s c (a|a′; f (s), e(s)) (5)have \"£f J P (f J |e2I +1) < 1, and our model is de \"£ \"£ a s c(a|a′; f (s), e(s)) 1 1 1 \"£ (s) (s) ficient, similar to IBM Models 3 and 4 (Brown et al., 1993).', 'We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion λ(e) = s c(φ| e; f , e ) s c(k|e; f (s), e(s)) (6) probability: the distortion probability is 0 if fertility where s is the number of bilingual sentences, andis not consistent with alignments, and uniform oth c(f |e; f J 2I +1 ˜ J J 2I +1 erwise.', 'The total number of consistent fertility and 1 , e1 ) = P (a1 |f1 , e1 ) × J alignments is J ! .', 'Replacing 1 with a1 φǫ ! J i ! φǫ ! J i !', '(2I +1)J δ(fj , f )δ(ei, e) J ! , we have: c(a|a′; f J , e2I +1) = j P˜(aJ |f J , e2I +1) × P (φI , φǫ, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n λ(ei)φi e−λ(ei ) × i=1 (I λ(ǫ))φǫ e−(I λ(ǫ)) × c(φ|e; f1 , e1 ) = δ(aj , a)δ(aj−1, a′) j P˜(a1 |f1 , e1 ) × J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 φ δ(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)δ(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency.', 'We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model.', 'For the fertility IBM Model 1, we do not need to estimate the distortion probability.', 'Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).', 'The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.', 'We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts.', '1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.', 'During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, · · · aj−1, aj+1 · · · aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, · · · , aj 1, a , · · · , a , f J , e2I +1) − j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end \"£ J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples.', 'We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6.', 'This Gibbs sampling method updates parameters constantly, so it is an “online learning” algorithm.', 'However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.', 'Instead, we do “batch learning”: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).', 'This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step.', 'In this case, the fertility hidden Markov model is not faster than the HMM.', 'Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm.', 'In the testing stage, the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P (a1 , f1 |e2I +1).', 'We need more the HMM do in the EM algorithms.', 'The algorithm aJ J J 1 for the E-step on one machine (all machines are independent) is in Algorithm 1.', 'For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . .', ', J ) is O(tI J ).', 'This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant.', 'Surprisingly, we can achieve better results than the HMM by computing as few as 1 sample for each alignment, so the fertility hidden Markov model is much faster than the HMM.', 'Even when choosing t such that our model is 5 times faster than the HMM, we achieve better results.', '2 For fertility IBM Model 1, we only need to compute I + 1.', 'values because e2I +1 are identical empty words.', 'samples in the testing stage because it is unlikely to get to the optimal alignments by sampling a few times for each alignment.', 'On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level.', 'Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster.', 'Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing.', 'Gibbs sampling for the fertility IBM Model 1 is similar but simpler.', 'We omit the details here.', 'Al ig n m en t M o d e l P R A E R e n → c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7.', '8 4 3.', '8 3 9.', '0 3 7.', '9 3 6.', '2 3 4.', '9 3 4.', '5 c n → e n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 52 .6 55 .9 66 .1 68 .6 71 .1 71 .1 69 .3 53 .7 56 .4 62 .1 60 .2 62 .2 62 .7 68 .5 4 6.', '9 4 3.', '9 3 5.', '9 3 5.', '7 3 3.', '5 3 3.', '2 3 1.', '1 Table 1: AER results.', 'IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM.', 'We choose t = 1, 5, and 30 for the fertility HMM.', '0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (en→cn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn →en) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison.', 'The training time for each model is calculated from scratch.', 'For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.', 'We evaluated our model by computing the word alignment and machine translation quality.', 'We use the alignment error rate (AER) as the word alignment evaluation criterion.', 'Let A be the alignments output by word alignment system, P be a set of possible alignments, and S be a set of sure alignments both labeled by human beings.', 'S is a subset of P . Precision, recall, and AER are defined as follows: recall = |A ∩ S| |S| precision = |A ∩ P | |A| AER(S, P, A) = 1 |A ∩ S| + |A ∩ P | |A| + |S| AER is an extension to F-score.', 'Lower AER is better.', 'We evaluate our fertility models on a ChineseEnglish corpus.', 'The ChineseEnglish data taken from FBIS newswire data, and has 380K sentence pairs, and we use the first 100K sentence pairs as our training data.', 'We used hand-aligned data as reference.', 'The ChineseEnglish data has 491 sentence pairs.', 'We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution.', 'We smooth all parameters (λ(e) and P (f |e)) by adding a small value (10−8), so they never become too small.', 'We run both models for 5 iterations.', 'AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm.', 'We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1.', 'We smooth all parameters (λ(e), P (a|a′) and P (f |e)) by adding a small value (10−8).', 'We run both models for 5 iterations.', 'AER results are computed using traditional HMM Viterbi decoding for both models.', 'It is always difficult to determine how many samples are enough for sampling algorithms.', 'However, both fertility models achieve better results than their baseline models using a small amount of samples.', 'For the fertility IBM Model 1, we sample 10 times for each aj , and restart 3 times in the training stage; we sample 100 times and restart 12 times in the testing stage.', 'For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing.', 'More samples give no further improvement.', 'Initially, the fertility IBM Model 1 and fertility HMM did not perform well.', 'If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter λ(e).', 'Hence, smoothing is needed.', 'One may try to solve it by forcing all these words to share a same parameter λ(einfrequent).', 'Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should.', 'We solve the problem in the following way: estimate the parameter λ(enon empty ) for all nonempty words, all infrequent words share this parameter.', 'We consider words that appear less than 10 times as infrequent words.', 'Table 1, Figure 1, and Figure 2 shows the AER results for different models.', 'We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM.', 'The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM.', 'Figure 3 show the training time for different models.', 'In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM.', 'It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase).', 'We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model.', 'We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007).', 'The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic.', 'Our test is 633 sentences of up to length 50, with four references.', 'Results are shown in Table 2; we see that better word alignment results do not lead to better translations.', 'Model BLEU HMM 19.55 HMMF30 19.26 IBM4 18.77 Table 2: BLEU results', 'We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM.', 'Our model is thus much faster than IBM Model 4.', 'Our model is also easier to understand than IBM Model 4.', 'The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.', 'While better word alignment results do not necessarily correspond to better translation quality, our translation results are comparable in translation quality to both the HMM and IBM Model 4.', 'Acknowledgments We would like to thank Tagyoung Chung, Matt Post, and the anonymous reviewers for helpful comments.', 'This work was supported by NSF grants IIS0546554 and IIS0910611.']\n",
      "['We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', 'It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', 'In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.', 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tˆN = tˆ1, ..., tˆN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.', 'Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.', 'For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.', 'The additional information may also help to disambiguate the (base) part of speech.', 'Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das Realita¨ t?', '(Is that reality?).', 'The word das is ambiguous between an article and a demonstrative.', 'Because of the lack of gender agreement between das (neuter) and the noun Realita¨ t (feminine), the article reading must be wrong.', 'The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', 'Large tagsets aggravate sparse data problems.', 'As an example, take the German sentence Das zu versteuernde Einkommen sinkt (“The to be taxed income decreases”; The tˆN N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).', 'This sentence should be tagged as shown in table 1.', 'Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p(tN , wN ) = n 1 1 i=1 p(ti|ti−1 ) i−k p(wi|ti) le .', '(1) context prob.', 'xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora.', 'Qc 2008.', 'Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).', 'Some rights reserved.', 'Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.', 'Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.', '(Neither does the pair consisting of the first two tags.)', 'The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777–784 Manchester, August 2008 context probability of the third POS tag is therefore 0.', 'If the probability is smoothed with the backoff distribution p(•|P ART .Z u), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut.', 'Thus, the agreement between the article and the adjective is not checked anymore.', 'A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: • All words appearing after an article (ART) and the infinitive particle zu (PART.zu) are attributive adjectives (ADJA) (10 of 10 cases).', '• All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).', '• All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases).', '• All adjectives appearing after a singular article and a particle are singular (32 of 32 cases).', '• All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases).', 'By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) ∗ p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) ∗ p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) ∗ p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) ∗ p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the ∗ p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) ∗ p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1.', 'Hence the context probability of the whole tag is. also 1.', 'Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context.', 'These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.', 'Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method.', 'The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.', 'Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.', 'Decision trees (Breiman et al., 1984; Quinlan, 1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as attribute vectors.', 'The non-terminal nodes are labeled with attribute tests, the edges with the possible outcomes of a test, and the terminal nodes are labeled with classes.', 'An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object.', 'Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class.', 'Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.', '2.1 Induction of Decision Trees.', 'Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class.', 'This test, which maximizes the information gain1 wrt.', 'the class, is following expression for the context probability: 1 The information gain measures how much the test de-.', 'p(ADJA | ART, PART.Zu) ∗ p(Pos | 2:ART, 1:PART, 0:ADJA) ∗ p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) creases the uncertainty about the class.', 'It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy yes 0:N.Name yes no 1:ART.Nom no 1:ADJA.Nom yes no which returns a probability of 0.3.', 'The third tree for neuter has one non terminal and two terminal nodes returning a probability of 0.3 and 0.5, respectively.', 'The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1.', 'This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities.', 'The probability of an attribute (such as “Nom”) is always conditioned on the respective base POS (such as “N”) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.', 'The test 1:ART.Nom checks if the preceding word is a nominative article.', 'assigned to the top node.', 'The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class.', 'In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data.', 'Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.', 'The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not.', 'The best test for each node is selected with the standard information gain criterion.', 'The recursive tree building process terminates if the information gain is 0.', 'The decision tree is pruned with the pruning criterion described below.', 'Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1.', 'To understand why, assume that there are three trees for the gender attributes.', 'Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS.', 'All context attributes other than the base POS are always used in combination with the base POS.', 'A typical context attribute is “1:ART.Nom” which states that the preceding tag is an article with the attribute “Nom”.', '“1:ART” is also a valid attribute specification, but “1:Nom” is not.', 'The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k ≥ 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.', 'This restriction improved the tagging accuracy for large contexts.', '2.2 Pruning Criterion.', 'The tagger applies3 the critical-value pruning strategy proposed by (Mingers, 1989).', 'A node is pruned if the information gain of the best test multiplied by the size of the data subsample is below a given threshold.', 'To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D1 (with 20 positive and 20 negative elements) and D2 (with 30 positive and 5 negative elements) are the two subsets induced by the best test.', 'The entropy of D is −2/3 log22/3 − 1/3 log21/3 = 0.92, the entropy of D1 is −1/2 log21/2−1/2 log21/2 = 1, and the entropy of D2 is −6/7 log26/7 − 1/7 log21/7 = 0.59.', 'The information gain is therefore 0.92 − (8/15 ∗ 1 − 7/15 ∗ 0.59) = 0.11.', 'The resulting score is 75 ∗ 0.11 = 8.25.', 'Given a threshold of 6, the node is therefore not pruned.', 'We experimented with pre-pruning (where a node is always pruned if the gain is below the in the two subsets.', 'The weight of each subset is proportional to its size.', '2 We did not directly compare the two alternatives (two- valued vs. multi-valued tests), because the implementational effort required would have been too large.', '3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size.', 'Thus, the simpler pruning strategy presented here was chosen.', 'threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes).', 'The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold.', 'A threshold of 6 consistently produced optimal or near optimal results for pre-pruning.', 'Thus, pre-pruning with a threshold of 6 was used in the experiments.', 'The tagger treats dots in POS tag labels as attribute separators.', 'The first attribute of a POS tag is the main category.', 'The number of additional attributes is fixed for each main category.', 'The additional attributes are category-specific.', 'The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.', 'The attributes occurring at a certain position constitute the value set of the feature.', 'Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.', 'The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|ti−1 ) is internally computed as a product of attribute probabili ties.', 'In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.', 'With athreshold of 10−3 or lower, the influence of prun ing on the tagging accuracy was negligible.', '4.1 Supplementary Lexicon.', 'The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.', 'If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags.', 'The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.', 'If the word w was observed with N different tags, and f (w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: f (w, t) + N p(t|[w]) the preceding attributes of the predicted POS tag is estimated with a decision tree as described be p(t|w) = f (w) + N fore.', 'The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way).', 'The smoothing is implemented by adding the weighted class probabilities pp(c) of the parent node to the frequencies f (c) before normalizing them to probabilities: p(c) = f (c) + αpp(c) α + �c f (c) The weight α was fixed to 1 after a few experiments on development data.', 'This smoothing strategy is closely related to Witten-Bell smoothing.', 'The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1).', 'The best tag sequence is computed with the Viterbi algorithm.', 'The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words.', 'The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words.', 'The tagger builds a suffix trie for each class of unknown words using the known word types from that class.', 'The maximal length of the suffixes is 7.', 'The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.', 'Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences.', '6 In earlier experiments, we had used a much larger number of word classes.', 'Decreasing their number to 4 turned out to be better.', 'a threshold of 1.', 'More precisely, if Tα is the set of POS tags that occurred with suffix α, |T | is the size of the set T , fα is the frequency of suffix α, and pα(t) is the probability of POS tag t among the words with suffix α, then the following condition must hold: tion between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the Tiger treebank annotates with “$(”.', 'A supplementary lexicon was created by analyzing a word list which included all words from the faα paα (t) log paα(t) < 1 training, development, and test data with a German computationa l morphology.', 'The analyses gener |Taα| t∈Taα pα(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.', 'Our tagger was first evaluated on data from the German Tiger treebank.', 'The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (Gime´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.', 'Therefore it was not possible to optimize the parameters systematically.', 'We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (Hladka´ et al., 2007) and compared to the TnT tag- ger.', '5.1 Tiger Corpus.', 'The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.', 'It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', 'After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left.', 'The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.', 'Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.', 'Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.', 'Some lexically decidable distinctions missing in the Tiger corpus have been tagset.', 'Note that only the words, but not the POS tags from the test and development data were used, here.', 'Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed.', 'In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.', 'This strategy returned the best results on the development data.', 'In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon.', '5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase.', 'In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).', 'Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).', 'The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).', 'For evaluation purposes, the refined tags are mapped back to the original tags.', 'This mapping is unambiguous.', '7 It was planned to include also the Stanford tagger.', '(Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data.', '8 In German, the genitive case of arguments is more and.', 'more replaced by the dative.', 'Table 2: Tagging accuracies on development data in percent.', 'Results for 2 and for 10 preceding POS tags as context are reported for our tagger.', 'much smaller.', 'Table 3 shows the results of an evaluation based on the plain STTS tagset.', 'The first result was obtained with TnT trained on Tiger data which was mapped to STTS before.', 'The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS.', 'The third row gives the corresponding figures for our tagger.', '5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.', 'The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.', 'The TnT tagger achieves 86.3% accuracy on the default tagset.', 'A tag is considered correct if all attributes are correct.', 'The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.', 'The SVMTool is slightly better than the TnT tagger on the default tagset, but shows little improvement from the tagset refinement.', 'Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement.', 'With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon.', 'A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.', 'de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7.', '2 8 9 7.', '1 7 97.26 97.51 9 7.', '3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.', 'These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features.', 'This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.', 'Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.', 'The best results are obtained with a context size of 10.', 'What type of information is relevant across a distance of ten words?', 'A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.', 'Since German is a verb-final language, these tests clearly make sense.', 'Table 4 shows the performance on the test data.', 'Our tagger was used with a context size of 10.', 'The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon.', 'These values were optimal on the development data.', 'The accuracy of our tagger is lower than on the development data.', 'This could be due to the higher rate of unknown words (10.0% vs. 7.7%).', 'Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.', 'The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3.', '4 5 84.11 89.14 8 5.', '0 0 85.92 91.07 Table 4: Tagging accuracies on test data.', 'By far the most frequent tagging error was the confusion of nominative and accusative case.', 'If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.', 'The resulting score of a binomial test is below 0.001.', 'this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.', 'Our tagger is quite fast, although not as fast as the TnT tagger.', 'With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a computer with an Athlon X2 4600 CPU.', 'The training with a context size of 10 took about 4 minutes.', '5.2 Czech Academic Corpus.', 'We also evaluated our tagger on the Czech Academic corpus (Hladka´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.', 'The data was divided into 80% training data, 10% development data and 10% test data.', '89 88.9 88.8 Provost & Domingos (2003) noted that well- known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al., 1984) fail to produce accurate probability estimates.', 'They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction).', 'Ferri et al.', '(2003) describe a more complex backoff smoothing method.', 'Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).', 'Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes.', 'These two-class trees can be pruned with a fixed pruning threshold.', 'Hence there is no need to put aside training data for parameter tuning.', '88.7 88.6 88.5 ’ c o n t e x t d a t a 2 ’ 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool (or oth er dis cri min ativ ely trai ned tag ger s) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size wasFigure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags.', 'The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5.', 'The corresponding figures for the test data are.', '89.53% for our tagger and 88.88% for the TnT tag- ger.', 'The difference is significant.', 'Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees.', 'A similar idea was previously presented in Kempe (1994), but apparently never applied again.', 'The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.', 'Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.', 'Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.', 'Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.', 'used.', 'We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size.', 'Czech POS tagging has been extensively studied in the past (Hajicˇ and Vidova´-Hladka´, 1998; Hajicˇ et al., 2001; Votrubec, 2006).', 'Spoustov et al.', '(2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (Morcˇe), and evaluated them on the Prague Dependency Treebank (PDT 2.0).', 'Morcˇe’s tagging accuracy was 95.12%, 0.3% better than the n-gram tagger.', 'A hybrid system based on four different tagging methods reached an accuracy of 95.68%.', 'Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult.', 'Furthermore, our tagger uses no corpus-specific heuristics, whereas Morcˇe e.g. is optimized for Czech POS tagging.', 'The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.', 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.', 'In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).']\n",
      "['Recently, confusion network decoding has been applied in machine translation system combination.', 'Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs.', 'This paper describes an improved confusion network based method to combine outputs from multiple MT systems.', 'In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring.', 'Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed.', 'A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR.', 'The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.', 'System combination has been shown to improve classification performance in various tasks.', 'There are several approaches for combining classifiers.', 'In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990).', 'Another approach is to combine outputs from a few highly specialized classifiers.', 'The classifiers may 312 be based on the same basic modeling techniques but differ by, for example, alternative feature representations.', 'Combination of speech recognition outputs is an example of this approach (Fiscus, 1997).', 'In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination.', 'Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems.', 'The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994).', 'Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001).', 'To generate confusion networks, hypotheses have to be aligned against each other.', 'In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network.', 'As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses.', 'In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003).', 'The size of the test set may influence the quality of these alignments.', 'Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments.', 'A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007.', 'Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007).', 'The alignments from TER are consistent as they do not depend on the test set size.', 'Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005).', 'A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering.', 'Confusion networks are generated by choosing one hypothesis as the “skeleton”, and other hypotheses are aligned against it.', 'The skeleton defines the word order of the combination output.', 'Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007).', 'The average TER score was computed between each system’s -best hypothesis and all other hypotheses.', 'The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER.', 'This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences.', 'However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton.', 'In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses.', 'All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights.', 'The combination outputs from confusion network decoding may be ungrammatical due to alignment errors.', 'Also the word-level decoding may break coherent phrases produced by the individual systems.', 'In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences.', 'This allows a log-linear addition of arbitrary features such as language model (LM) scores.', 'The LM scores should increase the total log-posterior of more grammatical hypotheses.', 'Powell’s method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set.', 'Tuning is fully automatic, as opposed to (Matusov et al., 2006) where global system weights were set manually.This paper is organized as follows.', 'Three evalu ation metrics used in weights tuning and reporting the test set results are reviewed in Section 2.', 'Section 3 describes confusion network decoding for MT system combination.', 'The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively.', 'Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7.', 'Conclusions and future work are discussed in Section 8.', 'Currently, the most widely used automatic MT evaluation metric is the NIST BLEU4 (Papineni et al., 2002).', 'It is computed as the geometric mean of - gram precisions up to -grams between the hypothesis and reference as follows (1) where is the brevity penalty and are the -gram precisions.', 'When mul tiple references are provided, the -gram counts against all references are accumulated to compute the precisions.', 'Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs.', 'The BLEU scores are between and , higher being better.', 'Often BLEU scores are reported as percentages and “one BLEU point gain” usually means a BLEU increase of . Other evaluation metrics have been proposed to replace BLEU.', 'It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie, 2005).', 'METEOR is based on the weighted harmonic mean of the precision and recall measured on uni- gram matches as follows (2) where is the total number of unigram matches, is the hypothesis length, is the reference length and is the minimum number of -gram matches that covers the alignment.', 'The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to when ; i.e., there are no matching -grams higher than . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer.', 'Additional matching modules including WordNet stemming and synonymy may also be used.', 'When multiple references are provided, the lowest score is reported.', 'Full test set scores are obtained by accumulating statistics over all test sentences.', 'The METEOR scores are also between and , higher being better.', 'The scores in the results section are reported as percentages.', 'Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1.', 'Each arc represents an alternative word at that.', 'position in the sentence and the number of votes for each word is marked in parentheses.', 'Confusion network decoding usually requires finding the path with the highest confidence in the network.', 'Based on vote counts, there are three alternatives in the example: “cat sat on the mat”, “cat on the mat” and “cat sitting on the mat”, each having accumulated 10 votes.', 'The alignment procedure plays an important role, as by switching the position of the word ‘sat’ and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, “cat on the mat”.', 'ric since it is based on the rate of edits required to transform the hypothesis into the reference.', 'The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length.', 'The only difference to word error rate is that the TER allows shifts.', 'A shift of a sequence of words is counted as a single edit.', 'The minimum translation edit alignment is usually found through a beam search.', 'When multiple references are provided, the edits from the closest reference are divided by the average reference length.', 'Full test set scores are obtained by accumulating the edits and the average reference lengths.', 'The perfect TER score is 0, and otherwise higher than zero.', 'The TER score may also be higher than 1 due to insertions.', 'Also TER is reported as a percentage in the results section.', 'Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination.', 'The other hypotheses are aligned against the skeleton.', 'Either votes or some form of confidences are assigned to each word in the network.', 'For example using “cat sat the mat” as the skeleton, aligning “cat sitting on the mat” and “hat on a mat” against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word.', 'In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs.', 'Different alignment methods yield different confusion networks.', 'The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning.', 'As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton.', 'Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score.', 'The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems.', 'This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007).', 'Other evaluation metrics may also be used as the MBR loss function.', 'For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007).', 'When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis.', 'In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis.', 'Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned.', 'Similar approach to estimate word posteriors is adopted in this work.', 'System weights may be used to assign a system specific confidence on each word in the network.', 'The weights may be based on the systems’ relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set.', 'In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding.', 'For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output.', 'Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores.', 'Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding.', 'Third, the system weights are independent of the skeleton selection.', 'Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton.', 'Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified.', 'Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network.', 'If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty.', 'Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights.', '3.1 Discussion.', 'There are several problems with the previous confusion network decoding approaches.', 'First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one.', 'Equation 6 may be viewed as a log-linear sum of sentence- level features.', 'The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score.', 'The last two terms are not completely independent but seem to help based on experimental results.', 'The number of paths through a confusion network grows exponentially with the number of nodes.', 'Therefore expanding a network with an -gram language model may result in huge lattices if is high.', 'Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice.', 'After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized.', 'On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice.', 'This -best list is then re-scored with the higher order -gram.', 'The second set of weights is used to find the final -best from the re-scored -best list.', 'As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation.', 'To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network.', 'All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network.', 'All confusion network are connected to a common end node with NULL arcs.', 'The final arcs have a probability of one.', 'The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen.', 'The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities.', 'These log-probabilities are scaled so that the priors sum to one.', 'There is a concern that the prior probabilities estimated this way may be inaccurate.', 'Therefore, the priors may have to be smoothed by a tunable exponent.', 'However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors.', 'Thus, no smoothing was used in the experiments presented later in this paper.', 'An example joint network with the priors is shown in Figure 2.', 'This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006).', 'However, this approach did not include sentence 1 Na.', 'Figure 2: Three confusion networks with prior probabilities.', 'specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights.', 'The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991).', 'A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features.', 'The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations.', 'For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used.', 'In this work, modified Powell’s method as proposed by (Brent, 1973) is used.', 'The algorithm explores better weights iteratively starting from a set of initial weights.', 'First, each dimension is optimized using a grid-based line minimization algorithm.', 'Then, a new direction based on the changes in the objective function is estimated to speed up the search.', 'To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs.', 'Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration.', 'Similarly, weights which maximize BLEU or METEOR may be optimized.', 'The same Powell’s method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003).', 'A more efficient algorithm for log-linear models was also proposed.', 'In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used.', 'The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks.', 'Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006).', 'All systems were trained on the same data and the outputs used the same tokenization.', 'The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU.', 'All decoder weight tuning was done on the NIST MT02 task.', 'The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration.', 'The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks.', 'All four reference translations available for the tuning and test sets were used.', 'A first set of weights with the bi- gram LM was optimized with three iterations.', 'A second set of weights was tuned for 5-gram -best list re-scoring.', 'The bi-gram and 5-gram English language models were trained on about 7 billion words.', 'The final combination outputs were detokenized and cased before scoring.', 'The tuning set results on the Arabic to English NIST MT03+MT04 task are shown in Table 1.', 'The Ar ab ic tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 44 .9 3 46 .4 1 46 .1 0 44 .3 6 45 .3 5 47 .1 0 45 .7 1 43 .0 7 46 .4 1 46 .8 3 45 .4 4 44 .5 2 66 .0 9 64 .7 9 65 .3 3 66 .9 1 65 .6 9 65 .2 8 no we ig hts ba sel in e 42 .3 5 42 .1 9 48 .9 1 49 .8 6 67 .7 6 68 .3 4 T E R t u n e d B L E U t u n e d M T R t u n e d 41 .8 8 42 .1 2 54 .0 8 51 .4 5 51 .7 2 38 .9 3 68 .6 2 68 .5 9 71 .4 2 Table 1: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT03+MT04.', 'Ar ab ic tes t T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 42 .9 8 43 .7 9 43 .9 2 40 .7 5 42 .1 9 44 .3 0 49 .5 8 47 .0 6 47 .8 7 52 .0 9 50 .8 6 50 .1 5 69 .8 6 68 .6 2 66 .9 7 71 .2 3 70 .0 2 69 .7 5 no we ig hts ba sel in e 39 .3 3 39 .2 9 53 .6 6 54 .5 1 71 .6 1 72 .2 0 T E R t u n e d B L E U t u n e d M T R t u n e d 39 .1 0 39 .1 3 51 .5 6 55 .3 0 55 .4 8 41 .7 3 72 .5 3 72 .8 1 74 .7 9 Table 2: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT05.', 'best score on each metric is shown in bold face fonts.', 'The row labeled as no weights corresponds to Equation 5 with uniform system weights and zero NULL weight.', 'The baseline corresponds to Equation 5 with TER tuned weights.', 'The following three rows correspond to the improved confusion network decoding with different optimization metrics.', 'As expected, the scores on the metric used in tuning are the best on that metric.', 'Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning.', 'However, the METEOR tuning yields extremely high TER and low BLEU scores.', 'This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04.', 'score.', 'Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision.', 'The Arabic test set results are shown in Table 2.', 'The TER and BLEU optimized combination results beat all single system scores on all metrics.', 'The best results on a given metric are again obtained by the combination optimized for the corresponding metric.', 'It should be noted that the TER optimized combination has significantly higher BLEU score than the TER optimized baseline.', 'Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points.', 'Also, the METEOR score using the METEOR optimized weights is very high.', 'However, the other scores are worse in common with the tuning set results.', 'The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3.', 'The baseline combination weights were tuned to optimize BLEU.', 'Again, the best scores on each metric are obtained by the combination tuned for that metric.', 'Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations.', 'The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05.', 'terms of METEOR.', 'Compared to the baseline, the BLEU score of the BLEU tuned combination is improved by 1.47 points.', 'Again, the METEOR tuned weights hurt the other metrics significantly.', 'An improved confusion network decoding method combining the word posteriors with arbitrary features was presented.', 'This allows the addition of language model scores by expanding the lattices or re-scoring -best lists.', 'The LM integration should result in more grammatical combination outputs.', 'Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores.', 'This guarantees that the best path will not be found from a network generated for a system with zero weight.', 'Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights.', 'The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks.', 'Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly.', 'The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR.', 'The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese.', 'It also seems like METEOR should not be used in tuning due to high insertion rate and low precision.', 'It would be interesting to know which tuning metric results in the best translations in terms of human judgment.', 'However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al., 2006).', 'The improved confusion network decoding approach allows arbitrary features to be used in the combination.', 'New features may be added in the future.', 'Hypothesis alignment is also very important in confusion network generation.', 'Better alignment methods which take synonymy into account should be investigated.', 'This method could also benefit from more sophisticated word posterior estimation.', 'This work was supported by DARPA/IPTO Contract No.', 'HR001106-C-0022 under the GALE program (approved for public release, distribution unlimited).', 'The authors would like to thank ISI and University of Edinburgh for sharing their MT system outputs.']\n",
      "['Automatic paraphrase discovery is an important but challenging task.', 'We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.', 'We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', 'The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets.', 'The second stage links sets which involve the same pairs of individual NEs.', 'A total of 13,976 phrases were grouped.', 'The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%.', 'One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event.', 'If the expression is a word or a short phrase (like “corporation” and “company”), it is called a “synonym”.', 'There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.', 'If the expression is longer or complicated (like “A buys B” and “A’s purchase of B”), it is called “paraphrase”, i.e. a set of phrases which express the same thing or event.', 'Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.', 'For example, in Information Retrieval (IR), we have to match a user’s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the user’s question even if the formulation of the answer in the document is different from the question.', 'Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.', 'We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.', 'For example, we can easily imagine that the number of paraphrases for “A buys B” is enormous and it is not possible to create a comprehensive inventory by hand.', 'Also, we don’t know how many such paraphrase sets are necessary to cover even some everyday things or events.', 'Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.', 'So, there is a limitation that IE can only be performed for a predefined task, like “corporate mergers” or “management succession”.', 'In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.', 'So, it is too costly to make IE technology “open- domain” or “on-demand” like IR or QA.', 'In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.', 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.', '2.1 Overview.', 'Before explaining our method in detail, we present a brief overview in this subsection.', 'First, from a large corpus, we extract all the NE instance pairs.', 'Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, “IBM plans to acquire Lotus”.', 'For each pair we also record the context, i.e. the phrase between the two NEs (Step1).', 'Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.', 'We use a simple TF/IDF method to measure the topicality of words.', 'Hereafter, each pair of NE categories will be called a domain; e.g. the “Company – Company” domain, which we will call CC- domain (Step 2).', 'For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3).', 'Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link “IBM” and “Lotus”) (Step 4).', 'As we shall see, most of the linked sets are paraphrases.', 'This overview is illustrated in Figure 1.', 'Corpus Step 1 NE pair instances Step 2 Step 1.', 'Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.', 'The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.', 'The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].', 'These 140 NE categories are designed by extending MUC’s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).', 'All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the ‘context’).', 'Figure 2 shows examples of extracted NE pair instances and their contexts.', 'The data is sorted based on the frequency of the context (“a unit of” appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. “NBC” and “General Electric Co.” appeared 10 times with the context “a unit of”).', 'Step 2.', 'Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word “unit” for the phrase “a unit of”).', 'Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword.', 'We used the TF/ITF metric to identify keywords.', 'keywords Step 3 Sets of phrases based on keywords Step 4 Links between sets of phrases All the contexts collected for a given domain are gathered in a bag and the TF/ITF scores are calculated for all the words except stopwords in the bag.', 'Here, the term frequency (TF) is the frequency of a word in the bag and the inverse term frequency (ITF) is the inverse of the log of the frequency in the entire corpus.', 'Figure 3 Figure 1.', 'Overview of the method 2.2 Step by Step Algorithm.', 'In this section, we will explain the algorithm step by step with examples.', 'Because of their size, the examples (Figures 2 to 4) appear at the end of the paper.', 'shows some keywords with their scores.', 'Step 3.', 'Gather phrases using keywords Next, we select a keyword for each phrase – the top-ranked word based on the TF/IDF metric.', '(If the TF/IDF score of that word is below a threshold, the phrase is discarded.)', 'We then gather all phrases with the same keyword.', 'Figure 4 shows some such phrase sets based on keywords in the CC-domain.', 'Step 4.', 'Cluster phrases based on Links We now have a set of phrases which share a keyword.', 'However, there are phrases which express the same meanings even though they do not share the same keyword.', 'For example, in Figure 3, we can see that the phrases in the “buy”, “acquire” and “purchase” sets are mostly paraphrases.', 'At this step, we will try to link those sets, and put them into a single cluster.', 'Our clue is the NE instance pairs.', 'If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.', 'For example, the two NEs “Eastern Group Plc” and “Hanson Plc” have the following contexts.', 'Here, “EG” represents “Eastern Group Plc”.', 'and “H” represents “Hanson Plc”.', 'x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x H’s agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above.', 'So, we set a threshold that at least two examples are required to build a link.', 'More examples are shown in Figure 5.', 'Notice that the CC-domain is a special case.', 'As the two NE categories are the same, we can’t differentiate phrases with different orders of par ticipants – whether the buying company or the to-be-bought company comes first.', 'The links can solve the problem.', 'As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.', 'In figure 4, reverse relations are indicated by `*’ next to the frequency.', 'Now we have sets of phrases which share a keyword and we have links between those sets.', '3.1 Corpora.', 'For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995.', 'They contain about 200M words (25M, 110M, 40M and 19M words, respectively).', 'All the sentences have been analyzed by our chunker and NE tag- ger.', 'The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory.', '3.2 Results.', 'In this subsection, we will report the results of the experiment, in terms of the number of words, phrases or clusters.', 'We will report the evaluation results in the next subsection.', 'Step 1.', 'Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances.', 'The most frequent NE category pairs are “Person - Person (209,236), followed by “Country - Coun- try” (95,123) and “Person - Country” (75,509).', 'The frequency of the Company – Company domain ranks 11th with 35,567 examples.', 'As lower frequency examples include noise, we set a threshold that an NE category pair should appear at least 5 times to be considered and an NE instance pair should appear at least twice to be considered.', 'This limits the number of NE category pairs to 2,000 and the number of NE pair instances to 0.63 million.', 'Step 2.', 'Find keywords for each NE pair The keywords are found for each NE category pair.', 'For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3.', 'It is natural that the larger the data in the domain, the more keywords are found.', 'In the “Person – Person” domain, 618 keywords are found, and in the “Country – Country” domain, 303 keywords are found.', 'In total, for the 2,000 NE category pairs, 5,184 keywords are found.', 'Step 3.', 'Gather phrases using keywords Now, the keyword with the top TF/ITF score is selected for each phrase.', 'If a phrase does not contain any keywords, the phrase is discarded.', 'For example, out of 905 phrases in the CC- domain, 211 phrases contain keywords found in step 2.', 'In total, across all domains, we kept 13,976 phrases with keywords.', 'Step 4.', 'Link phrases based on instance pairs Using NE instance pairs as a clue, we find links between sets of phrases.', 'In the CC-domain, there are 32 sets of phrases which contain more than 2 phrases.', 'We concentrate on those sets.', 'Among these 32 sets, we found the following pairs of sets which have two or more links.', 'Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.', 'buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.', 'We will describe the evaluation of such clusters in the next subsection.', '3.3 Evaluation Results.', 'We evaluated the results based on two metrics.', 'One is the accuracy within a set of phrases which share the same keyword; the other is the accuracy of links.', 'We picked two domains, the CC-domain and the “Person – Company” domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate.', 'It is not easy to make a clear definition of “paraphrase”.', 'Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria.', 'If two phrases can be used to express the same relationship within an information extraction application (“scenario”), these two phrases are paraphrases.', 'Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.', 'In general, different modalities (“planned to buy”, “agreed to buy”, “bought”) were considered to express the same relationship within an extraction setting.', 'We did have a problem classifying some modified noun phrases where the modified phrase does not represent a qualified or restricted form of the head, like “chairman” and “vice chairman”, as these are both represented by the keyword “chairman”.', 'In this specific case, as these two titles could fill the same column of an IE table, we regarded them as paraphrases for the evaluation.', 'Evaluation within a set The evaluation of paraphrases within a set of phrases which share a keyword is illustrated in Figure 4.', 'For each set, the phrases with bracketed frequencies are considered not paraphrases in the set.', \"For example, the phrase “'s New York-based trust unit,” is not a paraphrase of the other phrases in the “unit” set.\", 'As you can see in the figure, the accuracy for the domain is quite high except for the “agree” set, which contains various expressions representing different relationships for an IE application.', 'The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.', 'The results, along with the total number of phrases, are shown in Table 1.', 'D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1.', 'Evaluation results within sets Table 1 shows the evaluation result based on the number of phrases in a set.', 'The larger sets are more accurate than the small sets.', 'We can make several observations on the cause of errors.', 'One is that smaller sets sometime have meaningless keywords, like “strength” or “add” in the CC-domain, or “compare” in the PC-domain.', 'Eight out of the thirteen errors in the high frequency phrases in the CC-domain are the phrases in “agree”.', 'As can be seen in Figure 3, the phrases in the “agree” set include completely different relationships, which are not paraphrases.', 'Other errors include NE tagging errors and errors due to a phrase which includes other NEs.', 'For example, in the phrase “Company-A last week purchased rival Marshalls from Company-B”, the purchased company is Marshalls, not Company-B.', 'Also there are cases where one of the two NEs belong to a phrase outside of the relation.', 'For example, from the sentence “Mr.', 'Smith estimates Lotus will make a profit this quarter…”, our system extracts “Smith esti mates Lotus” as an instance.', 'Obviously “Lotus” is part of the following clause rather than being the object of “estimates” and the extracted instance makes no sense.', 'We will return to these issues in the discussion section.', 'Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase.', 'All the links in the “CC-domain are shown in Step 4 in subsection 3.2.', 'Out of those 15 links, 4 are errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.', 'When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event.', 'The similar explanation applies to the link to the “stake” set.', 'We checked whether the discovered links are listed in WordNet.', 'Only 2 link in the CC- domain (buy-purchase, acquire-acquisition) and 2 links (trader-dealer and head-chief) in the PC- domain are found in the same synset of Word- Net 2.1 (http://wordnet.princeton.edu/).', 'This result suggests the benefit of using the automatic discovery method.', 'D o m ai n Li n k ac cu ra cy W N c o v e r a g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.', 'Evaluation results for links', 'The work reported here is closely related to [Ha- segawa et al. 04].', 'First, we will describe their method and compare it with our method.', 'They first collect the NE instance pairs and contexts, just like our method.', 'However, the next step is clearly different.', 'They cluster NE instance pairs based on the words in the contexts using a bag- of-words method.', 'In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30.', 'Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited.', 'Instead, we focused on phrases and set the frequency threshold to 2, and so were able to utilize a lot of phrases while minimizing noise.', '[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results.', 'The number of NE instance pairs used in their experiment is less than half of our method.', 'There have been other kinds of efforts to discover paraphrase automatically from corpora.', 'One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01].', 'The availability of comparable corpora is limited, which is a significant limitation on the approach.', 'Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].', 'This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.', 'There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].', 'The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.', 'This can be repeated several times to collect a list of author / book title pairs and expressions.', 'However, those methods need initial seeds, so the relation between entities has to be known in advance.', 'This limitation is the obstacle to making the technology “open domain”.', 'Keywords with more than one word In the evaluation, we explained that “chairman” and “vice chairman” are considered paraphrases.', 'However, it is desirable if we can separate them.', 'This problem arises because our keywords consist of only one word.', 'Sometime, multiple words are needed, like “vice chairman”, “prime minister” or “pay for” (“pay” and “pay for” are different senses in the CC-domain).', 'One possibility is to use n-grams based on mutual information.', 'If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate.', 'Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.', 'As was explained in the results section, “strength” or “add” are not desirable keywords in the CC-domain.', 'In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.', 'Also, “agree” in the CC-domain is not a desirable keyword.', 'It is a relatively frequent word in the domain, but it can be used in different extraction scenarios.', 'In this domain the major scenarios involve the things they agreed on, rather than the mere fact that they agreed.', '“Agree” is a subject control verb, which dominates another verb whose subject is the same as that of “agree”; the latter verb is generally the one of interest for extraction.', 'We have checked if there are similar verbs in other major domains, but this was the only one.', 'Using structural information As was explained in the results section, we extracted examples like “Smith estimates Lotus”, from a sentence like “Mr.', 'Smith estimates Lotus will make profit this quarter…”.', 'In order to solve this problem, a parse tree is needed to understand that “Lotus” is not the object of “estimates”.', 'Chunking is not enough to find such relationships.', 'This remains as future work.', 'Limitations There are several limitations in the methods.', 'The phrases have to be the expressions of length less than 5 chunks, appear between two NEs.', 'Also, the method of using keywords rules out phrases which don’t contain popular words in the domain.', 'We are not claiming that this method is almighty.', 'Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.', 'Applications The discovered paraphrases have multiple applications.', 'One obvious application is information extraction.', 'In IE, creating the patterns which express the requested scenario, e.g. “management succession” or “corporate merger and acquisition” is regarded as the hardest task.', 'The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.', 'Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.', 'While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.', 'We proposed an unsupervised method to discover paraphrases from a large untagged corpus.', 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.', 'In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.', 'The accuracies for link were 73% and 86% on two evaluated domains.', 'These results are promising and there are several avenues for improving on these results.', 'This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection, Extraction and Summarization (TIDES) program, under Grant N66001001-18917 from the Space and Naval Warfare Systems Center, San Diego, and by the National Science Foundation under Grant IIS00325657.', 'This paper does not necessarily reflect the position of the U.S. Government.', 'We would like to thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa and Mr. Yusuke Shinyama for useful comments, discussion and evaluation.']\n",
      "N01-1011 not well-formed (invalid token): line 13, column 100\n",
      "C94-2154 not well-formed (invalid token): line 10, column 202\n",
      "['The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', 'One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.', \"The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.\", 'The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems.', \"Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.\", \"These furnmlisms were developed relatively independentIy but actually had common properties; th'~t is, they used data structures called ftmctional structures or feature structures and they were based on unilieathm operation on these data structures.\", 'These formalisms were applied in the field of natural language processing and, based on these formalisms, ~:~ystems such as machine translation systems were developed [l<ol;u, e et a l 8gJ.', 'In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.', 'The efficiency of systems based on ..~uch formalisms, such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies.', 'Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs.', 'For example, a spoken Present.', 'affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories.', \"lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan.\", 'Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.', \"Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.\", 'These methods uses rooted directed graphs (DGs) to represent FSs.', 'These methods take two DGs as their inputs and give a unification result DG.', 'Previous research identified DG copying as a significant overhead.', 'Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).', 'Ile proposed an incremental copy graph unification method to avoid over copying and early copying.', 'itowever, the problem with his method is that a unitication result graph consists only of newly created structures.', 'This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.', 'Copying sharable parts is called redundant copying.', 'A better method would nfinimize the copying of sharable varts.', 'The redundantly copied parts are relatively large when input graphs have few common feature paths.', 'In natural language processing, such cases are ubiquitous.', 'I\"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h\\'equent, ly.', \"In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts.\", 'Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.', 'I)eveloping a method which avoids memory wastage is very important.', \"Pereira's structure sharing FS unification method can avoid this problem.\", 'The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.', 'The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information.', \"3'he skeleton part is shared by one of the input FSs and the result FS.\", \"Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification.\", \"However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts.\", 'This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG.', 'Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing.', \"This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.\", \"This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.\", 'The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).', 'In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.', 'The advantages of such a system include: (1)rule writers are not required to describe control infimnation such as eonstraiut application order in a rule, and (12)rule descriptions can be used iu different processing directions, i.e., analysis and general,ion.', 'However, these advantages in describing rules are disadvantages in applying them because of tt~e lack of control information.', \"For example, when constructing a phrase structure from its parts (e.g., a sentence fi'om a subject NP and VP), unueeessary computation can be reduced if the semantic representation is assembled after checking constraints such as grammatical agreements, which can fail.\", 'This is impossible in straightforward unification-based formalisms.', \"In contrast, in a procedure-based system which uses IF-TItEN style rules (i.e., consisting of explicit test and structure-building operations), it is possible to construct the semantic representation (TIIEN par'g) after checking the agreement (IF part).\", 'Such a system has the advantage of processing efficiency but the disadvantage of lacking multidirectionality.', 'In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.', 'That is, an FS unification method is proposed that introduces a strategy called the e_arly failure £inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.', 'This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).', 'These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).', 'Section 2 explains typed feature structures (TFSs) and unification on them.', \"Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.\", 'The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method.', 'Section 3 and 4 introduce the LING method and the SING method, respectively.', 'Ordinary FSs used in unification-based grammar formalisms such as PAT].{[Shieher 851 arc classified into two classes, namely, atomic leSs and complex FSs.', 'An atomic FS is represented by an atomic symbol and a complex FS is represented by a set of feature-value pairs.', 'Complex FSs are used to partially describe objects by specifying values for certain features or attributes of described objects.', 'Complex FSs can have complex FSs as their feature values and can share certain values among features.', 'For ordinary FSs, unification is defined by using partial ordering based on subsumption relationships.', 'These properties enable flexible descriptions.', 'An extension allows complex FSs to have type symbols which define a lattice structure on them, for example, as in [Pollard and Sag 8\"11.', 'The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.', 'An example of a type symbol lattice is shown in Fig.', '1.', 'An extended complex FS is represented by a type symbol and a set of feature-value pairs.', 'Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.', 'Extended complex FSs are called typed feature structures (TFSs).', 'TFSs are denoted by feature-value pair matrices or rooted directed graphs as shown in Fig.', '2.', \"Among such structures, unification c'm be defined IAP,- Kaci 861 by using the following order; ATFS tl is less than or equal to a TFS t2 if and only if: \\x95 the type symbol of tl is less than or equal to the type syn'bol of/2; and \\x95 each of the features of t2 exists in t1 and.\", 'has as its value a TFS which is not less than its counterpart in tl ; and each of the coreference relationships in t2 is also held in tl.', 'Top Sign Syn Head List POS /77 Lexical Phrase Sign NonEmpty Empty V N P ADV Slgn Li.', 'Lis~ ust I I I I NonEmpty Emply I I i I Sign Sign I I/ / List List 5/ /5 ....', 'U_ Bottom Figure 1: Exainple of a type symbol lattice --2-- peSymb°10 eaturel TypeSymboll ] ]] I feature2 TypeSymbol2 I feature3 ?Tag T ypeSymbol3 ] ]feature4 TypeSymbol4 L [.feature5 TypeSymbol5 TIeature3 7Tag (a) feature-value matrix notation \"?\" i~ the prefix for a tag and TFSs with the same tag are token-identical.', 'TypeSym bol/~ feo~.,o/ I TypeSymboll ~ [.', 'TypeSymbol2 4¢\" \\'~°~\\'~/.~ypeSymbol3 featury \"X~ature5 TypeSymbol4 4r \"~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i\\'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | [\\'first ]1 ?×3 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase \"syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P\"\" ~×~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.', 'A unification example is shown in Fig.', '3.', 'In tile directed graph notation, TFS unification corresponds to graph mergi ng.', 'TFSs are very convenient for describing linguistic information in unlfication-based formalisms.', \"In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig.\", '4.', 'The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY.', \"When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships.\", 'A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship.', 'When a NODE node1 has a NODE node2 as its FORWARD value, the other contents of tile node1 are ignored and tim contents of node2 are used.', 't{owever, when a NODE has another NODE as its COPY value, the contents of the COPY value are used only when the COPY value is cub:rent.', 'After the process finishes, all COPY slot values are ignored and thus original structures are not destroyed.', 'The unification procedure based on this method takes as its input two nodes which are roots of the DGs to be unified.', 'The procedure incrementally copies nodes and ares on the subgraphs of each input 1)G until a node with an empty ARCS value is found.', 'The procedure first dereferences both root nodes of the input DGs (i.e., it follows up FORWARD and COPY slot values).', 'If the dereferenee result nodes arc identical, the procedure finishes and returns one of the dereference result nodes.', 'Next, the procedure calculates the meet of their type symbol.', 'If the meet is Bottom, which means inconsistency, the procedure finishes and returns Bottom.', 'Otherwise, the procedure obtains the output node with the meet as its TYPESYMBOL.', 'The output node has been created only when neither input node is current; or otherwise the output node is an existing current node.', 'Next, the procedure treats arcs.', 'The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs.', 'The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order.', 'The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: \"<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski\\'s method Input graph GI Input graph 62 ¢ .......\\'77 ........ i : Sobg,\\'aphs not required to be copied L ...........................................', 'Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.', 'its arguments and gives one list of arcs whose labels are unique to one input list.', 'The unification procedure first treats arc pairs obtained by SharedArcs.', \"The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom.\", 'Next, the procedure treats arcs obtained by ComplementArcs.', 'Each arc value is copied and an arc with the same label and the copied value is added to the output node.', 'For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.', '5.', 'The unification procedure is applied recursively to feature a values of the input nodes.', \"The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not.\", 'The whole subgraph rooted by 6 l/<a c> is then copied.', 'This is because such subgraphs can be modified later.', 'For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated.', 'Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel).', 'node2 = Dereferencelnode2).', 'IF Eq?(nodel, node2) THEN Return(node1).', 'ELSE meet = Meet(nodel.typesymbol, node2.typesymbol) IF Equal?(meet, Bottom) THEN Return(Bottom).', 'ELSE outnode = GetOutNode(nodel, node2, meet).', '(sharedst, shareds2) = SharedArcs(nodel.arcs, node2.arcs).', 'complements1 = ComplementArcs(node|.arcs, node2.arcs).', 'complements2 = ComplementArcs(node2.arcs, nodel.arcs).', 'FOR ALL (sharedt, shared2) IN (sharedsl, shareds2) DO arcnode = Unify(sharedl.value, shared2.value).', 'IF Equal?(arcnode, Bottom) ]HEN Return(Bottom).', 'ELSE AddArc(outnode, sharedl.label, arcnode).', \"ENDIF IF Eq?(outnode, node1) THEN coi'nplements = complement2.\", 'ELSE IF Eq?(outnode, node2) THEN complements = complementL ELSE complements = Append(complements1, complements2].', 'ENDIF FORALL complement IN complements DO newnode = CopyNode(complement.value).', 'AddArc(outnode, complement.label, newnode).', 'Return(outnode).', \"ENDIF ENDIE ENDPROCEDURE Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures.\", 'In the example in Fig.', \"5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels.\", 'This order is related to the unification failure tendency.', 'Unification fails in treating arcs with common labels more often than in treating arcs with unique labels.', 'Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation.', 'This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.', 'In Section 5, a method which uses this generalized strategy is proposed.', \"In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig.\", '5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.', 'With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.', '5 due to a change of node Y G3/<a c g>).', 'To achieve this, I, he LING unification method, which uses copy dependency information, was developed.', 'The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately.', 'The revised procedure uses a newly introduced slot COPY-DEPENDENCY.', 'The slot has pairs consisting of nodes and arcs as its value.', \"The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.\", '7): (1) if nodel \\', the dereference result of node/, is current, then CopyNode returns node l\" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1\" and if it returns ,~;everal arc copies, CopyNode creates a new copy node.', 'It then adds the arc copies and arcs of node/\\' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1\" and returns Nil_.', \",',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results.\", \"When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig.\", '6).', 'It substitutes arcs with newly copied nodes for existing arcs.', 'That is, antecedent nodes in the COPY-DEPENDENCY values are also copied.', 'In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity.', ']lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously.', 'The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.', \"Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method.\", 'The efficiency of the LING unification method depends on the proportion of newly created structures in the unification result structures.', \"Two worst eases can be considered: (t) If there are no arcs whose labels are unique to an input node witlh respect to each other, the procedure in LING unification method behaves in the same way as the procedure in the Wroblewski's method.\", '(2) In the worst eases, in which there are unique label arcs but all result structures are newly created, the method CopyNode PROCEDURE CopyNode(node, arc, ancestor) node = Dereference(node).', 'IF Current?(node) THEN Return(node).', 'ELSE IF NotEmpty?(newarcs = CopyArcs(node)) THEN newnode = Create(node.typesymbol).', 'node.copy = newnode.', 'FOR ALL arc IN node.arcs DO IF NotNIL?(newarc = FindArc(arc.label, newarcs)) THEN AddArc(newnode, newarc.label, newarc.value}.', 'ELSE AddArc(newnode, arc.label, arc.value).', 'ENDIF Returo(newnode).', 'ELSE node.copy-dependency = node.copy-dependency U {Cons(ancestor, arc)}.', 'Return(Nil_).', 'ENDIF ENDPROCEDURE CopyArcs PROCEDURE AlcsCopied(node) newarcs = O- FOR ALL arc IN node.arcs DO newnode = CopyNode(arc.value, arc, node).', 'IF NotNIL?(newnode) THEN newarc = CreateArc(arc.label, newnode).', 'newarcs = {newarc} U newarcs.', 'ENDIF Return(newarcs).', 'ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.', 'However, these two cases are very rare.', 'Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different.', 'For example, in Kasper\\'s disjunctive feature description unification, a definite part [\"S is larger than a disjunet definite part t\"S.', 'Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often.', 'For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail.', 'In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails.', 'For example, when unification of features for case markers does fail, treating these features first avoids treating features for senmntic representations.', 'The SING unification method uses this failure tendency infornmtion.', 'These unification failure tendencies depend on systems such as analysis systems or generation systems.', 'Unlike the analysis case, unification of features for semantic representations tends to fail.', 'in this method, theretbre, the failure tendency information is acquired by a learning process.', 'That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.', 'in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.', \"As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag.\", \"This is because the type symbol of a 'rFS represents salient information on the whole TFS.\", 'By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.', 'This is achieved by the sorting procedure of common label arc pairs attached to the meet type symbol.', 'The arc pairs obtained by the SharedArcs procedure are sorted before treating arcs.', 'The efficiency of the SING unification method depends on the following factors: (1) The overall FS unification failure rate of the process: in extreme cases, if Go unification failure occurs, the method has no advantages except the overhead of feature unification order sorting.', 'However, such cases do not occur in practice.', '(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small.', '(3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage.', 'However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints.', 'In such cases, the SING unification method obtains efl]ciency gains.', 'The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted.', 'Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.', 'The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.', \"The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.\", \"Structure sharing avoids memory wastage'.\", 'Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.', 'This reduces repeated calculation of substructures.', 'The SING unification method introduces the concept of feature unification strategy.', \"'the method treats features tending to fail in unification first.\", \"Thus, the efficiency gain fi'om this method is high when the overall FS unification failure rate of the application process is high.\", \"The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.', 'First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.', 'Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.', 'Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG.', 'Fourth, we show how to build better models for three different parsers.', 'Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2–5% F1.', 'It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima’an, 2008), and the effect of variable word order (Collins et al., 1999).', 'Certainly these linguistic factors increase the difficulty of syntactic disambiguation.', 'Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; Ku¨ bler, 2005).', '1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajicˇ and Zema´nek, 2004; Habash and Roth, 2009).', 'To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results.', 'The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).', 'Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.', 'But Arabic contains a variety of linguistic phenomena unseen in English.', 'Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation.', 'For humans, this characteristic can impede the acquisition of literacy.', 'How do additional ambiguities caused by devocalization affect statistical learning?', 'How should the absence of vowels and syntactic markers influence annotation choices and grammar development?', 'Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering.', 'Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (§2).', 'Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (§3).', 'We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (§4).', 'To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (§5).', 'Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (§6).', 'We quantify error categories in both evaluation settings.', 'To our knowledge, ours is the first analysis of this kind for Arabic parsing.', 'Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.', 'The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.', 'Particles are uninflected.', \"Word Head Of Complement POS 1 '01 inna “Indeed, truly” VP Noun VBP 2 '01 anna “That” SBAR Noun IN 3 01 in “If” SBAR Verb IN 4 01 an “to” SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.\", 'The distinctions in the ATB are linguistically justified, but complicate parsing.', 'Table 8a shows that the best model recovers SBAR at only 71.0% F1.', 'Diacritics can also be used to specify grammatical relations such as case and gender.', 'But diacritics are not present in unvocalized text, which is the standard form of, e.g., news media documents.3 VBD she added VP PUNC S VP VBP NP ...', 'VBD she added VP PUNC “ SBAR IN NP 0 NN.', 'Let us consider an example of ambiguity caused by devocalization.', 'Table 1 shows four words “ 0 Indeed NN Indeed Saddamwhose unvocalized surface forms 0 an are indistinguishable.', 'Whereas Arabic linguistic theory as Saddam (a) Reference (b) Stanford signs (1) and (2) to the class of pseudo verbs 01 +i J>1� inna and her sisters since they can beinflected, the ATB conventions treat (2) as a com plementizer, which means that it must be the head of SBAR.', 'Because these two words have identical complements, syntax rules are typically unhelpful for distinguishing between them.', 'This is especially true in the case of quotations—which are common in the ATB—where (1) will follow a verb like (2) (Figure 1).', 'Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues.', 'Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness.', 'Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order.', 'However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009).', 'In particular, the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish.', 'Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop).', '3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007).', 'However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance.', \"4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun � '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).\", 'more frequently than is done in English.', 'Process nominals name the action of the transitive or ditransitive verb from which they derive.', 'The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case.', 'When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct � ?f iDafa.', 'Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set.', 'Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.', 'All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly.', 'For parsing, the most challenging form of ambiguity occurs at the discourse level.', 'A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases (Ryding, 2005).', 'Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as � wa and � fa to link new elements to both preceding clauses and the text as a whole.', 'As a result, Arabic sentences are usually long relative to English, especially after Length English (WSJ) Arabic (ATB) ≤ 20 41.9% 33.7% ≤ 40 92.4% 73.2% ≤ 63 99.7% 92.6% ≤ 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2–23) and the ATB (p1–3).', 'English parsing evaluations usually report results on sentences up to length 40.', 'Arabic sentences of up to length 63 would need to be.', 'evaluated to account for the same fraction of the data.', 'We propose a limit of 70 words for Arabic parsing evaluations.', 'ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks.', 'Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test).', 'Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.', 'segmentation (Table 2).', 'The ATB gives several different analyses to these words to indicate different types of coordination.', 'But it conflates the coordinating and discourse separator functions of wa (<..4.b � �) into one analysis: conjunction(Table 3).', 'A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990).', 'We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).', '3.1 Gross Statistics.', 'Linguistic intuitions like those in the previous section inform language-specific annotation choices.', 'The resulting structural differences between tree- banks can account for relative differences in parsing performance.', 'We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4).', 'The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).', 'We map the ATB morphological analyses to the shortened “Bies” tags for all experiments.', 'yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (μ Constituents / μ Length).', 'Evalb, the standard parsing metric, is biased toward such corpora (Sampson and Babarczy, 2003).', 'Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.', 'In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance.', '3.2 Inter-annotator Agreement.', 'Annotation consistency is important in any supervised learning task.', 'In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008).', 'To improve agreement during the revision process, a dual-blind evaluation was performed in which 10% of the data was annotated by independent teams.', 'Maamouri et al.', '(2008) reported agreement between the teams (measured with Evalb) at 93.8% F1, the level of the CTB.', 'But Rehbein and van Genabith (2007) showed that Evalb should not be used as an indication of real difference— or similarity—between treebanks.', 'Instead, we extend the variation n-gram method of Dickinson (2005) to compare annotation error rates in the WSJ and ATB.', 'For a corpus C, let M be the set of tuples ∗n, l), where n is an n-gram with bracketing label l. If any n appears 6 Generative parsing performance is known to deteriorate with sentence length.', 'As a result, Habash et al.', '(2006) developed a technique for splitting and chunking long sentences.', 'In application settings, this may be a profitable strategy.', 'NN � .e NP NNP NP DTNNP NN � .e NP NP NNP NP Table 5: Evaluation of 100 randomly sampled variation nuclei types.', 'The samples from each corpus were independently evaluated.', 'The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate.', 'summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add ∗n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity.', 'Human evaluation is one way to distinguish between the two cases.', 'Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error.', 'The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus.', 'The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.', 'The results clearly indicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference.', 'On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample.', 'At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts.', 'For example, one of the ATB samples was the determiner -\"\" ; dhalik“that.” The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent.', 'If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%.', 'The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions.', '7 Unlike Dickinson (2005), we strip traces and only con-.', 'Figure 2: An ATB sample from the human evaluation.', 'The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).', 'But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).', 'We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).', 'Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions.', 'A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).', 'In our grammar, features are realized as annotations to basic category labels.', 'We start with noun features since written Arabic contains a very high proportion of NPs.', 'genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter.', 'This is the form of recursive levels in iDafa constructs.', 'We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).', 'For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).', 'Base NPs are the other significant category of nominal phrases.', 'markBaseNP indicates these non-recursive nominal phrases.', 'This feature includes named entities, which the ATB marks with a flat NP node dominating an arbitrary number of NNP pre-terminal daughters (Figure 2).', 'For verbs we add two features.', 'First we mark any node that dominates (at any level) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs).', 'Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei.', '8 We use head-finding rules specified by a native speaker.', 'of Arabic.', 'This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.', 'termined by the category of the word that follows it.', 'Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.', 'We create equivalence classes for verb, noun, and adjective POS categories.', 'Table 6: Incremental dev set results for the manually annotated grammar (sentences of length ≤ 70).', 'phrase (markContainsVerb).', 'This feature has a linguistic justification.', \"Historically, Arabic grammar has identified two sentences types: those that begin with a nominal (� '.i �u _..\", '), and thosethat begin with a verb (� ub..i �u _..', 'But for eign learners are often surprised by the verbless predications that are frequently used in Arabic.', 'Although these are technically nominal, they have become known as “equational” sentences.', 'mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.', 'We also mark all nodes that dominate an SVO configuration (containsSVO).', 'In MSA, SVO usually appears in non-matrix clauses.', 'Lexicalizing several POS tags improves performance.', 'splitIN captures the verb/preposition idioms that are widespread in Arabic.', 'Although this feature helps, we encounter one consequence of variable word order.', 'Unlike the WSJ corpus which has a high frequency of rules like VP →VB PP, Arabic verb phrases usually have lexi calized intervening nodes (e.g., NP subjects and direct objects).', 'For example, we might have VP → VB NP PP, where the NP is the subject.', 'This annotation choice weakens splitIN.', 'We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers.', 'All experiments use ATB parts 1–3 divided according to the canonical split suggested by Chiang et al.', '(2006).', 'Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.', 'At the phrasal level, we remove all function tags and traces.', 'We also collapse unary chains withidentical basic categories like NP → NP.', 'The pre terminal morphological analyses are mapped to the shortened “Bies” tags provided with the tree- bank.', 'Finally, we add “DT” to the tags for definite nouns and adjectives (Kulick et al., 2006).', 'The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents.', 'We retain segmentation markers—which are consistent only in the vocalized section of the treebank—to differentiate between e.g. � “they” and � + “their.” Because we use the vocalized section, we must remove null pronoun markers.', 'In Table 7 we give results for several evaluation metrics.', 'Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.', 'For parsing, this is a mistake, especially in the case of interrogatives.', 'splitPUNC restores the convention of the WSJ.', 'We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).', 'To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).', 'The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.', 'able at http://nlp.stanford.edu/projects/arabic.shtml.', '10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.', '11 taTweel (-) is an elongation character used in Arabic script to justify text.', 'It has no syntactic function.', 'Variants of alif are inconsistently used in Arabic texts.', 'For alif with hamza, normalization can be seen as another level of devocalization.', '12 For English, our Evalb implementation is identical to the most recent reference (EVALB20080701).', 'For Arabic we M o d e l S y s t e m L e n g t h L e a f A n c e s t o r Co rpu s Sent Exact E v a l b L P LR F1 T a g % B a s e l i n e 7 0 St an for d (v 1.', '6. 3) all G o l d P O S 7 0 0.7 91 0.825 358 0.7 73 0.818 358 0.8 02 0.836 452 80.', '37 79.', '36 79.', '86 78.', '92 77.', '72 78.', '32 81.', '07 80.', '27 80.', '67 95.', '58 95.', '49 99.', '95 B a s e li n e ( S e lf t a g ) 70 a l l B i k e l ( v 1 . 2 ) B a s e l i n e ( P r e t a g ) 7 0 a l l G o l d P O S 70 0.7 70 0.801 278 0.7 52 0.794 278 0.7 71 0.804 295 0.7 52 0.796 295 0.7 75 0.808 309 77.', '92 76.', '00 76.', '95 76.', '96 75.', '01 75.', '97 78.', '35 76.', '72 77.', '52 77.', '31 75.', '64 76.', '47 78.', '83 77.', '18 77.', '99 94.', '64 94.', '63 95.', '68 95.', '68 96.', '60 ( P e tr o v, 2 0 0 9 ) all B e r k e l e y ( S e p . 0 9 ) B a s e l i n e 7 0 a l l G o l d P O S 70 — — — 0 . 8 0 9 0.839 335 0 . 7 9', '0 . 8 3 1 0.859 496 76.', '40 75.', '30 75.', '85 82.', '32 81.', '63 81.', '97 81.', '43 80.', '73 81.', '08 84.', '37 84.', '21 84.', '29 — 95.', '07 95.', '02 99.', '87 Table 7: Test set results.', 'Maamouri et al.', '(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length ≤ 40.', 'The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them.', 'We are unaware of prior results for the Stanford parser.', 'F1 85 Berkeley 80 Stanford.', 'Bikel 75 training trees 5000 10000 15000 Figure 3: Dev set learning curves for sentence lengths ≤ 70.', 'All three curves remain steep at the maximum training set size of 18818 trees.', 'The Leaf Ancestor metric measures the cost of transforming guess trees to the reference (Sampson and Babarczy, 2003).', 'It was developed in response to the non-terminal/terminal bias of Evalb, but Clegg and Shepherd (2005) showed that it is also a valuable diagnostic tool for trees with complex deep structures such as those found in the ATB.', 'For each terminal, the Leaf Ancestor metric extracts the shortest path to the root.', 'It then computes a normalized Levenshtein edit distance between the extracted chain and the reference.', 'The range of the score is between 0 and 1 (higher is better).', 'We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB.', 'Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation.', 'with the number of exactly matching guess trees.', '5.1 Parsing Models.', 'The Stanford parser includes both the manually annotated grammar (§4) and an Arabic unknown word model with the following lexical features: 1.', 'Presence of the determiner J Al. 2.', 'Contains digits.', '3.', 'Ends with the feminine affix :: p. 4.', 'Various verbal (e.g., �, .::) and adjectival.', 'suffixes (e.g., �=) Other notable parameters are second order vertical Markovization and marking of unary rules.', 'Modifying the Berkeley parser for Arabic is straightforward.', 'After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization.', 'We use the default inference parameters.', 'Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.', 'However, when we pre- tag the input—as is recommended for English— we notice a 0.57% F1 improvement.', 'We use the log-linear tagger of Toutanova et al.', '(2003), which gives 96.8% accuracy on the test set.', '5.2 Discussion.', 'The Berkeley parser gives state-of-the-art performance for all metrics.', 'Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result.', 'The difference is due to more careful S-NOM NP NP NP VP VBG :: b NP restoring NP ADJP NN :: b NP NN NP NP ADJP DTJJ ADJP DTJJ NN :: b NP NP NP ADJP ADJP DTJJ J ..i NN :: b NP NP NP ADJP ADJP DTJJ NN _;� NP PRP DTJJ DTJJ J ..i _;� PRP J ..i NN _;� NP PRP DTJJ NN _;� NP PRP DTJJ J ..i role its constructive effective (b) Stanford (c) Berkeley (d) Bik el (a) Reference Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmentation).', 'The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.', 'Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993).', 'In the ATB, :: b asta’adah is tagged 48 times as a noun and 9 times as verbal noun.', 'Consequently, all three parsers prefer the nominal reading.', 'Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.', 'None of the models attach the attributive adjectives correctly.', 'pre-processing.', 'However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).', 'Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input.', 'In Figure 4 we show an example of variation between the parsing models.', 'We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.', 'The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.', '6 Joint Segmentation and Parsing.', 'Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words.', 'Since these are distinct syntactic units, they are typically segmented.', 'The ATB segmentation scheme is one of many alternatives.', 'Until now, all evaluations of Arabic parsing—including the experiments in the previous section—have assumed gold segmentation.', 'But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.', 'Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.', 'Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.', 'Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.', 'We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.', 'To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).', 'Formally, for a lexicon L and segments I ∈ L, O ∈/ L, each word automaton accepts the language I∗(O + I)I∗.', 'Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.', 'Our evaluation includes both weighted and un- weighted lattices.', 'We weight edges using a unigram language model estimated with Good- Turing smoothing.', 'Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).', 'MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer.', 'For each 13 Of course, this weighting makes the PCFG an improper distribution.', 'However, in practice, unknown word models also make the distribution improper.', 'Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths ≤ 70 (dev set, gold segmentation).', '(a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse.', 'We showed in §2 that lexical ambiguity explains the underperformance of these categories.', '(b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ).', 'Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.', '(c) Coordination ambiguity is shown in dependency scores by e.g., ∗SSS R) and ∗NP NP NP R).', '∗NP NP PP R) and ∗NP NP ADJP R) are both iDafa attachment.', 'input token, the segmentation is then performed deterministically given the 1-best analysis.', 'Since guess and gold trees may now have different yields, the question of evaluation is complex.', 'Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.', 'But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal.', 'Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.', 'Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.', 'However, MADA is language-specific and relies on manually constructed dictionaries.', 'Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.', 'Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.', 'A cell in the bottom row of the parse chart is required for each potential whitespace boundary.', 'As we have said, parse quality decreases with sentence length.', 'Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew.', 'Table 9: Dev set results for sentences of length ≤ 70.', 'Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference.', 'Each model was able to produce hypotheses for all input sentences.', 'In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.', 'By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.', 'We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.', 'With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.', 'Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.', 'Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work.', 'We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions.', 'The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship.', 'This paper is based on work supported in part by DARPA through IBM.', 'The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.']\n",
      "['We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.', 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.', 'We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.', 'We find that the unsupervised method we tried cannot be consistently applied to our data.', 'However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well.', 'Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies.', 'Learning the argument structure properties of verbs—the semantic roles they assign and their mapping to syntactic positions—is both particularly important and difficult.', 'A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).', 'Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).', 'We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.', 'Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).', 'As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).', 'However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.', 'Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.', 'It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features.', 'We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).', 'In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.', 'Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).', 'On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).', 'By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.', 'However, a general feature space means that most features will be irrelevant to any given verb discrimination task.', 'In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to “the curse of dimensionality”?', 'In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand.', 'In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner.', 'Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.', 'In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).', 'Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).', 'To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.', 'The unsupervised feature selection method, on the other hand, was not usable for our data.', 'In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs.', 'We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.', 'We conclude with a discussion of related work, our contributions, and future directions.', 'Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).', 'Levin’s classes form a hierarchy of verb groupings with shared meaning and syntax.', 'Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.', 'It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind.', 'Thus, the features serve as approximations to the underlying distinctions among classes.', 'Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.', 'Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.', 'We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.', 'In addition to approximating the syntactic frames themselves, we also want to capture regularities in the mapping of arguments to particular slots.', 'For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck.', 'These allowable alternations in the expressions of arguments vary according to the class of a verb.', 'We measure this behaviour using features that encode the degree to which two slots contain the same entities—that is, we calculate the overlap in noun (lemma) usage between pairs of syntactic slots.', 'Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).', 'In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.', 'The Animacy Features (76 features) Semantic properties of the arguments that fill certain roles, such as animacy or motion, are more challenging to detect automatically.', 'Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001).', 'We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as “person” by our chunker (Abney, 1991).', 'We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.', 'Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.', '3.1 The Verb Classes.', 'Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.', 'These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.', 'For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers.', 'Benefactive versus Recipient verbs.', 'Mary baked... a cake for Joan/Joan a cake.', 'Mary gave... a cake to Joan/Joan a cake.', 'These dative alternation verbs differ in the preposition and the semantic role of its object.', '1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.', 'Admire versus Amuse verbs.', 'I admire Jane.', 'Jane amuses me. These psychological state verbs differ in that the Experiencer argument is the subject of Admire verbs, and the object of Amuse verbs.', 'Run versus Sound Emission verbs.', 'The kids ran in the room./*The room ran with kids.', 'The birds sang in the trees./The trees sang with birds.These activity verbs both have an Agent subject in the in transitive, but differ in the prepositional alternations they allow.', 'Cheat versus Steal and Remove verbs.', 'I cheated...', 'Jane of her money/*the money from Jane.', 'I stole...', '*Jane of her money/the money from Jane.', 'These classes also assign the same semantic arguments, but differ in their prepositional alternants.', 'Wipe versus Steal and Remove verbs.', 'Wipe... the dust/the dust from the table/the table.', 'Steal... the money/the money from the bank/*the bank.', 'These classes generally allow the same syntactic frames, but differ in the possible semantic role assignment.', '(Location can be the direct object of Wipe verbs but not of Steal and Remove verbs, as shown.)', 'Spray/Load versus Fill versus Other Verbs of Putting (several related Levin classes).', 'I loaded... hay on the wagon/the wagon with hay.', 'I filled...', '*hay on the wagon/the wagon with hay.', 'I put... hay on the wagon/*the wagon with hay.', 'These three classes also assign the same semantic roles but differ in prepositional alternants.', 'Note, however, that the options for Spray/Load verbs overlap with those of the other two types of verbs.', 'Optionally Intransitive: Run versus Change of State versus “Object Drop”.', 'The horse raced./The jockey raced the horse.', 'The butter melted./The cook melted the butter.', 'The boy played./The boy played soccer.These three classes are all optionally intransitive but as sign different semantic roles to their arguments (Merlo and Stevenson, 2001).', '(Note that the Object Drop verbs are a superset of the Benefactives above.)', 'For many tasks, knowing exactly what PP arguments each verb takes may be sufficient to perform the classification (cf.', 'Dorr and Jones, 1996).', 'However, our features do not give us such perfect knowledge, since PP arguments and adjuncts cannot be distinguished with high accuracy.', 'Using our simple extraction tools, for example, the PP argument in I admired Jane for her honesty is not distinguished from the PP adjunct in I amused Jane for the money.', 'Furthermore, PP arguments differ in frequency, so that a highly distinguishing but rarely used alternant will likely not be useful.', 'Indicators of PP usage are thus useful but not definitive.', 'Ve rb Cl as s C la ss N u m b er # Ve rbs Be ne fa cti ve 26.', '1, 26.', '3 3 5 Re ci pi en t 13.', '1, 13.', '3 2 7 Ad mi re 31.', '2 3 5 A m us e 31.', '1 1 3 4 Ru n 51.', '3.2 7 9 So un d E mi ssi on 43.', '2 5 6 C he at 10.', '6 2 9 St ea l an d Re m ov e 10.', '5, 10.', '1 4 5 Wi pe 10.', '4.1 , 10.', '4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi ll 9.8 6 3 Ot he r V. of Pu tti ng 9.1 –6 4 8 C ha ng e of St at e 45.', '1– 4 1 6 9 O bj ec t Dr op 26.', '1, 26.', '3, 26.', '7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).', '3.2 Verb Selection.', 'Our experimental verbs were selected as follows.', 'We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).', 'Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense.', 'Table 1 above shows the number of verbs in each class at the end of this process.', 'Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).', 'We began with this same set of 20 verbs per class for our current work.', 'We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).', 'All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).', '3.3 Feature Extraction.', 'All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.', 'Since it is a general corpus, we do not expect any strong overall domain bias in verb usage.', 'We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it.', 'Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.', 'From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.', '4.1 Clustering Parameters.', 'We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.', 'In performing hierarchical clustering, both a vector distance measure and a cluster distance (“linkage”) measure are specified.', 'We used the simple Euclidean distance for the former, and Ward linkage for the latter.', 'Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods.', 'We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes.', 'To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy.', 'In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.', 'However, we did experiment with (as in Strehl et al., 2000), and found that performance was generally better (even on our measure, described below, that discounts oversplitting).', 'This supports our intuition that the approach may enable us to find more consistent clusters at a finer grain, without too much fragmentation.', '4.2 Evaluation Measures.', 'We use three separate evaluation measures, that tap into very different properties of the clusterings.', '4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.', 'Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed.', 'Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.', 'As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clustering—that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be.', 'The theoretical maximum is, of course, 1.', 'To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks.', 'Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline.', 'These figures are reported with our results in Table 2 below.', '4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.', 'Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.', 'The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between them— there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not.', 'It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.', 'It is therefore considered “corrected for chance,” given a fixed number of clusters.3 In tests of the measure on some contrived cluster- ings, we found it quite conservative, and on our experimental clusterings it did not often attain values higher than .25.', 'However, it is useful as a relative measure of good-.', 'ness, in comparing clusterings arising from different feature sets.', '4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.', 'Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs.', 'However, since we fix our number of clusters to the number of classes, the measure remains informative.', '3 In our experiments for estimating the baseline, we in-.', 'deed found a mean value of 0.00 for all random clusterings.', '1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89', 'W e re p or t he re th e re su lt s of a n u m be r of cl us te ri n g ex - pe ri m en ts, us in g fe at ur e se ts as fo ll o w s: (1 ) th e fu ll fe at ur e sp ac e; (2 ) a m an ua ll y se le ct ed su bs et of fe at ur es ; (3 ) u n- su pe rv is ed se le ct io n of fe at ur es ; an d (4 ) se mi su p er vi se d se le ct io n, us in g a su pe rv is ed le ar ne r ap pl ie d to se ed ve rb s to se le ct th e fe at ur es . F or ea ch ty pe of fe at ur e se t, w e pe rf or m ed th e sa m e te n cl us te ri n g ta sk s, sh o w n in th e fir st co lu m n of Ta bl e 2.', 'T he se ar e th e sa m e ta sk s pe rf or m ed in th e su pe rv is ed se t- ti n g of Jo an is an d St ev en so n (2 0 0 3) . T he 2- an d 3 w ay ta sk s, an d th ei r m ot iv at io n, w er e de sc ri be d in S ec ti o n 3.', '1. T hr ee m ul ti w ay ta sk s ex pl or e pe rf or m an ce o ve r a la rg er n u m be r of cl as se s: T he 6 w ay ta sk in v ol v es th e C he at , St ea l– R e m ov e, W ip e, S pr ay /L o a d, Fi ll, an d “ O th er V er bs of P ut ti n g ” cl as se s, al l of w hi ch u n de rg o si m il ar lo ca ti v e Figure 1: The dendrograms and values for the 2-way Wipe/Steal–Remove task, using the Ling and Seed sets.', 'The higher (.89 vs. .33) reflects the better separation of the data.', 'regard to the target classes.', 'We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.', 'Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster).', 'A value of 0 suggests that a point is not clearly in a particular cluster.', 'We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.', 'Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of “better” and “worse” clusterings.', '(A dendrogram is a tree diagram whose leaves are the data points, and whose branch lengths indicate similarity of subclusters; roughly, shorter vertical lines indicate closer clusters.)', 'For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes.', 'The Seed set has slightly lower values for and , but a much higher value (.89) for , indicating a better separation of the data.', 'This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters.', 'The measure is independent of the true classification, and could be high when the other dependent measures are low, or vice versa.', 'However, it gives important information about the quality of a clustering: The other measures being equal, a clustering with a higher value indicates tighter and more separated clusters, suggesting stronger inherent patterns in the data.', 'alternations.', 'To these 6, the 8-way task adds the Run and Sound Emission verbs, which also undergo locative alternations.', 'The 13-way task includes all of our classes.', 'The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.', 'These are the results of a 10-fold cross- validation (with boosting) repeated 50 times.4 In our earlier work, we found that cross-validation performance averaged about .02, .04, and .11 higher than test performance on the 2-way, 3-way, and multiway tasks, respectively, and so should be taken as an upper bound on what can be achieved.', 'The third column of Table 2 gives the baseline we calculated from random clusterings.', 'Recall that this is an upper bound on random performance.', 'We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below.', '5.1 Full Feature Set.', 'The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).', 'Although generally higher than the baseline, is well below that of the supervised learner, and and are generally low.', '5.2 Manual Feature Selection.', 'One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task.', 'Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.', 'Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.', '.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.', 'All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.', 'C5.0 is supervised accuracy; Base is on random clusters.', 'Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set.', 'See text for further description.', 'indicated by the class description given in Levin.', 'For each task, then, the linguistically-relevant subset is defined as the union of these subsets for all the classes in the task.', 'The results for these feature sets in clustering are given in the second subcolumn (Ling) under each of the , , and measures in Table 2.', 'On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures.', 'On the 3-way and multiway tasks, there is a larger performance gain using the subset of features, with an increase in the reduction of the error rate (over Base ) of 67% over the full feature set.', 'Overall, there is a small performance gain using the Ling subset of features (with an increase in error rate reduction from 13% to 17%).', 'Moreover, the value for the manually selected features is almost always very much higher than that of the full feature set, indicating that the subset of features is more focused on the properties that lead to a better separation of the data.', 'This performance comparison tentatively suggests that good feature selection can be helpful in our task.', 'However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist.', 'In the next two sections, we present unsupervised and minimally supervised approaches to this problem.', '5.3 Unsupervised Feature Selection.', 'In order to deal with excessive dimensionality, Dash et al.', '(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.', 'Unfortunately, this promising method did not prove practical for our data.', 'We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).', 'Many feature sets performed very well, and some far outperformed our best results using other feature selection methods.', 'However, across our 10 experimental tasks, there was no consistent range of feature ranks or feature set sizes that was correlated with good performance.', 'While we could have selected a threshold that might work reasonably well with our data, we would have little confidence that it would work well in general, considering the inconsistent pattern of results.', '5.4 Semi-Supervised Feature Selection.', 'Unsupervised methods such as Dash et al.’s (1997) are appealing because they require no knowledge external to the data.', 'However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches.', 'In our domain in particular, verb class discovery “in a vacuum” is not necessary.', 'A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines.', 'To model this kind of approach, we selected a sample of five seed verbs from each class.', 'Each set of verbs was judged (by the authors’ intuition alone) to be “representative” of the class.', 'We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1).', 'For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.', 'learner (C5.0) on the seed verbs for those classes, in a 5-fold cross-validation (without boosting).', 'We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task.', 'Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.', 'This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.', 'Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.', 'More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set.', 'The increased reduction in error rate is particularly striking for the 2-way tasks, of 37% for the Seed set compared to 20% for the Ling set.', 'Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full).', 'Thus, not only do we see a sizeable increase in performance, we also obtain tighter and better separated clusters with our proposed feature selection approach.', '5.5 Further Discussion.', 'In our clustering experiments, we find that smaller subsets of features generally perform better than the full set of features.', '(See Table 3 for the number of features in the Ling and Seed sets.)', 'However, not just any small set of features is adequate.', 'We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information (MI) measure used in decision-tree induction (Quinlan, 1986).', 'We calculated the MI of each feature with respect to the classification of the seed verbs, and computed clusterings using the features above a certain MI threshold.', 'This method did not work as well as running C5.0, which presumably captures important feature interactions that are ignored in the individual MI calculations.', 'is the number of classes (a simple linear function roughly approximating the number of features in the Seed sets).', 'Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases).', 'Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification.', 'This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery.', 'We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling).', 'We might also ask, would any subset of verbs do as well?', 'To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class.', 'We found that the mean and values are the same as that of the Seed set reported above, but mean is a little lower.', 'We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others.', 'This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach.', 'Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.', 'However, their study used a small set of five features manually devised for a set of three particular classes.', 'Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.', 'Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.', 'The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks.', 'However, Schulte im Walde’s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).', 'Performance differences may be due to the types of features (ours are noisier, but capture information beyond subcat), or due to the number or size of classes.', 'While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be “easier” than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together.', 'It is a question for future research to explore the effect of these variables in clustering performance.', 'We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.', 'We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.', 'An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion.', 'We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.', 'We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics.', 'Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set.', 'As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner.', 'More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering.', 'Furthermore, we might question the clustering approach itself, in the context of verb class discovery.', 'Rather than trying to separate a set of new verbs into coherent clusters, we suggest that it may be useful to perform a nearest-neighbour type of classification using a seed set, asking for each new verb “is it like these or not?” In some ways our current clustering task is too easy, because all of the verbs are from one of the target classes.', 'In other ways, however, it is too difficult: the learner has to distinguish multiple classes, rather than focus on the important properties of a single class.', 'Our next step is to explore these issues, and investigate other methods appropriate to the practical problem of grouping verbs in a new language.', 'We are indebted to Allan Jepson for helpful discussions and suggestions.', 'We gratefully acknowledge the financial support of NSERC of Canada and Bell University Labs.']\n",
      "H89-2014 not well-formed (invalid token): line 82, column 106\n",
      "['The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.', 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', 'Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.', 'We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.', 'We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.', 'Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).', 'In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP.', 'Unfortunately, these resource are extremely time- consuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.', 'Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.', 'Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.', 'Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.', 'Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.', 'Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.', 'Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.', 'By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.', 'consistency when classifying similar words into categories.', 'For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earth’s atmosphere.', 'These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.', 'Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.', 'Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.', 'This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.', 'Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.', 'The supersenses of these synonyms are then combined to determine the supersense.', 'This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.', '26 Proceedings of the 43rd Annual Meeting of the ACL, pages 26–33, Ann Arbor, June 2005.', 'Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET', 'There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lex- files).', 'For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.', 'Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.', 'Lex-files form a set of coarse-grained sense distinctions within WORDNET.', 'For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.', 'The names and descriptions of the noun lex-files are shown in Table 1.', 'Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).', 'For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time.', 'Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.', 'There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.', 'Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.', 'Ciaramita et al.', '(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appear ing directly underneath.', 'Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget’s (Yarowsky, 1992).', 'These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.', 'Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.', 'They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms.', 'Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.', 'Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.', 'Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.', 'Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.', 'A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English.', 'For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.', 'The co-occurrence window was 500 words which was designed to approximate average document length.', 'Caraballo and Charniak (1999) have explored determining noun specificity from raw text.', 'They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation.', 'The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.', 'Specificity ordering is a necessary step for building a noun hierarchy.', 'However, this approach clearly cannot build a hierarchy alone.', 'For instance, entity is less frequent than many concepts it subsumes.', 'This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.', 'Hearst and Schu¨ tze (1993) flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size.', 'These categories are used to label paragraphs with topics, effectively repeating Yarowsky’s (1992) experiments using the their categories rather than Roget’s thesaurus.', 'Schu¨ tze’s (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).', 'Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.', 'Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.', 'He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.', 'This same technique as is used in our approach to supersense tagging.', 'Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.', 'Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy.', 'They developed an efficient algorithm for estimating the model over hierarchical training data.', 'Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.', 'They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.', 'Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.', 'Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).', 'The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.', 'The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).', 'They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.', 'Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.', 'Some examples from the test sets are given in Table 2 with their supersenses.', 'We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search.', 'The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium’s news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.', 'The components and their sizes including punctuation are given in Table 3.', 'The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.', 'C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.', '2 M 5 5 9 M NA N T S 9 4 2 1 6 7 2 5.', '2 M 5 0 7 M AC QU A I N T 1 03 3 46 1 2 1.', '3 M 4 9 1 M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).', 'Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.', 'The rest of the pipeline is described in the next section.', 'Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.', 'This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in.', 'In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.', 'The key parameters are the context extraction method and the similarity measure used to compare context vectors.', 'Our approach to vector-space similarity is based on the SEXTANT system described in Grefenstette (1994).', 'Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.', 'SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word.', 'The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.', 'We describe the shallow pipeline in detail below.', 'Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette’s weighted JACCARD measure performed the best: R E L AT I O N D E S C R I P T I O N adj noun–adjectival modifier relation dobj verb–direct object relation iobj verb–indirect object relation nn noun–noun modifier relation nnprep noun–prepositional head relation subj verb–subject relation Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001) – and is very efficient, analysing over 80 000 words per second.', 'morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass.', 'This inconsis L min(wgt(w1 , ∗r , ∗wI ), wgt(w2 , ∗r , ∗wI )) L max(wgt(w1 , ∗r , ∗wI ), wgt(w2 , ∗r , ∗wI )) (1) tency is problematic when using morphological analysis to smooth vector-space models.', 'However, morphological smoothing still produces better results in practice.', 'where wgt(w, r, wt ) is the weight function for relation (w, r, wt ).', 'Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction.', 'Here, the t-test compares the joint and product probability distributions of the headword and context: 6.3 Grammatical Relation Extraction.', 'After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.', 'This consists of five passes over each sentence that first identify noun and verb phrase heads and p(w, r, wt ) − p(∗, r, wt )p(w, ∗, ∗) p(∗, r, wt )p(w, ∗, ∗) (2) then collect grammatical relations between each common noun and its modifiers and verbs.', 'A global list of grammatical relations generated by each pass is maintained where ∗ indicates a global sum over that element of the relation tuple.', 'JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen’s configuration for our super- sense tagging experiments.', '6.1 Part of Speech Tagging and Chunking.', 'Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).', 'The only similar performing tool is the Trigrams ‘n’ Tags tagger (Brants, 2000) which uses a much simpler statistical model.', 'Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.', 'Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette’s table-based results, i.e. the SEXTANT always prefers noun attachment.', '6.2 Morphological Analysis.', 'Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation.', 'morpha has wide coverage – nearly 100% across the passes.', 'The global list is used to determine if a word is already attached.', 'Once all five passes have been completed this association list contains all of the noun- modifier/verb pairs which have been extracted from the sentence.', 'The types of grammatical relation extracted by SEXTANT are shown in Table 4.', 'For relations between nouns (nn and nnprep), we also create inverse relations (wt , rt , w) representing the fact that wt can modify w. The 5 passes are described below.', 'Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifier’s right, up to a preposition or the phrase end.', 'This corresponds to assuming right-branching noun compounds.', 'Within each NP only the NP and PP heads remain unattached.', 'Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs.', 'If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created.', 'This corresponds to assuming right-branching PP attachment.', 'After this phrase only the NP head remains unattached.', 'Tense Determination The rightmost verb in each VP is considered the head.', 'A VP is initially categorised as active.', 'If the head verb is a form of be then the VP becomes attributive.', 'Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active.', 'Only the noun heads on either side of VPs remain unattached.', 'The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP.', 'Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head.', 'If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created.', 'For example, antigen is the subject of represent.', 'Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head.', 'If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created.', 'If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.', 'If the following phrase is a PP then an indirect object (iobj) relation is created.', 'The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.', 'However, SEXTANT always attaches the PP to the previous phrase.', 'Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached).', 'Progressive verbs can function as nouns, verbs and adjectives and once again a na¨ıve approximation to the correct attachment is made.', 'Any progressive verb which appears after a determiner or quantifier is considered a noun.', 'Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects.', 'Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation.', 'Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences.', 'Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.', 'This technique is similar to Hearst and Schu¨ tze (1993) and Widdows (2003).', 'However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.', 'In these cases, our SUFFIX EXAMPLE SUPERSENSEness remoteness attribute -tion, -ment annulment act -ist, -man statesman person -ing, -ion bowling act -ity viscosity attribute -ics, -ism electronics cognition -ene, -ane, -ine arsine substance -er, -or, -ic, -ee, -an mariner person -gy entomology cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.', 'These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6.', 'The supersense guessing rules are given in Table 5.', 'If none of the rules match, then the default supersense artifact is assigned.', 'The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.', 'Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6.', 'There are many parameters to consider: • how many extracted synonyms to use; • how to weight each synonym’s vote; • whether unreliable synonyms should be filtered out; • how to deal with polysemous synonyms.', 'The experiments described below consider a range of options for these parameters.', 'In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.', 'We have experimented with up to 200 voting extracted synonyms.', 'There are several ways to weight each synonym’s contribution.', 'The simplest approach would be to give each synonym the same weight.', 'Another approach is to use the scores returned by the similarity system.', 'Alternatively, the weights can use the ranking of the extracted synonyms.', 'Again these options have been considered below.', 'A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.', 'The final issue is how to deal with polysemy.', 'Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?', 'Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.', 'A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.', 'This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.', 'We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).', 'The experiments were performed by considering all possible configurations of the parameters described above.', 'The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.', 'The initial weight could then be divided by the number of supersenses to share out the weight (SHARED).', 'The weight could also be divided by the rank (RANK) to penalise supersenses further down the list.', 'The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties.', 'The extracted synonyms are filtered before contributing to the vote with their supersense(s).', 'This filtering involves checking that the synonym’s frequency and number of contexts are large enough to ensure it is reliable.', 'We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonym’s frequency and the number of contexts it appears in.', 'The next question is how many synonyms are considered.', 'We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms.', 'All of the top performing configurations used 50 synonyms.', 'We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms.', 'However, such a filter turned out to make little difference.', 'Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules.', 'This is determined by looking at the frequency and number of attributes for the unknown word.', 'Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.', 'The results are summarised in Table 6.', 'The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.', 'On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set.', 'By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set.', 'Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.', 'The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder.', 'Table 7 shows the breakdown in performance for each supersense.', 'The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages.', 'The most frequent supersenses in both test sets were person, attribute and act.', 'Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance.', 'This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.', 'These factors are conducive for extracting reliable synonyms.', 'These results also support Ciaramita and Johnson’s view that abstract concepts like communication, cognition and state are much harder.', 'We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.', 'Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.', 'An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words.', 'This has the advantage of producing a much smaller number of vectors to compare against.', 'In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors.', 'The question now becomes how to construct vectors of supersenses.', 'The most obvious solution is to sum the context vectors across the words which have each supersense.', 'However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word.', 'Also, the same questions arise in the construction of these vectors.', 'How are words with multiple supersenses handled?', 'Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results.', 'One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in).', 'However, given the sparseness of the data this may not leave very large context vectors.', 'A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.', 'Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons.', 'There are a number of problems our system does not currently handle.', 'Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).', 'Further, our similarity system does not currently incorporate multi-word terms.', 'We overcome this by using the synonyms of the last word in the multi-word term.', 'However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term.', 'Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.', 'We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.', 'We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.', 'Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.', 'Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu¨ tze (1993) and Widdows (2003).', 'To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.', 'We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.', 'Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.', 'Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).', 'This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.', 'Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.', 'This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.', 'We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.', 'This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.']\n",
      "E03-1020 not well-formed (invalid token): line 34, column 62\n",
      "['The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.', ' For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.', ' In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.', ' In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.', ' The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.', ' We evaluate the system\\'s performance by comparing its segmentation \\'Tudgments\" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.', 'Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).', 'An initial step of any text\\xad analysis task is the tokenization of the input into words.', 'For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.', \"Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces.\", \"A moment's reflection will reveal that things are not quite that simple.\", \"There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am.\", 'If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up.', \"And if one is interested in TIS, one would probably consider the single orthographic word ACL to consist of three phonological words-lei s'i d/-corresponding to the pronunciation of each of the letters in the acronym.\", 'Space- or punctuation-delimited * 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.', 'Email: rlls@bell-labs.', 'com t 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.', 'Email: cls@bell-labs.', 'com t 600 Mountain Avenue, 2c278, Murray Hill, NJ 07974, USA.', 'Email: gale@research.', 'att.', \"com §Cambridge, UK Email: nc201@eng.cam.ac.uk © 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.\", 'In (b) is a plausible segmentation for this sentence; in (c) is an implausible segmentation.', 'orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.', 'Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion \"orthographic word\" is not universal.', 'Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writ\\xad ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.', 'In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion \"word\" has never played a role in Chinese philological tradition, and the idea that Chinese lacks any\\xad thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).', 'Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.', 'All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..', \"2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..\", '3 Throughout this paper we shall give Chinese examples in traditional orthography, followed.', 'immediately by a Romanization into the pinyin transliteration scheme; numerals following each pinyin syllable represent tones.', 'Examples will usually be accompanied by a translation, plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose.', 'In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.', \"raphy: A ren2 'person' is a fairly uncontroversial case of a monographemic word, and rplil zhong1guo2 (middle country) 'China' a fairly uncontroversial case of a di\\xad graphernic word.\", \"The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.'\", 'Arguably this consists of about three phonological words.', 'On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.', 'Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.', 'For example, suppose one is building a ITS system for Mandarin Chinese.', 'For that application, at a minimum, one would want to know the phonological word boundaries.', 'Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character.', 'However, there are several reasons why this approach will not in general work: 1.', 'Many hanzi have more than one pronunciation, where the correct.', \"pronunciation depends upon word affiliation: tfJ is pronounced deO when it is a prenominal modification marker, but di4 in the word §tfJ mu4di4 'goal'; fl; is normally ganl 'dry,' but qian2 in a person's given name.\", 'including Third Tone Sandhi (Shih 1986), which changes a 3 (low) tone into a 2 (rising) tone before another 3 tone: \\'j\";gil, xiao3 [lao3 shu3] \\'little rat,\\' becomes xiao3 { lao2shu3 ], rather than xiao2 { lao2shu3 ], because the rule first applies within the word lao3shu3 \\'rat,\\' blocking its phrasal application.', '3.', 'In various dialects of Mandarin certain phonetic rules apply at the word.', 'level.', \"For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO.\", 'The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own.', '4.', 'TIS systems in general need to do more than simply compute the.', 'pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances.', 'It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.', 'Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information.', 'Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.', \"The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.\", 'There are thus some very good reasons why segmentation into words is an important task.', 'A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.', 'For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.', 'Among these are words derived by various productive processes, including: 1.', 'Morphologically derived words such as, xue2shengl+men0.', \"(student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl.\", '2.', \"Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.'\", 'Of course, we.', \"can expect famous names like Zhou Enlai's to be in many dictionaries, but names such as :fi lf;f; shi2jil-lin2, the name of the second author of this paper, will not be found in any dictionary.\", \"'Malaysia.'\", \"Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PM± R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found.\", 'In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.', \"The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities).\", 'The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.', 'It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen con\\xad structions, including morphological derivatives and personal names.', 'We will evaluate various specific aspects of the segmentation, as well as the overall segmentation per\\xad formance.', 'This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.', 'Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.', '2.', 'A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.', 'The first point we need to address is what type of linguistic object a hanzi repre\\xad sents.', 'Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.', 'The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologi\\xad cally.', \"Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes.\", \"Of course, since the number of attested (phonemic) Mandarin syllables (roughly 1400, including tonal distinctions) is far smaller than the number of morphemes, it follows that a given syllable could in principle be written with any of several different hanzi, depending upon which morpheme is intended: the syllable zhongl could be lfl 'middle,''clock,''end,' or ,'loyal.'\", 'A morpheme, on the other hand, usually corresponds to a unique hanzi, though there are a few cases where variant forms are found.', 'Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes: The prenominal modifi\\xad cation marker eg deO is presumably a different morpheme from the second morpheme of §eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi.', \"Following the system devised under the Qing emperor Kang Xi, hanzi have traditionally been classified according to a set of approximately 200 semantic radicals; members of a radical class share a particular structural component, and often also share a common meaning (hence the term 'semantic').\", \"For example, hanzi containing the INSECT radical !R tend to denote insects and other crawling animals; examples include tr wal 'frog,' feng1 'wasp,' and !Itt she2 'snake.'\", \"Similarly, hanzi sharing the GHOST radical _m tend to denote spirits and demons, such as _m gui3 'ghost' itself, II: mo2 'demon,' and yan3 'nightmare.'\", 'While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking: for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits.', 'As we shall argue, the semantic class affiliation of a hanzi constitutes useful information in predicting its properties.', '3.', 'Previous Work.', 'There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).', 'Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexi\\xad cal rule-based approaches, and approaches that combine lexical information with sta\\xad tistical information.', 'The present proposal falls into the last group.', 'Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.', 'In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.', 'Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.', 'A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.', '4 To be sure, it is not always true that a hanzi represents a syllable or that it represents a morpheme.', 'For.', \"example, in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns, and which is phonologically incorporated into the syllable to which it attaches: thus men2+r (door+R) 'door' is realized as mer2.\", 'This is orthographically represented as 7C.', \"so that 'door' would be and in this case the hanzi 7C, does not represent a syllable.\", \"Similarly, there is no compelling evidence that either of the syllables of f.ifflll binllang2 'betelnut' represents a morpheme, since neither can occur in any context without the other: more likely fjfflll binllang2 is a disyllabic morpheme.\", '(See Sproat and Shih 1995.)', 'However, the characterization given in the main body of the text is correct sufficiently often to be useful.', 'Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.', 'Nonstochastic lexical-knowledge-based approaches have been much more numer\\xad ous.', 'Two issues distinguish the various proposals.', 'The first concerns how to deal with ambiguities in segmentation.', 'The second concerns the methods used (if any) to ex\\xad tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.', 'The most popular approach to dealing with seg\\xad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.', 'This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin\\xad ning) of the sentence is reached.', 'Papers that use this method or minor variants thereof include Liang (1986), Li et al.', '(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).', 'The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.', 'Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.', 'Some approaches depend upon some form of con\\xad straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).', 'Others depend upon various lexical heuris\\xad tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.', 'Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).', 'Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or cost\\xad based scoring mechanism.', 'Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.', 'The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).', 'More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.', 'Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.', 'Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis.', 'Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular se\\xad quence of hanzi might be a name, but that it is likely to be a name with some probabil\\xad ity.', 'Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).', 'Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not ac\\xad tually tag the words as belonging to one or another class of expression.', 'This is not ideal for some applications, however.', 'For instance, for TTS it is necessary to know that a particular sequence of hanzi is of a particular category because that knowl\\xad edge could affect the pronunciation; consider, for example the issues surrounding the pronunciation of ganl I qian2 discussed in Section 1.', 'Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.', 'However, it is almost universally the case that no clear definition of what constitutes a \"correct\" segmentation is given, so these performance measures are hard to evaluate.', 'Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.', 'In a few cases, the criteria for correctness are made more explicit.', 'For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.', 'Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.', 'The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.', 'The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.', 'Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.', 'Chinese word segmentation can be viewed as a stochastic transduction problem.', 'More formally, we start by representing the dictionary D as a Weighted Finite State Trans\\xad ducer (WFST) (Pereira, Riley, and Sproat 1994).', 'Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels.', 'Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.', 'Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).', 'We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.', 'selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.', 'then define the best segmentation to be the cheapest or best path in Id(I) o D* (i.e., Id(I) composed with the transitive closure of 0).6 Consider the abstract example illustrated in Figure 2.', 'In this example there are four \"input characters,\" A, B, C and D, and these map respectively to four \"pronunciations\" a, b, c and d. Furthermore, there are four \"words\" represented in the dictionary.', 'These are shown, with their associated costs, as follows: ABj nc 4.0 AB C/jj 6.0 CD /vb 5.', '0 D/ nc 5.0 The minimal dictionary encoding this information is represented by the WFST in Figure 2(a).', 'An input ABCD can be represented as an FSA as shown in Figure 2(b).', 'This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).', 'This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.', 'Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model.', 'It is important to bear in mind, though, that this is not an inherent limitation of the model.', 'For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)', 'In Section 6 we dis\\xad cuss other issues relating to how higher-order language models could be incorporated into the model.', '4.1 Dictionary Representation.', 'As we have seen, the lexicon of basic words and stems is represented as a WFST; most arcs in this WFST represent mappings between hanzi and pronunciations, and are costless.', 'Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word.', 'The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.', 'Note that hanzi that are not grouped into dictionary words (and are not identified as single\\xad hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.', 'Other strategies could readily 6 As a reviewer has pointed out, it should be made clear that the function for computing the best path is. an instance of the Viterbi algorithm.', '7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.', 'It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.', '(a) IDictionary D I D:d/0.000 B:b/0.000 B:b/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cps:nd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.', 'The transitive closure of the dictionary in (a) is composed with Id(input) (b) to form the WFST (c).', 'The segmentation chosen is the best path through the WFST, shown in (d).', '(In this figure eps is c) be implemented, though, such as a maximal-grouping strategy (as suggested by one reviewer of this paper); or a pairwise-grouping strategy, whereby long sequences of unattached hanzi are grouped into two-hanzi words (which may have some prosodic motivation).', 'We have not to date explored these various options.', 'Word frequencies are estimated by a re-estimation procedure that involves apply\\xad ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.', 'newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.', 'This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.', 'The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.', 'Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in partic\\xad ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.', 'In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.', 'Note also that the costs currently used in the system are actually string costs, rather than word costs.', \"This is because our corpus is not annotated, and hence does not distinguish between the various words represented by homographs, such as, which could be /adv jiangl 'be about to' orInc jiang4 '(military) general'-as in 1j\\\\xiao3jiang4 'little general.'\", 'In such cases we assign all of the estimated probability mass to the form with the most likely pronunciation (determined by inspection), and assign a very small probability (a very high cost, arbitrarily chosen to be 40) to all other variants.', 'In the case of, the most common usage is as an adverb with the pronunciation jiangl, so that variant is assigned the estimated cost of 5.98, and a high cost is assigned to nominal usage with the pronunciation jiang4.', 'The less favored reading may be selected in certain contexts, however; in the case of , for example, the nominal reading jiang4 will be selected if there is morphological information, such as a following plural affix ir, menD that renders the nominal reading likely, as we shall see in Section 4.3.', \"Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, g:t¥ zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and i¥inl.\", \"nan2gual 'pumpkin.'\", \"4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1.\", \"As noted, this sentence consists of four words, namely B X ri4wen2 'Japanese,' :¥, zhanglyu2 'octopus/ :&P:l zen3me0 'how,' and IDt shuol 'say.'\", \"As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with X:¥ wen2zhangl 'essay/ and f!!.\", \"yu2 'fish.'\", 'Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.', '4.3 Morphological Analysis.', 'The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.', 'One class comprises words derived by productive morphologi\\xad cal processes, such as plural noun formation using the suffix ir, menD.', '(Other classes handled by the current system are discussed in Section 5.)', 'The morphological anal\\xadysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.', 'each word in the lexicon whether or not each string is actually an instance of the word in question.', '£ : _ADV: 5.88 If:!', \":zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 £: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I •=- :il: .;ss:;zhangt • '-:.\", 'I • JAPANS :rl4 .·········\"\\\\)··········\"o·\\'·······\"\\\\:J········· ·········\\'\\\\; . \\'.:: ..........0 6.51 9.51 : jj / JAPANESE OCTOPUS 10·28i£ :_nc HOW SAY f B :rl4 :il: :wen2 t \\'- • :zhang!', '!!:\\\\ :yu2 e:_nc [::!!:zen3 l!f :moO t:_adv il!:shuot ,:_vb i i i 1 • 10.03 13...', '7.96 5.55 1 l...................................................................................................................................................................................................J..', \"Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'.\", 'A non-optimal analysis is shown with dotted lines in the bottom frame.', 'ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t:-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.', 'However, for our purposes it is not sufficient to repre\\xad sent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word.', 'For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry.', \"So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02.\", \"But we also need an estimate of the probability for a non-occurring though possible plural form like i¥JJ1l.f, nan2gua1-men0 'pumpkins.'\", '10 Here we use the Good-Turing estimate (Baayen 1989; Church and Gale 1991), whereby the aggregate probability of previously unseen instances of a construction is estimated as ni/N, where N is the total number of observed tokens and n1 is the number of types observed only once.', 'Let us notate the set of previously unseen, or novel, members of a category X as unseen(X); thus, novel members of the set of words derived in f, menO will be de\\xad noted unseen(f,).', 'For irt the Good-Turing estimate just discussed gives us an estimate of p(unseen(f,) I f,)-the probability of observing a previously unseen instance of a construction in ft given that we know that we have a construction in f,.', 'This Good\\xad Turing estimate of p(unseen(f,) If,) can then be used in the normal way to define the probability of finding a novel instance of a construction in ir, in a text: p(unseen(f,)) = p(unseen(f,) I f,) p(fn Here p(ir,) is just the probability of any construction in ft as estimated from the frequency of such constructions in the corpus.', 'Finally, as\\xad suming a simple bigram backoff model, we can derive the probability estimate for the particular unseen word i¥1J1l.', 'irL as the product of the probability estimate for i¥JJ1l., and the probability estimate just derived for unseen plurals in ir,: p(i¥1J1l.ir,) p(i¥1J1l.)p(unseen(f,)).', 'The cost estimate, cost(i¥JJ1l.fn is computed in the obvious way by summing the negative log probabilities of i¥JJ1l.', 'and f,.', 'Figure 5 shows how this model is implemented as part of the dictionary WFST.', 'There is a (costless) transition between the NC node and f,.', 'The transition from f, to a final state transduces c to the grammatical tag PL with cost cost(unseen(f,)): cost(i¥JJ1l.ir,) == cost(i¥JJ1l.)', '+ cost(unseen(fm, as desired.', \"For the seen word ir, 'gen\\xad erals,' there is an c:NC transduction from to the node preceding ir,; this arc has cost cost( f,) - cost(unseen(f,)), so that the cost of the whole path is the desired cost( f,).\", 'This representation gives ir, an appropriate morphological decomposition, pre\\xad serving information that would be lost by simply listing ir, as an unanalyzed form.', 'Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural.', 'An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20, p < 0.005; see Figure 6.', 'This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form.', '10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.', 'attaching to terms denoting human beings.', \"However, it is possible to personify any noun, so in children's stories or fables, i¥JJ1l.\", \"f, nan2gual+men0 'pumpkins' is by no means impossible.\", 'J:j:l :zhongl :0.0 ;m,Jlong4 :0.0 (mHHaryg9tltHBI) £: _ADV: 5.98 ¥ :hua2:o.o E :_NC: 4.41 :mln2:o.o mm : guo2 : 0.0 (RopubllcofChlna) .....,.', '0 Figure 5 An example of affixation: the plural affix.', '4.4 Chinese Personal Names.', 'Full Chinese personal names are in one respect simple: they are always of the form family+given.', 'The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones.', 'Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.', 'wo rd => na m e 2.', 'na me =>1 ha nzi fa mi ly 2 ha nzi gi ve n 3.', 'na me =>1 ha nzi fa mi ly 1 ha nzi gi ve n 4.', 'na me =>2 ha nzi fa mi ly 2 ha nzi gi ve n 5.', 'na me =>2 ha nzi fa mi ly 1 ha nzi gi ve n 6.1 ha nzi fa mi ly => ha nz ii 7.2 ha nzi fa mi ly => ha nzi i ha nz ij 8.1 ha nzi gi ve n => ha nz ii 9.2 ha nzi giv en => ha nzi i ha nz ij The difficulty is that given names can consist, in principle, of any hanzi or pair of hanzi, so the possible given names are limited only by the total number of hanzi, though some hanzi are certainly far more likely than others.', 'For a sequence of hanzi that is a possible name, we wish to assign a probability to that sequence qua name.', 'We can model this probability straightforwardly enough with a probabilistic version of the grammar just given, which would assign probabilities to the individual rules.', 'For example, given a sequence F1G1G2, where F1 is a legal single-hanzi family name, and Plural Nouns X g 0 g \"\\' X X 0 T!i c\"\\'.', '0 X u} \"\\' o; .2 X X><X X XX X X X X X X x X X X X X x X V X X X X .;t\\'*- XXX:OX X X X X X X 9 x X X XX XX X X X X X X X XXX:< X X>O<XX>!KXX XI<>< »C X X XX :X: X X \"\\' X X XX >OO<X>D<XIK X X X X X X --XX»: XXX X X»C X X«X...C:XXX X Xll< X X ><XX>IIC:liiC:oiiiiCI--8!X:liiOC!I!S8K X X X 10 100 1000 10000 log(F)_base: R\"2=0.20 (p < 0.005) X 100000 Figure 6 Plot of log frequency of base noun, against log frequency of plural nouns.', 'G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of: • the probability that a word chosen randomly from a text will be a name-p(rule 1), and • the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and • the probability that the family name is the particular hanzi F1-p(rule 6), and • the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.', '(1992).', \"The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.\", 'This model is easily incorporated into the segmenter by building a WFST restrict\\xad ing the names to the four licit types, with costs on the arcs for any particular name summing to an estimate of the cost of that name.', \"This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.\", \"There are two weaknesses in Chang et al.'s model, which we improve upon.\", 'First, the model assumes independence between the first and second hanzi of a double given name.', \"Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model.\", 'As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost.', 'The second weakness is purely conceptual, and probably does not affect the per\\xad formance of the model.', 'For previously unseen hanzi in given names, Chang et al. assign a uniform small cost; but we know that some unseen hanzi are merely acci\\xad dentally missing, whereas others are missing for a reason-for example, because they have a bad connotation.', 'As we have noted in Section 2, the general semantic class to which a hanzi belongs is often predictable from its semantic radical.', 'Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class.', 'Other good classes include JADE and GOLD; other bad classes are DEATH and RAT.', 'We can better predict the probability of an unseen hanzi occurring in a name by computing a within-class Good-Turing estimate for each radical class.', \"Assuming unseen objects within each class are equiprobable, their probabilities are given by the Good-Turing theorem as: cis E( n'J.ls) Po oc N * E(N8ls) (2) where p815 is the probability of one unseen hanzi in class cls, E(n'J.15 ) is the expected number of hanzi in cls seen once, N is the total number of hanzi, and E(N(/ 5 ) is the expected number of unseen hanzi in class cls.\", 'The use of the Good-Turing equation presumes suitable estimates of the unknown expectations it requires.', 'In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of.', 'hanzi in the various name positions, derived from a million names.', \"12 One class of full personal names that this characterization does not cover are married women's names.\", \"where the husband's family name is optionally prepended to the woman's full name; thus ;f:*lf#i xu3lin2-yan2hai3 would represent the name that Ms. Lin Yanhai would take if she married someone named Xu.\", 'This style of naming is never required and seems to be losing currency.', 'It is formally straightforward to extend the grammar to include these names, though it does increase the likelihood of overgeneration and we are unaware of any working systems that incorporate this type of name.', 'We of course also fail to identify, by the methods just described, given names used without their associated family name.', 'This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.', 'Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.', 'JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14.', '98 15.', '52 15.', '76 16.', '25 16.', '30 16.', '42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.', 'In the numerator, however, the counts of ni1s are quite irregular, in\\xad cluding several zeros (e.g., RAT, none of whose members were seen).', 'However, there is a strong relationship between ni1s and the number of hanzi in the class.', 'For E(ni1s), then, we substitute a smooth S against the number of class elements.', 'This smooth guarantees that there are no zeroes estimated.', 'The final estimating equation is then: (3) Since the total of all these class estimates was about 10% off from the Turing estimate n1/N for the probability of all unseen hanzi, we renormalized the estimates so that they would sum to n 1jN.', 'This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.', 'Note that the good classes JADE, GOLD and GRASS have lower costs than the bad classes SICKNESS, DEATH and RAT, as desired, so the trend observed for the results of this method is in the right direction.', '4.5 Transliterations of Foreign Words.', 'Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.', 'Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identi\\xad fication of such names is tricky.', \"Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal Chi\\xad nese personal name, retains a foreign flavor because of liM.\", 'As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil\\xad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.', 'As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).', 'Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.', 'For instance, the common \"suffixes,\" -nia (e.g.,.', 'Virginia) and -sia are normally transliterated as fbSi!', 'ni2ya3 and @5:2 xilya3, respectively.', 'The interdependence between fb or 1/!i, and 5:2 is not captured by our model, but this could easily be remedied.', 'logical rules, and personal names; the transitive closure of the resulting machine is then computed.', 'In this section we present a partial evaluation of the current system, in three parts.', \"The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis.\", 'To date we have not done a separate evaluation of foreign-name recognition.', 'Evaluation of the Segmentation as a Whole.', 'Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.', 'The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.', 'Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.', 'To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.)', 'We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.', 'Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud.', \"An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.\", '(See also Wu and Fung [1994].)', 'Various segmentation approaches were then compared with human performance: 1.', 'A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.', '2.', 'An anti-greedy algorithm, AG: instead of the longest match, take the.', 'shortest match at each point.', '3.', 'The method being described-henceforth ST..', 'Two measures that can be used to compare judgments are: 1.', 'Precision.', 'For each pair of judges consider one judge as the standard,.', \"computing the precision of the other's judgments relative to this standard.\", '2.', 'Recall.', 'For each pair of judges, consider one judge as the standard,.', \"computing the recall of the other's judgments relative to this standard.\", 'Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.', 'from the subset of the United Informatics corpus not used in the training of the models.', 'Table 2 Similarity matrix for segmentation judgments.', 'Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively, the recall and precision.', 'We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity.', 'Table 2 shows these similarity measures.', 'The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that dis\\xad tance matrix, and plotting the first two most significant dimensions.', 'The result of this is shown in Figure 7.', 'The horizontal axis in this plot represents the most significant dimension, which explains 62% of the variation.', 'In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).', 'This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.', 'As can be seen, GR and this \"pared-down\" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.', 'It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together, and the third Tai\\xad wan speaker is also close in the most significant dimension (the x axis).', 'Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.', 'The breakdown of the different types of words found by ST in the test corpus is given in Table 3.', 'Clearly the percentage of productively formed words is quite small (for this particular corpus), meaning that dictionary entries are covering most of the 15 GR is .73 or 96%..', '16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.', 'That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case.', 'In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.', 'As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes.', 'The question is how to normalize the probabilities in such a way that smaller groupings have a better shot at winning.', 'This is an issue that we have not addressed at the current stage of our research.', 'i..f,..', '\"c\\' 0 + 0 \"0 \\' • + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y • Taiwan 0 ·;; 0 c CD E i5 0\"\\' 9 9 • Mainland • • • • -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.', 'The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question.', 'Table 3 Classes of words found by ST for the test corpus.', 'Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.', 'Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.', 'It may seem surprising to some readers that the interhuman agreement scores reported here are so low.', 'However, this result is consistent with the results of ex\\xad periments discussed in Wu and Fung (1994).', 'Wu and Fung introduce an evaluation method they call nk-blind.', 'Under this scheme, n human judges are asked independently to segment a text.', 'Their results are then compared with the results of an automatic segmenter.', 'For a given \"word\" in the automatic segmentation, if at least k of the hu\\xad man judges agree that this is a word, then that word is considered to be correct.', 'For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.', 'Proper-Name Identification.', 'To evaluate proper-name identification, we randomly se\\xad lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically, tagging personal names; note that for names, there is always a sin\\xad gle unambiguous answer, unlike the more general question of which segmentation is correct.', 'The performance was 80.99% recall and 61.83% precision.', 'Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.', \"However, we have reason to doubt Chang et al.'s performance claims.\", 'Without using the same test corpus, direct comparison is obviously difficult; fortunately, Chang et al. include a list of about 60 sentence fragments that exemplify various categories of performance for their system.', 'The performance of our system on those sentences ap\\xad peared rather better than theirs.', 'On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.', 'However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision.', 'On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.', 'Note that it is in precision that our over\\xad all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.', 'Thus we have some confidence that our own performance is at least as good as that of Chang et al.', '(1992).', \"In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.\", 'Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang.', 'Examples are given in Table 4.', 'In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repre\\xad senting differences in the capabilities of the model per se.', 'The first issue relates to the completeness of the base lexicon.', \"The Wang, Li, and Chang system fails on fragment (b) because their system lacks the word youlyoul 'soberly' and misinterpreted the thus isolated first youl as being the final hanzi of the preceding name; similarly our system failed in fragment (h) since it is missing the abbreviation i:lJI!\", \"tai2du2 'Taiwan Independence.'\", 'This is a rather important source of errors in name identifi\\xad cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.', \"17 They also provide a set of title-driven rules to identify names when they occur before titles such as $t. 1: xianlshengl 'Mr.' or i:l:itr!J tai2bei3 shi4zhang3 'Taipei Mayor.'\", 'Obviously, the presence of a title after a potential name N increases the probability that N is in fact a name.', 'Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose.', '18 We are grateful to ChaoHuang Chang for providing us with this set.', \"Note that Wang, Li, and Chang's.\", 'set was based on an earlier version of the Chang et a!.', 'paper, and is missing 6 examples from the A set.', \"19 We note that it is not always clear in Wang, Li, and Chang's examples which segmented words.\", 'constitute names, since we have only their segmentation, not the actual classification of the segmented words.', 'Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical.', 'Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).', 'Our System Wang, Li, and Chang a. 1\\\\!f!IP Eflltii /1\\\\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. \"*:t: w _t ff 1 \"* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 \\'music by Chen Zhongshen \\' huang2rong2 youlyoul de dao4 \\'Huang Rong said soberly\\' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 \\'after the county president You Qing had assumed the position\\' lin2 quan2 \\'Lin Quan\\' wang2jian4 \\'Wang Jian\\' oulyang2-ke4 \\'Ouyang Ke\\' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 \\'because it cannot permit Taiwan Independence so\\' silfa3-yuan4zhang3 lin2yang2-gang3 \\'president of the Judicial Yuan, Lin Yanggang\\' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol \\'Lin Zhanghu will give an ex\\xad planation live\\' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 \\'in two years the distributed money will stop\\' gaoltangl da4chi2 ye1zi0 fen3 \\'chicken stock, a tablespoon of coconut flakes\\' you2qingl ru4zhu3 xian4fu3 lwu4 \\'after You Qing headed the county government\\' Table 5 Performance on morphological analysis.', 'Affix Pron Base category N found N missed (recall) N correct (precision) t,-,7 The second issue is that rare family names can be responsible for overgeneration, especially if these names are otherwise common as single-hanzi words.', \"For example, the Wang, Li, and Chang system fails on the sequence 1:f:p:]nian2 nei4 sa3 in (k) since 1F nian2 is a possible, but rare, family name, which also happens to be written the same as the very common word meaning 'year.'\", 'Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name.', 'Finally, the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words.', 'An example is in (i), where the system fails to group t;,f;?\"$?t!: lin2yang2gang3 as a name, because all three hanzi can in principle be separate words (t;,f; lin2 \\'wood\\';?\"$ yang2 \\'ocean\\'; ?t!; gang3 \\'harbor\\').', 'In many cases these failures in recall would be fixed by having better estimates of the actual prob\\xad abilities of single-hanzi words, since our estimates are often inflated.', \"A totally non\\xad stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended.\", 'Evaluation of Morphological Analysis.', 'In Table 5 we present results from small test cor\\xad pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.', \"The first four affixes are so-called resultative affixes: they denote some prop\\xad erty of the resultant state of a verb, as in E7 wang4bu4-liao3 (forget-not-attain) 'cannot forget.'\", 'The last affix in the list is the nominal plural f, men0.20 In the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure).', 'In this paper we have argued that Chinese word segmentation can be modeled ef\\xad fectively using weighted finite-state transducers.', 'This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.', 'Other kinds of productive word classes, such as company names, abbreviations (termed fijsuolxie3 in Mandarin), and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO, but as part of a resultative it is liao3..', 'handled given appropriate models.', '(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)', 'We have argued that the proposed method performs well.', 'However, some caveats are in order in comparing this method (or any method) with other approaches to seg\\xad mentation reported in the literature.', 'First of all, most previous articles report perfor\\xad mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.', 'What both of these approaches presume is that there is a sin\\xad gle correct segmentation for a sentence, against which an automatic algorithm can be compared.', 'We have shown that, at least given independent human judgments, this is not the case, and that therefore such simplistic measures should be mistrusted.', 'This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.', 'May 1995).', 'However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.', 'Second, comparisons of different methods are not meaningful unless one can eval\\xad uate them on the same corpus.', 'Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.', 'One hopes that such a corpus will be forth\\xad coming.', 'Finally, we wish to reiterate an important point.', 'The major problem for our seg\\xad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).', 'We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.', 'However, there will remain a large number of words that are not readily adduced to any produc\\xad tive pattern and that would simply have to be added to the dictionary.', 'This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.', 'The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.', 'However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.', 'For example, as Gan (1994) has noted, one can construct examples where the segmen\\xad tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.', \"Two sets of examples from Gan are given in (1) and (2) (:::::: Gan's Appendix B, exx.\", 'lla/llb and 14a/14b respectively).', 'In (1) the sequencema3lu4 cannot be resolved locally, but depends instead upon broader context; similarly in (2), the sequence :::tcai2neng2 cannot be resolved locally: 1.', \"(a) 1 § . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1§: . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' :$ chel jinglguo4 car pass by 2.\", \"(a) I f f fi * fi :1 }'l ij 1§: {1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty?'\", \"(b) 89 :1 t& tal de cai2neng2 hen3 he DE talent very 'He has great talent' f.b ga ol hig h While the current algorithm correctly handles the (b) sentences, it fails to handle the (a) sentences, since it does not have enough information to know not to group the sequences.ma3lu4 and?]cai2neng2 respectively.\", \"Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases).\", 'An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).', 'A high-level relation is agent, which relates an animate nominal to a predicate.', 'Particular instances of relations are associated with goodness scores.', 'Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are \"popular\" or not.', \"While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.\", 'Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.', 'For the examples given in (1) and (2) this certainly seems possible.', 'Consider first the examples in (2).', \"The segmenter will give both analyses :1 cai2 neng2 'just be able,' and ?]cai2neng2 'talent,' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them.\", \"In (2a), we want to split the two morphemes since the correct analysis is that we have the adverb :1 cai2 'just,' the modal verb neng2 'be able' and the main verb R: Hke4fu2 'overcome'; the competing analysis is, of course, that we have the noun :1 cai2neng2 'talent,' followed by }'lijke4fu2 'overcome.'\", 'Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available, then that is to be preferred over Noun+ Verb: such a rule could be stated in terms of (finite-state) local grammars in the sense of Mohri (1993).', \"Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'\", \"However, there is again local grammatical information that should favor the split in the case of (1a): both .ma3 'horse' and .ma3 lu4 are nouns, but only .ma3 is consistent with the classifier pil, the classifier for horses.21 By a similar argument, the preference for not splitting , lm could be strengthened in (lb) by the observation that the classifier 1'1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road' but not with,ma3 'horse.'\", 'Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.', \"Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework.\", 'With regard to purely morphological phenomena, certain processes are not han\\xad dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.', 'Mandarin exhibits several such processes, including A-not-A question formation, il\\xad lustrated in (3a), and adverbial reduplication, illustrated in (3b): 3.', \"(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'\", 'JI!', \"gaolxing4 'happy' => F.i'JF.i'J Jl!\", \"gaolbu4-gaolxing4 (hap-not-happy) 'happy?'\", \"(b) F.i'JJI!\", \"gaolxing4 'happy'=> F.i'JF.i'JJI!JI!\", \"gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb.\", 'In the case of adverbial reduplication illustrated in (3b) an adjective of the form AB is reduplicated as AABB.', 'The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer.', 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.', 'The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.', 'The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.', 'As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.', 'Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).', 'Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended.', 'While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.', '21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by.', 'a classifier.', 'The particular classifier used depends upon the noun.', 'Mohri [1995]) shows promise for improving this situation.', 'The model described here thus demonstrates great potential for use in widespread applications.', 'This flexibility, along with the simplicity of implementation and expansion, makes this framework an attractive base for continued research.', \"We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.'\", 'We further thank Dr. J.-S.', 'Chang of Tsinghua University, Taiwan, R.O.C., for kindly providing us with the name corpora.', 'We also thank ChaoHuang Chang, reviewers for the 1994 ACL conference, and four anonymous reviewers for Computational Linguistics for useful comments.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J00-3003 not well-formed (invalid token): line 12, column 25\n",
      "H05-1115 not well-formed (invalid token): line 6, column 109\n",
      "['New words such as names, technical terms, etc appear frequently.', 'As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.', 'Comparable corpora such as news documents of the same period from different news agencies are readily available.', 'In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.', 'We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.', 'New words such as person names, organization names, technical terms, etc. appear frequently.', 'In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.', 'Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).', 'But parallel corpora are scarce resources, especially for uncommon lan guage pairs.', 'Comparable corpora refer to texts that are not direct translation but are about the same topic.', 'For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.', 'Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.', 'Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999).', 'When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both.', 'For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation.', 'On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.', 'In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.', 'Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.', 'We fully implemented our method and tested it on ChineseEnglish comparable corpora.', 'We translated Chinese words into English.', 'That is, Chinese is the source language and English is the target language.', 'We achieved encouraging results.', 'While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.', 'The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar.', 'We could view this as a document retrieval problem.', 'The context (i.e., the surrounding words) of c is viewed as a query.', \"The context of each candidate translation e' is viewed as a document.\", 'Since the context of the correct translation e is similar to e , is considered as a document in IR.', 'If an English word e is the translation of a Chinese word c , they will have similar contexts.', 'So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.', 'The English word e document.', 'We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.', 'More details are given in Section 3.', 'On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation.', 'We use a variant of Within IR, there is a new approach to document retrieval called the language modeling approach (Ponte & Croft, 98).', 'In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998).', 'More details are is estimated.', 'The document with the highest given in Section 4.', 'Each of the two individual methods provides a P(Q | D) is the one that best matches the query.', 'ranked list of candidate words, associating with each candidate a score estimated by the particular method.', 'If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general.', 'Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work.', 'To estimate P(Q | D) , we use the approach of (Ng, 2000).', 'We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists.', 'We then n!', 'rank these words e1 , e2 ,..., ek according to the P (Q | D ) = ∏ P (t | D ) c t average of their rank positions in the two lists.', '∏ t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.', 'If no words appear within the top M positions in both lists, then no translation is output.', 'Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus.', 'In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose, the first fraction n!', '/ ∏t ct ! can be omitted as this part depends on the query only and thus is the same for all the documents.', 'in the English target corpus.', 'In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document.', 'So', 'our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection.', '∏ P(tc tc∈C ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word.', 'q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.', 'Then we sum lingual dictionary.', 'If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.', 'ble of pinyin, api is the ith sylla li is the English letter sequence estimation.', 'We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = α ⋅ Pml (tc | Tc (C (e))) + (1 −α ) ⋅ Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem.', 'We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.', 'To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, where Pml (•) are the maximu m likelihood esti 1, or 2.', 'Also.', 'we do not allow cross mappin gs.', 'mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.', 'α is set to 0.6 in our experiments.', 'must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e | c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.', 'Another difference is Onaizan and Knight, 2002b).', 'Knight and Graehl (1998) proposed a probabilistic model for machine transliteration.', 'In this model, a word in the target language (i.e., English in our task) is written and pronounced.', 'This pronunciation is converted to source language pronunciation and then to source language word that our method estimates stead of P(c | e) and P(e) .', '5.1 Resources.', 'P(e | c)directly, in (i.e., Chinese in our task).', 'AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters.', 'Pinyin is the standard Romanization system of Chinese characters.', 'It is phonetic-based.', 'For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) = ∑ P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995.', 'The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English.', 'The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995.', 'Chinese Giga- word corpus consists of news from two agencies: = ∑∏ P(l a a i | pi ) Xinhua News Agency and Central News Agency.', 'As for English corpus, we used the LDC English Gigaword Corpus from Jul to Dec 1995.', 'The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service.', 'To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words.', 'For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates.', 'English Service.', 'The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes.', 'We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context.', 'For the training of transliteration probability, we required a ChineseEnglish name list.', 'We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.', 'lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used.', 'The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected.', 'The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing.', 'Unlike English, Chinese text is composed of Chinese characters with no demarcation for words.', 'So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004).', 'for each pair of Chinese source word and English translation candidate word.', 'For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period.', 'Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times.', 'These words made up our test set.', 'We call these words Chinese source words.', 'They were the words that we were supposed to find translations from the English corpus.', 'For the English corpus, we performed sentence segmentation and converted each word to its morphological root form and to lower case.', 'We also divided the English corpus into 12 periods, each containing text from a half-month period.', 'For each period, we selected those English words occurring at least 10 times and were not present in the 10,000-word ChineseEnglish dictionary we used and were not stop words.', 'We considered these English words as potential translations of the Chinese source words.', 'We call Section 4), which was used to rank the English candidate words based on transliteration.', 'Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).', 'If no words appear within the top M positions in both ranked lists, then no translation is output.', 'Note that for many Chinese words, only one English word e appeared within the top M positions for both lists.', 'And among those cases where more than one English words appeared within the top M positions for both lists, many were multiple translations of a Chinese word.', 'This happened for example when a Chinese word was a non-English person name.', 'The name could have multiple translations in English.', 'For example, 米 洛西娜 was a Russian name.', 'Mirochina and Miroshina both appeared in top 10 positions of both lists.', 'Both were correct.', '5.4 Evaluation.', 'We evaluated our method on each of the 12 half- month periods.', 'The results when we set M = 10 are shown in Table 1.', 'We also investigated the effect of varying M . The results are shown in Table 2.', 'Table 1.', 'Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 – Jul 15, period 2 is Jul 16 – Jul 31, …, period 12 is Dec 16 – Dec 31.', '#c is the total number of new Chinese source words in the period.', '#e is the total number of English translation candidates in the period.', '#o is the total number of output English translations.', '#Cor is the number of correct English translations output.', 'Prec.', 'is the precision.', 'The correctness of the English translations was manually checked.', 'Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus.', 'We attempted to estimate recall by manually finding the English translations for all the Chinese source words for the two periods Dec 01 – Dec 15 and Dec 16 – Dec 31 in the English part of the corpus.', 'During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words.', 'So we estimate that English translations are present in the English part of the corpus for Table 2.', 'Precision and recall for different values of M The past research of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs.', 'Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable.', 'To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.', 'As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus.', 'This list of 43 words is shown in Table 3.', '8 of the 43 words are translated to English multi-word phrases (denoted as “phrase” in Table 3).', 'Since our method currently only considers unigram English words, we are not able to find translations for these words.', 'But it is not difficult to extend our method to handle this problem.', 'We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases.', 'The translations of 6 of the 43 words are words in the dictionary (denoted as “comm.” in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as “insuff”).', 'Our method is not able to find 43 (329 + 205) × 4499 = 362words in all 12 pe these translations.', 'But this is due to search space riods.', 'And our program finds correct translations for 115 words.', 'So we estimate that recall (for M = 10) is approximately 115 / 362 = 31.8% . pruning.', 'If we are willing to spend more time on searching, then in principle we can find these translations.', 'Table 3.', 'Rank of correct translation for period Dec 01 – Dec 15 and Dec 16 – Dec 31.', '‘Cont.', 'rank’ is the context rank, ‘Trans.', 'Rank’ is the transliteration rank.', '‘NA’ means the word cannot be transliterated.', '‘insuff’ means the correct translation appears less than 10 times in the English part of the comparable corpus.', '‘comm’ means the correct translation is a word appearing in the dictionary we used or is a stop word.', '‘phrase’ means the correct translation contains multiple English words.', 'As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position.', 'And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position.', 'On the other hand, using our method of combining both sources of information and setting M = ∞, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except 巴佐亚,坩埚,普利法) have their correct English translations at rank one position.', 'If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except 叶玛斯,巴佐亚,坩埚,普利法) have their correct English translations at rank one position.', 'Hence, our method of using both sources of information outperforms using either information source alone.', 'As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information.', 'For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation.', 'On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language.', 'In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone.', 'Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling.', 'But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling).', 'The work that is most similar to ours is the recent research of (Huang et al., 2004).', 'They attempted to improve named entity translation by combining phonetic and semantic information.', 'Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.', 'It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging.', 'They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.', 'In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information.', 'We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.', 'We thank Jia Li for implementing the EM algorithm to train transliteration probabilities.', 'This research is partially supported by a research grant R252000-125112 from National University of Singapore Academic Research Fund.']\n",
      "['Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', 'One of the disadvantages of developing a knowledge\\xad based system, however, is that it is a very labour\\xad intensive and time-consuming task.', 'This paper pres\\xad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', 'Input is checked against agreement and for a number of antecedent indicators.', 'Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.', 'Evaluation reports a success rate of 89.7% which is better than the suc\\xad cess rates of the approaches selected for comparison and tested on the same data.', 'In addition, preliminary experiments show that the approach can be success\\xad fully adapted for other languages with minimum modifications.', 'For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).', 'However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.', 'While various alternatives have been proposed, making use of e.g. neural networks, a situation se\\xad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro\\xad cessing of growing language resources.', 'Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin\\xad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).', 'Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph\\xad ora resolution.', 'It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.', 'Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).', 'With a view to avoiding complex syntactic, seman\\xad tic and discourse analysis (which is vital for real\\xad world applications), we developed a robust, knowl\\xad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', 'It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as \"antecedent indicators\").', 'The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re\\xad maining candidates (see next section).', 'The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im\\xad mediate reference.', 'If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.', 'If this does not help, the candidate with the higher score for indicating verbs is preferred.', 'If still no choice is possible, the most recent from the remaining candi\\xad dates is selected as the antecedent.', '2.1 Antecedent indicators.', 'Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.', 'Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante\\xad cedent.', 'The antecedent indicators have been identi\\xad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non\\xad prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.', 'Whilst some of the indicators are more genre-specific (term prefer\\xad ence) and others are less genre-specific (\"immediate reference\"), the majority appear to be genre\\xad independent.', 'In the following we shall outline some the indicators used and shall illustrate them by ex\\xad amples.', 'Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).', 'We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or posses\\xad sive pronouns.', \"This rule is ignored if there are no definite articles, possessive or demonstrative pro\\xad nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles).\", 'Givenness Noun phrases in previous sentences representing the \"given information\" (theme) 1 are deemed good candidates for antecedents and score I (candidates not representing the theme score 0).', 'In a coherent text (Firbas 1992), the given or known information, or theme, usually appears first, and thus forms a co\\xad referential link with the preceding text.', 'The new information, or rheme, provides some information about the theme.', '1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence.', 'Indicating verbs If a verb is a member of the Verb_set = {discuss, present, illustrate, identify, summarise, examine, describe, define, show, check, develop, review, re\\xad port, outline, consider, investigate, explore, assess, analyse, synthesise, study, survey, deal, cover}, we consider the first NP following it as the preferred an\\xad tecedent (scores 1 and 0).', 'Empirical evidence sug\\xad gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators.', 'Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not).', 'Lexically reiterated items include re\\xad peated synonymous noun phrases which may often be preceded by definite articles or demonstratives.', 'Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. \"toner bottle\", \"bottle of toner\", \"the bottle\").', 'Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we con\\xad sider it as the preferred candidate (1, 0).', '\"Non-prepositional\" noun phrases A \"pure\", \"non-prepositional\" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ).', 'Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording.', 'Here \"the VCR\" is penalised (-1) for being part of the prepositional phrase \"into the VCR\".', 'This preference can be explained in terms of sali\\xad ence from the point of view of the centering theory.', 'The latter proposes the ranking \"subject, direct ob\\xad ject, indirect object\" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects.', 'Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0).', 'The collocation preference here is restricted to the patterns \"noun phrase (pronoun), verb\" and \"verb, noun phrase (pronoun)\".', 'Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).', 'Example: Press the keyi down and turn the volume up...', 'Press iti again.', 'Immediate reference In technical manuals the \"immediate reference\" clue can often be useful in identifying the antecedent.', 'The heuristics used is that in constructions of the form \"...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)\", where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun \"it\" imme\\xad diately following V2 and is therefore given preference (scores 2 and 0).', 'This preference can be viewed as a modification of the collocation preference.', 'It is also quite fre\\xad quent with imperative constructions.', 'Example: To print the paper, you can stand the printeri up or lay iti flat.', 'To turn on the printer, press the Power buttoni and hold iti down for a moment.', 'Unwrap the paperi• form iti and align iti• then load iti into the drawer.', 'Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).', 'For anaphors in simple sentences, noun phrases in the previous sen\\xad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1).', 'Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).', '21dentification of clauses in complex sentences is do e heuristically.', 'As already mentioned, each of the antecedent in\\xad dicators assigns a score with a value {-1, 0, 1, 2}.', 'These scores have been determined experimentally on an empirical basis and are constantly being up\\xad dated.', 'Top symptoms like \"lexical reiteration\" as\\xad sign score \"2\" whereas \"non-prepositional\" noun phrases are given a negative score of \"-1\".', 'We should point out that the antecedent indicators are preferences and not absolute factors.', 'There might be cases where one or more of the antecedent indicators do not \"point\" to the correct antecedent.', 'For in\\xad stance, in the sentence \"Insert the cassette into the VCRi making sure iti is turned on\", the indicator \"non-prepositional noun phrases\" would penalise the correct antecedent.', 'When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the \"non-prepositional noun phrases\" heuristics (penalty) would be overturned by the \"collocational preference\" heuristics.', '2.2 Informal description of the algorithm.', 'The algorithm for pronoun resolution can be de\\xad scribed informally as follows: 1.', 'Examine the current sentence and the two pre\\xad.', 'ceding sentences (if available).', 'Look for noun phrases3 only to the left of the anaphor4 2.', 'Select from the noun phrases identified only.', 'those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates', 'tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric \"it\" occurring in constructions such as \"It is important\", \"It is necessary\" is eliminated by a \"referential filter\" 5Note that this restriction may not always apply in lan\\xad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. \"government\", \"team\", \"parliament\" etc. can be referred to by \"they\"; equally some plural nouns (e.g. \"data\") can be referred to by \"it\") and are exempted from the agree\\xad ment test.', 'For this purpose we have drawn up a compre\\xad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resolu\\xad tion has addressed the problem of \"agreement excep\\xad tions\".', 'antecedent.', 'If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.', 'If immediate reference does not hold, propose the candidate with higher score for collocational pattern.', 'If collocational pattern suggests a tie or does not hold, select the candidate with higher score for indicating verbs.', 'If this indicator does not hold again, go for the most recent candidate.', '3.', 'Evaluation.', 'For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real\\xad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.', 'The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.', 'Syntactic parallelism, useful in discrimi\\xad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.', 'Lack of semantic knowledge rules out the use of verb se\\xad mantics and semantic parallelism.', 'Our evaluation, however, suggests that much less is lost than might be feared.', 'In fact, our evaluation shows that the re\\xad sults are comparable to syntax-based methods (Lappin & Leass I994).', 'We believe that the good success rate is due to the fact that a number of ante\\xad cedent indicators are taken into account and no fac\\xad tor is given absolute preference.', 'In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism prefer\\xad ences (see below).', '3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).', 'There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.', 'The resolution of anaphors was carried out with a suc\\xad cess rate of 95.8%.', 'The approach being robust (an attempt is made to resolve each anaphor and a pro\\xad posed antecedent is returned), this figure represents both \"precision\" and \"recall\" if we use the MUC terminology.', 'To avoid any terminological confusion, we shall therefore use the more neutral term \"success rate\" while discussing the evaluation.', 'In order to evaluate the effectiveness of the ap\\xad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece\\xad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', 'The success rate of the \"Baseline Subject\" was 29.2%, whereas the success rate of \"Baseline Most Recent NP\" was 62.5%.', 'Given that our knowledge\\xad poor approach is basically an enhancement of a baseline model through a set of antecedent indica\\xad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.', 'Typically, our preference-based model proved superior to both baseline models when the antece\\xad dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.', 'Example: Identify the draweq by the lit paper port LED and add paper to itj.', 'The aggregate score for \"the drawer\" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer\\xad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (\"the lit paper port LED\") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera\\xad tion 0 + section heading 0 + collocation 0 + referen\\xad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).', 'From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and the· antecedent have not only different syntactic functions but also different semantic roles.', 'Usually knowledge-based ap\\xad proaches have difficulties in such a situation because they use preferences such as \"syntactic parallelism\" or \"semantic parallelism\".', 'Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the syn\\xad tactic function/semantic role of each individual word.', 'As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more com\\xad plex syntactic structure.', 'This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.', 'Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.', 'where \"blank sheet of paper\" scores only 2 as op\\xad posed to the \"the paper through key\" which scores 6.', \"3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994).\", 'Out of 223 pro\\xad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric \"it\").', 'The evaluation carried out was manual to ensure that no added error was gen\\xad erated (e.g. due to possible wrong sentence/clause detection or POS tagging).', \"Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3).\", 'The evaluation indicated 83.6% success rate.', 'The \"Baseline subject\" model tested on the same data scored 33.9% recall and 67.9% precision, whereas \"Baseline most recent\" scored 66.7%.', 'Note that \"Baseline subject\" can be assessed both in terms of recall and precision because this \"version\" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many im\\xad perative zero-subject sentences).', 'In the second experiment we evaluated the ap\\xad proach from the point of view also of its \"critical success rate\".', 'This measure (Mitkov 1998b) applies only to anaphors \"ambiguous\" from the point of view of number and gender (i.e. to those \"tough\" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.', 'Our evaluation estab\\xad lished the critical success rate as 82%.', 'A case where the system failed was when the anaphor and the antecedent were in the same sen\\xad tence and where preference was given to a candidate in the preceding sentence.', 'This case and other cases suggest that it might be worthwhile reconsider\\xad ing/refining the weights for the indicator \"referential distance\".', 'Similarly to the first evaluation, we found that the robust approach was not very successful on sen\\xad tences with too complicated syntax - a price we have to pay for the \"convenience\" of developing a knowl\\xad edge-poor system.', 'The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.', 'R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9.', '7 % 31.', '55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in \"Baseline subject\" corresponds to \"recall\" and the higher figure- to \"precision\".', 'If we regard as \"discriminative power\" of each antecedent indicator the ratio \"number of successful antecedent identifications when this indicator was applied\"/\"number of applications of this indicator\" (for the non-prepositional noun phrase and definite\\xad ness being penalising indicators, this figure is calcu\\xad lated as the ratio \"number of unsuccessful antece\\xad dent identifications\"/\"number of applications\"), the immediate reference emerges as the most discrimi\\xad native indicator (100%), followed by non\\xad prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).', 'The rela\\xad tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the ba\\xad sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent.', 'In terms of frequency of use (\"number of nonzero applications\"/\"number of anaphors\"), the most fre\\xad quently used indicator proved to be referential dis\\xad tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reit\\xad eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and colloca\\xad tion (11.1%).', 'As expected, the most frequent indica\\xad tors were not the most discriminative ones.', '3.3 Comparison to similar approaches: compara\\xad.', 'tive evaluation of Breck Baldwin\\'s CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin\\'s Cog\\xad NIAC (Baldwin 1997) approach which features \"high precision coreference with limited knowledge and linguistics resources\".', \"The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate.\", 'Given that our approach is robust and returns an\\xad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC\\'s \"resolve all\" version by simulating it manually on the same training data used in evaluation B above.', 'CogNIAC successfully resolved the pronouns in 75% of the cases.', 'This result is comparable with the results described in (Baldwin 1997).', 'For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).', 'It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.', 'languages An attractive feature of any NLP approach would be its language \"universality\".', 'While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adapta\\xad tion.', 'We used the robust approach as a basis for devel\\xad oping a genre-specific reference resolution approach in Polish.', 'As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997).', 'For the time being, we are using the same scores for Polish.', 'The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).', 'The sample texts con\\xad tained 180 pronouns among which were 120 in\\xad stances of exophoric reference (most being zero pro\\xad nouns).', 'The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolv\\xad ing anaphors (with critical success rate of 86.2%).', 'Similarly to the evaluation for English, we com\\xad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.', 'Our preference-based approach showed clear su\\xad periority over both baseline models.', 'The first Base\\xad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.', 'There\\xad fore, the 93.3% success rate (see above) demon\\xad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences.', 'We have recently adapted the approach for Ara\\xad bic as well (Mitkov & Belguith 1998).', 'Our evalua\\xad tion, based on 63 examples (anaphors) from a tech\\xad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %).', 'We have described a robust, knowledge-poor ap\\xad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', 'Evaluation shows a success rate of 89.7% for the genre of tech\\xad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.', 'We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).']\n",
      "['We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', 'We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.', 'In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.', 'By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.', 'The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).', 'Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”.', 'We found that so far all the existing implementations were using character-based IOB tagging.', 'In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.', 'If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.', 'Taking the same example mentioned above, “ (whole) (Beijing city)” is labeled as ” (whole)/O (Beijing)/B (city)/I” in the subword-based tagging, where ” (Beijing)/B” is labeled as one unit.', 'We will give a detailed description of this approach in Section 2.', '∗ Now the second author is affiliated with NTT.', 'In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).', 'In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.', 'While OOV recognition is very important in word segmentation, a higher IV rate is also desired.', 'In this work we propose a confidence measure approach to lessen the weakness.', 'By this approach we can change R-oovs and R-ivs and find an optimal tradeoff.', 'This approach will be described in Section 2.2.', 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', 'Section 3 presents our experimental results.', 'Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.', 'Section 5 provides the concluding remarks.', 'Our word segmentation process is illustrated in Fig.', '1.', 'It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.', 'An example exhibiting each step’s results is also given in the figure.', 'Since the dictionary-based approach is a well-known method, we skip its technical descriptions.', 'However, keep in mind that the dictionary-based approach can produce a higher R-iv rate.', 'We will use this advantage in the confidence measure approach.', '2.1 Subword-based IOB tagging using CRFs.', 'There are several steps to train a subword-based IOB tag- ger.', 'First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.', 'Qc 2006 Association for Computational Linguistics input 咘㣅\\u1bf9ԣ\\u0cfc࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\\\ Dictionary-based word segmentation 咘 㣅 \\u1bf9 ԣ \\u0cfc ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\\\ Subword-based IOB tagging 咘/% 㣅/, \\u1bf9/, ԣ/2 \\u0cfc/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\\\/, Confidence-based segmentation 咘/% 㣅/, \\u1bf9/, ԣ/2 \\u0cfc/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\\\/, output 咘㣅\\u1bf9 ԣ \\u0cfc ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\\\ Figure 1: Outline of word segmentation process data.', 'We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.', 'If the subset consists of Chinese characters only, it is a character-based IOB tagger.', 'We regard the words in the subset as the subwords for the IOB tagging.', 'Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.', 'For a character-based IOB tagger, there is only one possibility of re-segmentation.', 'However, there are multiple choices for a subword-based IOB tagger.', 'For example, “ (Beijing-city)” can be segmented as “ (Beijing-city)/O,” or “ (Beijing)/B (city)/I,” or ” (north)/B (capital)/I (city)/I.” In this work we used forward maximal match (FMM) for disambiguation.', 'Of course, backward maximal match (BMM) or other approaches are also applicable.', 'We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach.', 'In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.', 'We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.', 'The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.', 'In order to overcome overfitting, a gaussian prior was imposed in the training.', 'The types of unigram features used in our experiments included the following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 where w stands for word.', 'The subscripts are position indicators.', '0 means the current word; −1, −2, the first or second word to the left; 1, 2, the first or second word to the right.', 'For the bigram features, we only used the previous and the current observations, t−1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.', 'We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff.', 'A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding.', '2.2 Confidence-dependent word segmentation.', 'Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.', 'However, neither was perfect.', 'The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.', 'In this section we introduce a confidence measure approach to combine the two results.', 'We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.', 'The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.', 'Its calculation is defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) where tiob is the word w’s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.', 'After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.', \"Each subword is given a prior IOB tag, tw . C Miob (t|w), a \\uf8eb M \\uf8eb\\uf8f6\\uf8f6 confidence probability derived in the process of IOB tag exp \\uf8ec)' \\uf8ec)' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W )\\uf8f7\\uf8f7 /Z, \\uf8ec\\uf8ed i=1 \\uf8ec\\uf8ed k k \\uf8f7\\uf8f8 \\uf8f7\\uf8f8 (1) ging, is defined as Z = )' T =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) T =t 0 t1 ··· tM P ( T | W ) where we call fk (ti−1 , ti , W ) bigram feature functions because the features trigger the previous observation ti−1 where the numerator is a sum of all the observation sequences with word wi labeled as t. δ(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.\", 'It is a Kronecker delta function defined as δ(tw , tiob )ng = { 1 if tw = tiob 0 otherwise In Eq. 2, α is a weighting between the IOB tagging and the dictionary-based word segmentation.', 'We found the value 0.7 for α, empirically.', 'By Eq. 2 the results of IOB tagging were reevaluated.', 'A confidence measure threshold, t, was defined for making a decision based on the value.', 'If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.', 'A new OOV was thus created.', 'For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.', 'In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold.', 'In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.', 'We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.', 'The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR).', 'Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.', 'Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).', 'For detailed info.', 'of the corpora and these scores, refer to (Emerson, 2005).', 'For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.', 'Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.', 'Table 1 shows the performance of the dictionary-based segmentation.', 'Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.', 'In fact, there were no OOV recognition.', 'Hence, this approach produced lower F-scores.', 'However, the R-ivs were very high.', '3.1 Effects of the Character-based and the.', 'subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.', 'For the character-based tagging, we used all the Chinese characters.', 'For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.', 'The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.', 'R P FR oo vR iv A S 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.', '67 8 0.', '64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.', '70 0 0.', '73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.', '78 3 0.', '75 4 0.9 49 0.9 55 M S R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.', '71 0 0.', '71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging.', 'The upper numbers are of the character- based and the lower ones, the subword-based.', 'using the FMM, and then labeled with “IOB” tags by the CRFs.', 'The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.', 'We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus.', 'There were no F-score changes for AS and PKU corpora, but the recall rates were improved.', 'Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.', 'However, the R-iv rates were getting worse in return for higher R-oov rates.', 'We will tackle this problem by the confidence measure approach.', '3.2 Effect of the confidence measure.', 'In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.', 'The effect of the confidence measure is shown in Table 3, where we used α = 0.7 and confidence threshold t = 0.8.', 'In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based.', 'We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.', 'The act of confidence measure made a tradeoff between R-ivs and R- oovs, yielding higher R-oovs than Table 1 and higher R R P FR oo vR iv A S 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.', '60 7 0.', '64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.', '68 2 0.', '74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.', '77 5 0.', '74 8 0.9 52 0.9 59 M S R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.', '67 4 0.', '71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure.', 'The upper numbers and the lower numbers are of the character-based and the subword-based, respectively A S CI T Y U M SR P K U Ba ke off be st 0.', '95 2 0.', '9 4 3 0.', '96 4 0.', '95 0 O u r s 0.', '95 1 0.', '9 5 1 0.', '97 1 0.', '95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.', 'Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging.', 'It proves the proposed word-based IOB tagging was very effective.', 'The IOB tagging approach adopted in this work is not a new idea.', 'It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.', 'Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).', 'Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.', 'We proved the new approach enhanced the word segmentation significantly.', 'Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.', 'We achieved the highest F-scores in CITYU, PKU and MSR corpora.', 'We think our proposed subword- based tagging played an important role for the good results.', 'Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used.', 'We could yield a better results than those shown in Table 4 using such information.', 'For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known.', 'For AS corpus, “Adam Smith” are two words in the training but become a one- word in the test, “AdamSmith”.', 'Our approaches produced wrong segmentations for labeling inconsistency.', 'Another advantage of the word-based IOB tagging over the character-based is its speed.', 'The subword-based approach is faster because fewer words than characters were labeled.', 'We found a speed up both in training and test.', 'The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.', 'In this work we used it more delicately.', 'By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.', 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', 'Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.', 'We also successfully employed the confidence measure to make a confidence-dependent word segmentation.', 'This approach is effective for performing desired segmentation based on users’ requirements to R-oov and R-iv.', 'The authors appreciate the reviewers’ effort and good advice for improving the paper.']\n",
      "['In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).', 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.', 'A search restriction especially useful for the translation direction from German to English is presented.', 'The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.', 'The goal of machine translation is the translation of a text given in some source language into a target language.', 'We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.', 'Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.', 'Our approach uses word-to-word dependencies between source and target words.', 'The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).', 'These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.', 'The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.', 'A simple extension will be used to handle this problem.', 'In Section 2, we brie y review our approach to statistical machine translation.', 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.', 'This approach is compared to another reordering scheme presented in (Berger et al., 1996).', 'In Section 4, we present the performance measures used and give translation results on the Verbmobil task.', 'In this section, we brie y review our translation approach.', 'In Eq.', '(1), Pr(eI 1) is the language model, which is a trigram language model in this case.', 'For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.', 'The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj\\U001000001; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj\\U001000001 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).', 'When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.', 'In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.', '2.1 Inverted Alignments.', 'To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).', 'An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi.', \"What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei\\U001000001 i\\U001000002) max bI 1 I Yi=1 [p(bijbi\\U001000001; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei\\U001000001 i\\U001000002) p(bijbi\\U001000001; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei\\U001000001 i\\U001000002) is the trigram language model probability.\", 'The inverted alignment probability p(bijbi\\U001000001; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.', 'The details are given in (Och and Ney, 2000).', 'The sentence length probability p(JjI) is omitted without any loss in performance.', 'For the inverted alignment probability p(bijbi\\U001000001; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.', \"The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.\", 'We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.', 'The word joining is done on the basis of a likelihood criterion.', 'An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.', \"E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment.\", 'In the following, we assume that this word joining has been carried out.', 'Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup.', 'In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).', 'The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1.', 'A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.', 'The resulting algorithm has a complexity of O(n!).', 'However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.', 'The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city.', 'Subsets C of increasing cardinality c are processed.', 'The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.', 'For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.', 'This algorithm can be applied to statistical machine translation.', 'Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.', 'The advantage is that we can recombine search hypotheses by dynamic programming.', 'The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.', 'input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Æ;e00 j02Cnfjg fp(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.', 'Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed.', 'For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).', 'e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.', 'Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.', 'The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and ei\\U001000001 = e0.', 'The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Æ = 0 or Æ = 1 new target words.', 'For Æ = 1, a new target language word is generated using the trigram language model p(eje0; e00).', 'For Æ = 0, no new target word is generated, while an additional source sentence position is covered.', 'A modified language model probability pÆ(eje0; e00) is defined as follows: pÆ(eje0; e00) =  1:0 if Æ = 0 p(eje0; e00) if Æ = 1 : We associate a distribution p(Æ) with the two cases Æ = 0 and Æ = 1 and set p(Æ = 1) = 0:7.', 'The above auxiliary quantity satisfies the following recursive DP equation: Qe0 (e; C; j) = Initial Skip Verb Final 1.', 'In.', '2.', 'diesem 3.', 'Fall.', '4.', 'mein 5.', 'Kollege.', '6.', 'kann 7.nicht 8.', 'besuchen 9.', 'Sie.', '10.', 'am 11.', 'vierten 12.', 'Mai.', '13.', 'Figure 2: Order in which source positions are visited for the example given in Fig.1.', '= p(fj je) max Æ;e00 j02Cnfjg np(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).', 'The resulting algorithm is depicted in Table 1.', 'The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.', '3.1 Word ReOrdering with Verbgroup.', 'Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.', 'On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.', 'No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !', '(f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) !', '(f1; ;mg n fl1g ; l) 3 (f1; ;mg n fl; l1; l2g ; l0) !', '(f1; ;mg n fl1; l2g ; l) 4 (f1; ;m \\U00100000 1g n fl1; l2; l3g ; l0) !', '(f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup.', 'In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.', 'Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup.', 'A typical situation is shown in Figure 1.', \"When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence.\", \"Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated.\", 'The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.', 'To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered.', 'Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.', 'Final (F): The rest of the sentence is processed monotonically taking account of the already covered positions.', 'While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.', 'The sequence of states needed to carry out the word reordering example in Fig.', '1 is given in Fig.', '2.', 'The 13 positions of the source sentence are processed in the order shown.', 'A position is presented by the word at that position.', 'Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !', '(S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account.', 'To be short, we omit the target words e; e0 in the formulation of the search hypotheses.', 'There are 13 types of extensions needed to describe the verbgroup reordering.', 'The details are given in (Tillmann, 2000).', 'For each extension a new position is added to the coverage set.', 'Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).', 'Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.', 'The search starts in the hypothesis (I; f;g; 0).', 'f;g denotes the empty set, where no source sentence position is covered.', 'The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Æ;e00 np(jjj0; J) p(Æ) pÆ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).', 'f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ \\U00100000L; ; Jg.', 'The final score is obtained from: max e;e0 j2fJ\\U00100000L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.', 'The complexity of the quasimonotone search is O(E3 J (R2+LR)).', 'The proof is given in (Tillmann, 2000).', '3.2 Reordering with IBM Style.', 'Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).', 'A detailed description of the search procedure used is given in this patent.', 'Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.', 'A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.', 'Here, we process only full-form words within the translation procedure.', 'the number of permutations carried out for the word reordering is given.', 'During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.', 'Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen, where n is set to 4.', 'The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.', 'This number must be less than or equal to n \\U00100000 1.', 'Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.', 'Ignoring the identity of the target language words e and e0, the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2.', 'In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction.', 'Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction.', 'A dynamic programming recursion similar to the one in Eq. 2 is evaluated.', 'In this case, we have no finite-state restrictions for the search space.', 'The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.', 'This approach leads to a search procedure with complexity O(E3 J4).', 'The proof is given in (Tillmann, 2000).', '4.1 The Task and the Corpus.', 'We have tested the translation system on the Verbmobil task (Wahlster 1993).', 'The Verbmobil task is an appointment scheduling task.', 'Two subjects are each given a calendar and they are asked to schedule a meeting.', 'The translation direction is from German to English.', 'A summary of the corpus used in the experiments is given in Table 3.', 'The perplexity for the trigram language model used is 26:5.', 'Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this paper is the (more or less) correct orthographic transcription of the spoken sentences.', 'Thus, the effects of spontaneous speech are present in the corpus, e.g. the syntactic structure of the sentence is rather less restricted, however the effect of speech recognition errors is not covered.', 'For the experiments, we use a simple preprocessing step.', 'German city names are replaced by category markers.', 'The translation search is carried out with the category markers and the city names are resubstituted into the target sentence as a postprocessing step.', 'Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks).', 'German English Training: Sentences 58 073 Words 519 523 549 921 Words* 418 979 453 632 Vocabulary Size 7939 4648 Singletons 3454 1699 Test-147: Sentences 147 Words 1 968 2 173 Perplexity { 26:5 Table 4: Multi-reference word error rate (mWER) and subjective sentence error rate (SSER) for three different search procedures.', 'Search CPU time mWER SSER Method [sec] [%] [%] MonS 0:9 42:0 30:5 QmS 10:6 34:4 23:8 IbmS 28:6 38:2 26:2 4.2 Performance Measures.', 'The following two error criteria are used in our experiments: mWER: multi-reference WER: We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors.', 'On average, 6 reference translations per automatic translation are available.', 'The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken.', 'This measure has the advantage of being completely automatic.', 'SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person.', 'For the error counts, a range from 0:0 to 1:0 is used.', 'An error count of 0:0 is assigned to a perfect translation, and an error count of 1:0 is assigned to a semantically and syntactically wrong translation.', '4.3 Translation Experiments.', 'For the translation experiments, Eq. 2 is recursively evaluated.', 'We apply a beam search concept as in speech recognition.', 'However there is no global pruning.', 'Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.', 'Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited.', 'For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words.', 'We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.', 'Table 4 shows translation results for the three approaches.', 'The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).', 'Here, the pruning threshold t0 = 10:0 is used.', 'Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER).', 'The monotone search performs worst in terms of both error rates mWER and SSER.', 'The computing time is low, since no reordering is carried out.', 'The quasi-monotone search performs best in terms of both error rates mWER and SSER.', 'Additionally, it works about 3 times as fast as the IBM style search.', 'For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.', 'The effect of the pruning threshold t0 is shown in Table 5.', 'The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.', 'The negative logarithm of t0 is reported.', 'The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.', 'Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.', 'Decreasing the threshold results in higher mWER due to additional search errors.', 'Table 5: Effect of the beam threshold on the number of search errors (147 sentences).', 'Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.', 'Again, the monotone search performs worst.', 'In the second and third translation examples, the IbmS word reordering performs worse than the QmS word reordering, since it can not take properly into account the word reordering due to the German verbgroup.', \"The German finite verbs 'bin' (second example) and 'k\\x7fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions).\", 'In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable.', 'In this paper, we have presented a new, eÆcient DP-based search procedure for statistical machine translation.', 'The approach assumes that the word reordering is restricted to a few positions in the source sentence.', 'The approach has been successfully tested on the 8 000-word Verbmobil task.', 'Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.', '2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated.', '3) A tight coupling with the speech recognizer output.', 'This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community.', 'Table 6: Example Translations for the Verbmobil task.', 'Input: Ja , wunderbar . K\\x7fonnen wir machen . MonS: Yes, wonderful.', 'Can we do . QmS: Yes, wonderful.', 'We can do that . IbmS: Yes, wonderful.', 'We can do that . Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen nur am dritten . Wie w\\x7fare es denn am \\x7fahm Samstag , dem zehnten Februar ? MonS: That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about \\x7fahm Saturday , the tenth of February ? QmS: That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . \\x7fAhm how about Saturday , February the tenth ? IbmS: That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . \\x7fAhm how about Saturday , February the tenth ? Input: Wenn Sie dann noch den siebzehnten k\\x7fonnten , w\\x7fare das toll , ja . MonS: If you then also the seventeenth could , would be the great , yes . QmS: If you could then also the seventeenth , that would be great , yes . IbmS: Then if you could even take seventeenth , that would be great , yes . Input: Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MonS: Yes , that suits me perfectly . Do we should best like that . QmS: Yes , that suits me fine . We do it like that then best . IbmS: Yes , that suits me fine . We should best do it like that .']\n",
      "['We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', 'Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).', 'These models enable word- alignment process to leverage topical contents of document-pairs.', 'Efficient variational approximation algorithms are designed for inference and parameter estimation.', 'With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects.', 'Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.', 'Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.', 'Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001).', 'Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.', 'For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations.', 'Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.', 'For example, the word shot in “It was a nice shot.” should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.', 'Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances.', 'In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.', 'With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.', 'Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.', 'This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches.', 'These approaches can be expensive, and they do not emphasize stochastic translation aspects.', 'Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.', 'We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.', 'Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).', 'Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.', 'In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.', 'Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent.', 'Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.', 'Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.', 'In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need handcrafted knowledge engineering.', 'The remainder of the paper is as follows: in section 2, we introduce notations and baselines; in section 3, we propose the topic admixture models; in section 4, we present the learning and inference algorithms; and in section 5 we show experiments of our models.', 'We conclude with a brief discussion in section 6.', 'In statistical machine translation, one typically uses parallel data to identify entities such as “word-pair”, “sentence-pair”, and “document- pair”.', 'Formally, we define the following terms1: • A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. • A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.• A document-pair (F, E) refers to two doc uments which are translations of each other.', 'Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair.', '• A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}.', '2.1 Baseline: IBM Model-1.', 'The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level.', 'The translation lexicon p(f |e) is the key component in this generative process.', 'An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).', 'We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.', 'Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.', 'In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.', 'A unique normalized and real-valued vector θ, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions.', 'Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.', 'Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions.', 'Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.', 'There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels.', 'We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.', 'J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).', '(1) j=1 i=1 1 We follow the notations in (Brown et al., 1993) for.', 'English-French, i.e., e ↔ f , although our models are tested,in this paper, for EnglishChinese.', 'We use the end-user ter minology for source and target languages.', 'In the first BiTAM model, we assume that topics are sampled at the sentence-level.', 'Each document- pair is represented as a random mixture of latent topics.', 'Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I β e I a α θ z f J B N M α θ z a a f J B α θ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.', 'A node in the graph represents a random variable, and a hexagon denotes a parameter.', 'Un-shaded nodes are hidden variables.', 'All the plates represent replicates.', 'The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.', '(a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.', 'a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic.', 'Given a specific topic-weight vector θd for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.', 'This generative process, for a document-pair (Fd, Ed), is summarized as below: 1.', 'Sample sentence-number N from a Poisson(γ)..', '2.', 'Sample topic-weight vector θd from a Dirichlet(α)..', '3.', 'For each sentence-pair (fn , en ) in the dtth doc-pair ,.', '(a) Sample sentence-length Jn from Poisson(δ); (b) Sample a topic zdn from a Multinomial(θd ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.', 'Note that, the sentence-pairs are now connected by the node θd. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector θd. Specifically, BiTAM1 assumes that each sentence-pair has one single topic.', 'Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair.', 'The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).', 'and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear.', 'For each document-pair, a K -dimensional Dirichlet random variable θd, referred to as the topic-weight vector of the document, can take values in the (K −1)-simplex following a probability density: to the proposed distributions.', 'We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random.', 'Given the parameters α, B, and the English part E, the joint conditional distribution of the topic-weight vector θ, the topic indicators z, the alignment vectors A, and the document F can be written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) where the hyperparameter α is a K -dimension vector with each component αk >0, and Γ(x) is the Gamma function.', 'The alignment is represented by a J -dimension vector a = {a1, a2, · · · , aJ }; for each French word fj at the position j, an position variable aj maps it to anEnglish word eaj at the position aj in English sen p(θ | α) n p(zn |θ)p(fn , an |en , α, Bzn), n=1 where N is the number of the sentence-pair.', 'Marginalizing out θ and z, we can obtain the marginal conditional probability of generating F from E for each document-pair: p(F, A|E, α, Bzn ) = tence.', 'The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }.', 'p(θ|α) n) p(zn |θ)p(fn , an |en , Bzn ) dθ, n=1 zn For simplicity, in our current models we omit the modelings of the sentence-number N and the sentence-length Jn, and focus only on the bilingual translation model.', 'Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.', 'For simplicity, we assume that the French words fj ’s are conditionally independent of each other; the alignment variables aj ’s are independent of other variables and are uniformly distributed a priori.', 'Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn “Null” is attached to every target sentence to align the source words which miss their translations.', 'Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).', '(6) contains only one word: “Null”, and the alignment link a is no longer a hidden variable.', 'Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn.', '5.', '3.2 BiTAM2: Monolingual Admixture.', 'In general, the monolingual model for English can also be a rich topic-mixture.', 'This is realized by using the same topic-weight vector θd and the same topic indicator zdn sampled according to θd, as described in §3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)).', 'Now e is generated', 'Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and θ is intractable.', 'A variational inference is used to approximate the true posteriors of these hidden variables.', 'The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted.', '4.1 Variational Approximation.', 'To approximate: p(θ, z, A|E, F, α, B), the joint posterior, we use the fully factorized distribution over the same set of hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· from a topic-based language model β, instead of a N Jn (7) uniform distribution in BiTAM1.', 'We refer to this n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model as BiTAM2.', 'n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics.', 'The topics are inferred more directly from the observed bilingual data, and as a result, improve alignment.', '3.3 BiTAM3: Word-level Admixture.', 'where the Dirichlet parameter γ, the multinomial parameters (φ1, · · · , φn), and the parameters (ϕn1, · · · , ϕnJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(·) to the original p(·) via an iterative fixed-point algorithm.', 'It can be shown that the fixed-point equations for the variational parameters in BiTAM1 are as follows: Nd γk = αk + ) φdnk (8) n=1 K It is straightforward to extend the sentence-level BiTAM1 to a word-level admixture model, by φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j for each word-pair (fj , eaj ) in the ntth sentence-pair, rather than once for all (words) in the sentence (Figure 1 (c)).', 'exp ( ) ) ϕdnji log Bf ,e ,k (9) j i j=1 i=1 K ( This gives rise to our BiTAM3.', 'The conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where Ψ(·) is a digamma function.', 'Note that inthe formulas in §3.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas φ dnkis the variational param 3.4 Incorporation of Word “Null”.', 'Similar to IBM models, “Null” word is used for the source words which have no translation counterparts in the target language.', 'For example, Chinese words “de” (ffl) , “ba” (I\\\\) and “bei” (%i) generally do not have translations in English.', 'eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair.', 'Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters α and B in an unsupervised fashion.', 'Essentially, Eqs.', '(810) above constitute the E-step, where the posterior estimations of the latent variables are obtained.', 'In the M-step, we update α and B so that they improve a lower bound of the log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].', '(11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {ϕdnji} in Eqn.', '10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) d n=1 j=1 i=1 For α, close-form update is not available, and we resort to gradient accent as in (Sjo¨ lander et al., 1996) with restarts to ensure each updated αk >0.', '4.2 Data Sparseness and Smoothing.', 'The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).', 'To reduce the data sparsity problem, we introduce two remedies in our models.', 'First: Laplace smoothing.', 'In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.', 'Second: interpolation smoothing.', 'Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).', '(13) As in Eqn.', '1, p(f |e) is learned via IBM1; λ is estimated via EM on held out data.', '4.3 Retrieving Word Alignments.', 'Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).', 'Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix ϕ ≡ {ϕdnji}.', 'UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query ϕ to get the best aligned English word (by taking the maximum point in a row of ϕ): adnj = arg max ϕdnji .', '(14) i∈[1,Idn ] icon’s strength.', 'We evaluate BiTAM models on the word alignment accuracy and the translation quality.', 'For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported.', 'Table 1: Training and Test Data Statistics Tra in #D oc.', '#S ent . #T ok en s En gli sh Ch ine se Tr ee b a n k F B IS . B J Si n or a m a Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes t 95 62 7 25, 50 0 19, 72 6 We have two training data settings with different sizes (see Table 1).', 'The small one consists of 316 document-pairs from Tree- bank (LDC2002E17).', 'For the large training data setting, we collected additional document- pairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)).', 'There are 27,940 document-pairs, containing 327K sentence-pairs or 12 million (12M) English tokens and 11M Chinese tokens.', 'To evaluate word alignment, we hand-labeled 627 sentence-pairs from 95 document-pairs sampled from TIDES’01 dryrun data.', 'It contains 14,769 alignment-links.', 'To evaluate translation quality, TIDES’02 Eval.', 'test is used as development set, and TIDES’03 Eval.', 'test is used as the unseen test data.', '5.1 Model Settings.', 'First, we explore the effects of Null word and smoothing strategies.', 'Empirically, we find that adding “Null” word is always beneficial to all models regardless of number of topics selected.', 'To pics Le xic ons To pic1 To pic2 To pic3 Co oc.', 'IBM 1 H M M IBM 4 p( Ch ao Xi an (Ji!', '$) |K ore an) 0.', '06 12 0.', '21 38 0.', '22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!', '� )|K ore an) 0.', '83 79 0.', '61 16 0.', '02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1.', 'The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean).', 'The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!�:South Korean).', 'The two candidate translations may both fade out in the learned translation lexicons.', 'Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h i n a u . s . dev elop men t trad e ente rpri ses tech nolo gy cou ntri es y e a r eco nom ic Topi c B. cho ngqi ng com pani es take over s co m pa ny cit y bi lli o n m o r e eco nom ic re a c h e d y u a n Topi c C. sp or ts dis abl ed te a m p e o p l e caus e w at e r na tio na l ga m es han dica ppe d me mb ers Table 3: Three most distinctive topics are displayed.', 'The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {φdnk }.', '33 functional words were removed to highlight the main content of each topic.', 'Topic A is about Us-China economic relationships; Topic B relates to Chinese companies’ merging; Topic C shows the sports of handicapped people.The interpolation smoothing in §4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1.', 'However, the interpolation leverages the competing baseline lexicon, and this can blur the evaluations of BiTAM’s contributions.', 'Laplace smoothing is chosen to emphasize more on BiTAM’s strength.', 'Without any smoothing, F- measure drops very quickly over two topics.', 'In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models.', 'We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish.', 'Choosing the number of topics is a model selection problem.', 'We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets.', 'The overall computation complexity of the BiTAM is linear to the number of hidden topics.', '5.2 Variational Inference.', 'Under a non-symmetric Dirichlet prior, hyperparameter α is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1.', 'Better initialization of B can help to avoid local optimal as shown in § 5.5.', 'With the learned B and α fixed, the variational parameters to be computed in Eqn.', '(810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10−5.', 'The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.', 'To estimate B, β (for BiTAM2) and α, at most eight variational EM iterations are run on the training data.', 'Figure 2 shows absolute 2∼3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1.', 'BiTam with Null and Laplace Smoothing Over Var.', 'EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.', 'IB M −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number of EM/Variational EM Iterations for IBM−1 and BiTam−1 Figure 2: performances over eight Variational EM iterations of BiTAM1 using both the “Null” word and the laplace smoothing; IBM1 is shown over eight EM iterations for comparison.', '5.3 Topic-Specific Translation.', 'Lexicons The topic-specific lexicons Bk are smaller in size than IBM1, and, typically, they contain topic trends.', 'For example, in our training data, North Korean is usually related to politics and translated into “ChaoXian” (Ji!', '$); South Korean occurs more often with economics and is translated as “HanGuo”(li!', '�).', 'BiTAMs discriminate the two by considering the topics of the context.', 'Table 2 shows the lexicon entries for “Korean” learned by a 3-topic BiTAM1.', 'The values are relatively sharper, and each clearly favors one of the candidates.', 'The co-occurrence count, however, only favors “HanGuo”, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context.', 'Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small.', 'With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.', '5.4 Evaluating Word.', 'Alignments We evaluate word alignment accuracies in various settings.', 'Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).', 'Additional heuristics are applied to further improve the accuracies.', 'Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6.', '45 8 15 .7 0 6.', '82 2 17 .7 0 6.', '92 6 18 .2 5 6.', '93 7 6.954 17 .93 18.14 6.', '90 4 6.976 18 .13 18.05 6.', '96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1.', 'For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality.', 'Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.', 'As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.', 'A close look at the three BiTAMs does not yield significant difference.', 'BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated.', 'The BDA align ments of BiTAM1∼3 yield 48.26%, 48.63% and 49.02%, which are even better than HMM and IBM4 — their best performances are at 44.26% and 45.96%, respectively.', 'This is because BDA partially utilizes similar heuristics on the approximated posterior matrix {ϕdnji} instead of di rect operations on alignments of two directions in the heuristics of Refined.', 'Practically, we also apply BDA together with heuristics for IBM1, HMM and IBM4, and the best achieved performances are at 40.56%, 46.52% and 49.18%, respectively.', 'Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model.', 'Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above.', 'As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic.', 'We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table.', '5).', '5.5 Boosting BiTAM Models.', 'The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments.', 'Better ini tializations can potentially lead to better performances because it can help to avoid the undesirable local optima in variational EM iterations.', 'We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models.', 'This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement.', 'The boosted alignments are denoted as BUDA and BBDA in Table.', '5, corresponding to the uni-direction and bi-direction alignments, respectively.', 'We see an improvement in alignment quality.', '5.6 Evaluating Translations.', 'To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities.', 'Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones.', 'We use TIDES Eval’02 CE test set as development data to tune the decoder parameters; the Eval’03 data (919 sentences) is the unseen data.', 'A trigram language model is built using 180 million English words.', 'Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment.', 'Shown in Table 4 are results for the small- data track; the large-data track results are in Table 5.', 'For the small-data track, the baseline Bleu scores for IBM1, HMM and IBM4 are 15.70, 17.70 and 18.25, respectively.', 'The UDA alignment of BiTAM1 gives an improvement over the baseline IBM1 from 15.70 to 17.93, and it is close to HMM’s performance, even though BiTAM doesn’t exploit any sequential structures of words.', 'The proposed BiTAM2 and BiTAM 3 are slightly better than BiTAM1.', 'Similar improvements are observed for the large-data track (see Table 5).', 'Note that, the boosted BiTAM3 us SE T TI N G IBM 1 H M M IBM 4 B I T A M 3 U D A BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E D ( % ) U N I O N ( % ) I N T E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N I S T B L E U 7.', '5 9 19 .1 9 7.', '7 7 21 .9 9 7.', '8 3 23 .1 8 7.', '64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table.', '1.', 'Other experimental conditions are similar to Table.', '4.', 'ing IBM4 as the seed lexicon, outperform the Refined IBM4: from 23.18 to 24.07 on Bleu score, and from 7.83 to 8.23 on NIST.', 'This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations.', 'In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.', 'Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.', 'The proposed models significantly improve the alignment accuracy and lead to better translation qualities.', 'Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.', 'A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word.', 'Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.', 'The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit.', 'Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.', 'One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction.', 'The term “multiword expression” has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on.', 'Calzolari et al.', '(2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis.', 'The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them.', 'For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die).', 'For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003).', 'It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997).', 'However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs.', 'In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system.', 'IR systems aim to provide users with quick access to data they are interested (BaezaYates and RibeiroNeto, 1999).', 'Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system.', 'The selection of appropriate indexing terms is a key factor for the quality of IR systems.', 'In an ideal system, the index terms should correspond to the concepts found in the documents.', 'If indexing is performed only with the atomic terms, there may be a loss of semantic content of the documents.', 'For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101–109, Portland, Oregon, USA, 23 June 2011.', 'Qc 2011 Association for Computational Linguistics return instead irrelevant documents about celestial bodies or carbonated drinks.', 'In order to investigate the effects of indexing of MWEs for IR, the results of queries are analyzed using IR quality metrics.', 'This paper is structured as follows: in section 2 we discuss briefly MWEs and some of the challenges they represent.', 'This is followed in section 3 by a discussion of the materials and methods employed in this paper, and in section 4 of the evaluation performed.', 'We finish with some conclusions and future work.', 'The concept of Multiword Expression has been widely viewed as a sequence of words that acts as a single unit at some level of linguistic analysis (Calzolari et al., 2002), or as Idiosyncratic interpretations that cross word boundaries (or spaces) (Sag et al., 2002).', 'One of the great challenges of NLP is the identification of such expressions, “hidden” in texts of various genres.', 'The difficulties encountered for identifying Multiword Expressions arise for reasons like: • the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); • even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); • in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain).', 'The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years.', 'With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques.', 'In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.', 'This approach is implemented in a lexico- graphic tool called Xtract.', 'More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates.', 'As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood.', 'Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others.', 'For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).', 'Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment.', 'The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents.', 'In this section we describe the different resources and methods used in the experiments.', '3.1 Resources and Tools.', 'For this evaluation we used two large newspaper corpora, containing a high diversity of terms: • Los Angeles Times (Los Angeles, USA1994) • The Herald (Glasgow, Scotland1995) Together, both corpora cover a large set of subjects present in the news published by these newspa pers in the years listed.', 'The language used is American English, in the case of the Los Angeles Times and British English, in the case of The Herald.', 'Hereafter, the corpus of the Los Angeles Times will be referred as LA94 and The Herald as GH95.', 'Together, they contain over 160,000 news articles (Table 1) and each news article is considered as a document.', 'C or pu s D oc u m en ts L A 9 4 1 1 0 . 2 4 5 G H 9 5 5 6 . 4 7 2 T o t a l 1 6 6 . 7 1 7 Table 1: Total documents The collection of documents, as well as the query topics and the list of relevance judgments (which will be discussed afterwards), were prepared in the context of the CLEF 2008 (Cross Language Evaluation Forum), for the task entitled Robust-WSD (Acosta et al., 2008).', 'This task aimed to explore the contribution of the disambiguation of words to bilingual or monolingual IR.', 'The task was to assess the validity of word-sense disambiguation for IR.', 'Thus, the documents in the corpus have been annotated by a disambiguation system.', 'The structure of a document contains information about the identifier of a term in a document (TERM ID), the lemma of a term (LEMA) and also its morphosyntactic tag (POS).', 'In addition, it contains the form in which the term appeared in the text (WF) and information of the term in the WordNet (Miller, 1995; Fellbaum, 1998) as SYNSET SCORE and CODE, both not used for <TERM ID=\"GH950102000000-126\" LEMA=\"underworld\" POS=\"NN\"> <WF>underworld</WF> <SYNSET SCORE=\"0.5\" CODE=\"06120171-n\"/> <SYNSET SCORE=\"0.5\" CODE=\"06327598-n\"/> </TERM> Figure 1: Structure of a term in the original documents In this paper, we extracted the terms located in the LEMA attribute, in other words, in their canonical form (e.g. letter bomb for letter bombs).', 'The use of lemmas and not the words (e.g. write for wrote, written, etc.) to the formation of the corpus, avoids linguistic variations that can affect the results of the experiments.', 'As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system.', 'For this task we used a tool called Zettair (Zettair, 2008), which is a compact textual search engine that can be used both for the indexing and for querying text collections.', 'Porter’s Stemmer (Porter, 1997) as implemented in Zettair was also used.', 'Stemming can provide further conflation of related terms.', 'For example, bomb and bombing were not merged in the lemmatized texts but after stemming they are conflated to a single representation.', 'After indexing, the next step is the preparation of the query topics.', 'Just as the corpus, only the lemmas of the query topics were extracted and used.', 'The test collection has a total of 310 query topics.', 'The judgment of whether a document is relevant to a query was assigned according to a list of relevant documents, manually prepared and supplied with the material provided by CLEF.', 'We used Zettair to generate the ranked list of documents retrieved in response to each query.', 'For each query topic, the 1,000 top scoring documents were selected.', 'We used the cosine metric to calculate the scores and rank the documents.', 'Finally, to calculate the retrieval evaluation metrics (detailed in Section 3.5) we used the tool trec eval.', 'This tool compares the list of retrieved documents (obtained from Zettair) against the list of relevant documents (provided by CLEF).', '3.2 Multiword Expression as Single Terms.', 'In this work, we focused on MWEs composed of exactly two words (i.e. bigrams).', 'In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using “ ” (e.g. letter bomb as letter bomb).', 'Figure 2 exemplifies this concatenation.', 'Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes.', 'The rationale was that documents containing specific MWEs can be indexed more adequately than those containing the words of the expression separately.', 'As a result, retrieval quality should increase.', '<TERM ID=\"GH950102000000-126\" LEMA=\"underworld\" POS=\"NN\"> <WF>underworld</WF> <SYNSET SCORE=\"0.5\" CODE=\"06120171-n\"/> <SYNSET SCORE=\"0.5\" CODE=\"06327598-n\"/> </TERM> Original Topic: - What was the role of the Hubble telescope in proving the existence of black holes?', 'Modified Topic: - what be the role of the hubble telescope in prove the existence of black hole ? black_hole <num>141</num> Figure 2: Modified query.', '<title> letter bomb for kiesbauer find information on the explosion of a letter bomb in the studio of the tv channel pro7 presenter arabella kiesbauer . 3le.t3ter_bMombullettitewr_obormdbEtv_xcpharnensesl ions Dictionaries </title> In order to determine the impact of the quality of the dictionary used in the performance of the IR system, we examined several different sources of MWE of varying quality.', 'The dictionaries containing the MWEs to be inserted into the corpus as a single term, are created by a number of techniques involving automatic and manual extraction.', 'Below we describe how these MWE dictionaries were created.', '• Compound Nouns (CN) - for the creation of this dictionary, we extracted all bigrams contained in the corpus.', 'Since the number of available bigrams was very large (99,744,811 bi- grams) we filtered them using the information in the original documents, the morphosyntactic tags.', 'Along with the LEMA field, extracted in the previous procedure, we also extracted the value of the field POS (part-of-speech).', 'In order to make the experiment feasible, we used only bigrams formed by compound nouns, in other words, when the POS of both words was NN (Noun).', 'Thus, with bigrams consisting of sequences of NN as a preprocessing step to eliminate noise that could affect the experiment, the number of bigrams with MWE candidates was reduced to 308,871.', 'The next step was the selection of bigrams that had the highest frequency in the text, so we chose candidates occurring at least ten times in the whole corpus.', 'As a result, the first list of MWEs was composed by 15,001 bigrams, called D1.', '• Best Compound Nouns - after D1, we refined the list with the use of statistical methods.', 'The methods used were the mutual information and chi-square.', 'It was necessary to obtain frequency values from Web using the search tool Yahoo!, because despite the number of terms per genre of our corpus would bias the counts.', 'For this work we used the number of pages in which a term occurs as a measure of frequency.', 'With the association measures based on web frequencies, we generated a ranking in decreasing order of score for each entry.', 'We merged the rankings by calculating the average rank between the positions of each MWE; the first 7,500 entries composed the second dictionary, called D2.', '• Worst Compound Nouns - this dictionary was created from bigrams that have between five and nine occurrences and are more likely to co- occur by chance.', 'It was created in order to evaluate whether the choice of the potentially more noisy MWEs entailed a negative effect in the results of IR, compared to the previous dictionaries.', 'The third dictionary, with 17,328 bi- grams, is called D3.', '• Gold Standard - this was created from a sub- list of the Cambridge International Dictionary of English (Procter, 1995), containing MWEs.', 'Since this list contains all types of MWEs, it was necessary to further filter these to obtain compound nouns only, using morphosyn- tactic information obtained by the TreeTagger (Schmid, 1994), which for English is reported to have an accuracy of 96.36%” (Schmid, 1994).', 'Formed by 568 MWEs, the fourth dictionary will be called D4.', '• Decision Tree - created from the use of the J48 algorithm (Witten and Frank, 2000) from Weka (Hall et al., 2009), a data mining tool.', 'With this algorithm it is possible to make a MWE classifier in terms of a decision tree.', 'This requires providing training data with true and false examples of MWE.', 'The training set contained 1,136 instances, half true (D4) and half false MWEs (taken from D3).', 'After combining several statistical methods, the best result for classification was obtained with the use of mutual information, chi-square, pointwise mutual information, and Dice.', 'The model obtained from Weka was applied to test data containing15,001 MWE candidates (D1).', 'The 12,782 bi tionary, called D5.', '• Manual - for comparative purposes, we also #Relevant P recision(P ) = n #Retrieved created two dictionaries by manually evaluating the text of the 310 query topics.', 'The first dictionary contained all bigrams which would achieve a different meaning if the words were concatenated (e.g. space shuttle).', 'This dictio #Retrieved #Relevant n #Retrieved Recall(R) = #Relevant (1) (2) nary, was called D6 and contains 254 expressions.', 'The other one was created by a specialist (linguist) who classified as true or false a list of MWE candidates from the query topics.', 'The linguist selection of MWEs formed D7 with 178 bigrams.', '3.4 Creating Indices.', 'For the experiments, we needed to manipulate the corpus in different ways, using previously built dictionaries.', 'The MWEs from dictionaries have been inserted in the corpus as single terms, as described Precision and Recall are set-based measures, therefore, they do not take into consideration the ordering in which the relevant items were retrieved.', 'In order to evaluate ranked retrieval results the most widely used measurement is the average precision (AvP ).', 'AvP emphasizes returning more relevant documents earlier in the ranking.', 'For a set of queries, we calculate the Mean Average Precision (MAP) according to Equation 3 (Manning et al., 2008).', 'before.', 'For each dictionary, an index was created in 1 M AP (Q) = |Q| mj 1 P (R ) (3) the IR system.', 'These indices are described below: 1.', 'Baseline (BL) - corpus without MWE..', '2.', 'Compound Nouns (CN) - with 15 MWEs of.', 'D1.', '3.', 'Best CN (BCN) - with 7,500 MWEs of D2..', '6.', 'Decision Tree (DT) - with 12,782 MWEs of.', 'D5.', '7.', 'Manual 1 (M1) - with 254 MWEs of D6..', '8.', 'Manual 2 (M2) - with 178 MWEs of D7..', '3.5 Evaluation Metrics.', 'To evaluate the results of the IR system, we need to use metrics that estimate how well a user’s query was satisfied by the system.', 'IR evaluation is based on recall and precision.', 'Precision (Eq. 1) is the portion of the retrieved documents which is actually relevant to the query.', 'Recall (Eq. 2) is the fraction of the relevant documents which is retrieved by the |Q| j=1 mj k=1 where |Q| is the number of queries, Rjk is the set of ranked retrieval results from the top result until document dk , and mj is the number of relevant documents for query j. 4 Experiment and Evaluations.', 'The experiments performed evaluate the insertion of MWEs in results obtained in the IR system.', 'The analysis is divided into two evaluations: (A) total set of query topics, where an overview is given of the MWE insertion effects and (B) topics modified by MWEs, where we evaluate only the query topics that contain MWEs.', '4.1 Evaluation A. This evaluation investigates the effects of inserting MWEs in documents and queries.', 'After each type of index was generated, MWEs were also included in the query topics, in accordance to the dictionaries used for each index (for Baseline BL, the query topics had no modifications).', 'With eight corpus variations, we obtained individual results for each one of them.', 'The results presented in Table 2 were summarized by the ab the MAP for the entire set of query topics.', 'In total, 6,379 relevant documents are returned for the 310 query topics.', 'is almost 20% of cases and the WCN, the difference between gain and loss is less than 2%.', 'Table 3: BCN x Baseline Table 2: Results — Evaluation A. It is possible to see a small improvement in the results for the indices M1 and M2 in relation to the baseline (BL).', 'This happens because the choice of candidate MWEs was made from the contents of the document topics and not, as with other indices, from the whole corpus.', 'Considering the indices built with MWEs extracted from the corpus, the best result is index GS.In second place, comes the CN index, with a subtle improvement over the Baseline.', 'BL surprisingly got a better result than the Best and Worst CN.', 'The loss in retrieval quality as a result from MWE identification for BCN was not expected.', 'When comparing the gain or loss in MAP of individual query topics, we can see how the index BCN compares to the Baseline: BCN had better MAP in 149 and worse MAP in 108 cases.', 'However, the average loss is higher than the average gain, this explains why BL obtains a better result overall.', 'In order do decide if one run is indeed superior to another, instead of using the absolute MAP value, we chose to calculate a margin of 5%.', 'The intuition behind this is that in IR, a difference of less than 5% between the results being compared is not considered significant (Buckley and Voorhees, 2000).', 'To be considered as gain the difference between the values resulting from two different indices for the same query topic should be greater than 5%.', 'Differences of less than 5% are considered ties.', 'This way, MAP values of 0.1111 and 0.1122 are considered ties.', 'Given this margin, we can see in Tables 3 and 4 that the indices BCN and WCN are better compared to the baseline.', 'In the case of BCN, the gain Table 4: WCN x Baseline Finally, this first experiment guided us toward a deeper evaluation of the query topics that have MWEs, because there is a possibility that the MWE insertions in documents can decrease the accuracy of the system on topics that have no MWE.', '4.2 Evaluation B. This evaluation studies in detail the effects on the document retrieval in response to topics in which there were MWEs.', 'For this purpose, we used the same indices used before and we performed an individual evaluation of the topics, to obtain a better understanding on where the identification of MWEs improves or degrades the results.', 'As each dictionary was created using a different methodology, the number of expressions contained in each dictionary is also different.', 'Thus, for each method, the number of query topics considered as having MWEs varies according to the dictionary used.', 'Table 5 shows the number of query topics containing MWEs for each dictionary used, and as a consequence, the percentage of modified query topics over the complete set of 310 topics.', 'First, it is interesting to observe the values of MAP for all topics that have been altered by the identification of MWEs.', 'These values are shown in Table 6.', 'As shown in Table 6 we verified that the GS index obtained the best result compared to others.', 'This M Table 5: Topics with MWEs In de x M A P C N 0.', '10 11 B C N 0.', '09 39 W C N 0.', '12 24 G S 0.', '23 93 D T 0.', '11 93 M 1 0.', '12 62 M 2 0.', '12 36 Table 6: Results - Evaluation B was somewhat expected since the MWEs in that dictionary are considered “real” MWEs.', 'After GS, best results were obtained from the manual indices M1 and M2.', 'The index that we consider as containing the lowest confident MWEs (WCN), obtained better results than Decision Trees, Nominal Compounds and Best Nominal Compounds, in this order.', 'One possible reason for this to happen is that the number of MWEs inserted is higher than in the other indices.', 'Compared with the BL, all indices with MWE insertion have improved more than degraded the results, in quantitative terms.', 'Our largest gain was with the index GS, where 55.56% of the topics have improved, but the same index showed the highest percentage of loss, 22.22%.', 'Analyzing the WCN, we can identify that this index has the lowest gain compared to all other indices: 32.14%, although having also the lowest loss.', 'But, 60.71 % of the topics modified had no significant differences compared to the Baseline.', 'Thus, we can conclude that the WCN index is the one that modifies the least the result of a query.', 'The indices CN and BCN had a similar result, and knowing that a dictionary used to create BCN is a subset of the dictionary CN, we can conclude that the gain values, choosing the best MWE candidates, <TERM ID=\"GH950102000000-126\" LEMA=\"underworld\" POS=\"NN\"> <WF>underworld</WF> <SYNSET SCORE=\"0.5\" CODE=\"06120171-n\"/> <SYNSET SCORE=\"0.5\" CODE=\"06327598-n\"/> </TERM> Original Topic: - What was the role of the Hubble telescope in proving the existence of black holes?', 'Modified Topic: - what be the role of the hubble telescope in prove the existence of black hole ? black_hole <num>141</num> <title> letter bomb for kiesbauer find information on the explosion of a letter bomb in the studio of the tv channel pro7 presenter arabella kiesbauer . letter_bomb letter_bomb tv_channel </title> Figure 3: Topic #141 Table 7 shows the top ten scoring documents retrieved for query topic 141 in the baseline.', 'The relevant document (in bold) is the fourth position in the Baseline.', 'After inserting the expression letter bomb twice (because it occurs twice in the original topic), and tv channel that were in dictionary D1 used by the CN index, the relevant document is scored higher and as a consequence is returned in the first position of the ranking(Table 8) . The MAP of this topic has increased 75 percentage points, from 0.2500 in Baseline to 1.000 in the CN index.', 'We see also that the document that was in first position in the Baseline ranking, has its score decreased and was ranked in fourth position in the ranking given by the CN.', 'This document contained information on a “small bomb located outside the of the Russian embassy” and has is not relevant to topic 141, being properly relegated to a lower position.', 'An interesting fact about this topic is that only the MWE letter bomb influences the result.', 'This was verified as in the index BCN, whose dictionary does not have this MWE, the topic was changed only because of the MWE tv channel and there was no gain or loss for the result.', 'The second highest gain was of M1 index, in topic 173.', 'The gain was of 28 percentage points.', 'On.', 'the other hand, we found a downside in M1 and M2 indices, although they improved results on average, they have reached very high values of loss in some topics.', 'Po sit io n D o c u m e n t S c o r e P 1 L A 04 30 94 02 30 0.', '47 09 00 P 2 G H9 50 823 00 01 05 0.', '45 99 94 P 3 G H9 51 120 00 01 82 0.', '43 95 36 P 4 G H9 50 610 00 01 64 0.', '43 07 84 P 5 G H9 50 614 00 01 22 0.', '42 87 66 P 6 L A 09 18 94 04 25 0.', '42 84 29 P 7 G H9 50 829 00 00 82 0.', '42 29 41 P 8 G H9 50 220 00 01 62 0.', '41 19 68 P 9 G H9 50 318 00 01 31 0.', '40 60 06 P 1 0 G H9 50 829 00 00 37 0.', '40 28 06 Table 7: Ranking for Topic #141 - Baseline Po sit io n D o c u m e n t S c o r e P 1 G H9 50 610 00 01 64 0.', '45 79 50 P 2 G H9 50 614 00 01 22 0.', '43 67 53 P 3 G H9 50 823 00 01 05 0.', '42 39 38 P 4 L A 04 30 94 02 30 0.', '42 17 57 P 5 G H9 51 120 00 01 82 0.', '40 01 23 P 6 G H9 50 829 00 00 82 0.', '39 31 95 P 7 L A 09 18 94 04 25 0.', '38 66 13 P 8 G H9 50 705 00 01 00 0.', '38 41 16 P 9 G H9 50 220 00 01 62 0.', '38 21 57 P 1 0 G H9 50 318 00 01 31 0.', '38 04 71 Table 8: Ranking for Topic #141CN In sum, the MWEs insertion seems to improve retrieval bringing more relevant documents, due to a more precise indexing of specific terms.', 'However, the use of these expressions also brought a negative impact for some cases, because some topics require a semantic analysis to return relevant documents (as for example topic 130, which requires relevant documents to mention the causes of the death of Kurt Cobain — documents which mention his death without mentioning the causes were not considered relevant).', 'This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions.', 'MWEs are found in all genres of texts and their appropriate use is being targeted for study, both in linguistics and computing, due to the different characteristic variations of this type of expression, which ends up causing problems for the success of computational methods that aim their processing.', 'In this work we aimed at achieving a better understanding of several important points associated with the use of Multiword Expressions in IR systems.', 'In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms.', 'Nevertheless, the use of these expressions made the results worse in some c]ases, because some topics require a semantic analysis to return relevant documents.', 'Some of these documents are related to the query, but do not satisfy all criteria in the query topic.', 'We conclude also that the quality of MWEs used directly influenced the results.', 'For future work, we would like to use other MWE types and not just compound nouns as used in this work.', 'Other methods of extraction and a further study in Named Entities are good themes to complement this subject.', 'A variation of corpora, different from newspaper articles, because each domain has a specific terminology, can also be an interesting subject for further evaluation.']\n",
      "['Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', 'It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.', 'Foma is largely compatible with the Xerox/PARC finite-state toolkit.', 'It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the ‘Mathematical Operators’ Unicode block.', 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', 'The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&T’s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).', 'One of Foma’s design goals has been compatibility with the Xerox/PARC toolkit.', 'Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.', 'Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.', 'The compiler and library are implemented in C and an API is available.', 'The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.', 'However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.', 'These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions.', 'The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton.', 'This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.', 'Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.', 'It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.', 'Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways, either through ASCII-based operators or through the Unicode-based extensions.', 'For example, one can either say: ContainsX = Σ* X Σ*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 ∩ Σˆ<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29–32, Athens, Greece, 3 April 2009.', 'Qc 2009 Association for Computational Linguistics Operators Compatibility variant Function [ ] () [ ] () grouping parentheses, optionality ∀ ∃ N/A quantifiers \\\\ ‘ term negation, substitution/homomorphism : : cross-product + ∗ + ∗ Kleene closures ˆ<n ˆ>n ˆ{m,n} ˆ<n ˆ>n ˆ{m,n} iterations 1 2 .1 .2 .u .l domain & range .f N/A eliminate all unification flags $ $.', '˜ $ $.', 'complement, containment operators / ./.', 'N/A N/A ‘ignores’, left quotient, right quotient, ‘inside’ quotient ∈ ∈/ = /= N/A language membership, position equivalence ≺ < > precedes, follows ∨ ∪ ∧ ∩ - .P. .p. | & − .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) × ◦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence.', 'Horizontal lines separate precedence classes.', 'define ContainsX ?* X ?*; define MyWords {cat}|{dog}|{mouse}; define MyRule n -> m || _ p; define ShortWords Mylex.i.l & ?ˆ<6; In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.', 'One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions (Hulden, 2008).', 'For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (∃x)(x ∈ L ∧ (∃y)(y ∈ L ∧ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to ∈ and ∧, and a kind of concatenative meaning to the predicate S(t1, t2).', 'Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.', 'In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions.', 'As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.', 'This practice stems back from the earliest two-level compilers (Karttunen et al., 1987).', 'Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%ˆs #; +Sing #;', 'The Foma API gives access to basic functions, such as constructing a finite-state machine from a regular expression provided as a string, performing a transduction, and exhaustively matching against a given string starting from every position.', 'The following basic snippet illustrates how to use the C API instead of the main interface of Foma to construct a finite-state machine encoding the language a+b+ and check whether a string matches it: 1.', 'void check_word(char *s) { 2.', 'fsm_t *network; 3.', 'fsm_match_result *result; 4.', '5. network = fsm_regex(\"a+ b+\"); 6.', 'result = fsm_match(fsm, s); 7.', 'if (result->num_matches > 0) 8.', 'printf(\"Regex matches\"); 9.', '10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol(\"a\")), fsm_kleene_plus( fsm_symbol(\"b\"))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.', 'educational use Foma has support for visualization of the machines it builds through the AT&T Graphviz library.', 'For educational purposes and to illustrate automata construction methods, there is some support for changing the behavior of the algorithms.', 'For instance, by default, for efficiency reasons, Foma determinizes and minimizes automata between nearly every incremental operation.', 'Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata.', 'However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possible—non-deterministic automata naturally being easier to inspect and analyze.', 'Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.', 'With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.', 'Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.', 'One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines.', 'For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).', 'The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata.', 'Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.', 'Foma is free software and will remain under the GNU General Public License.', 'As the source code is available, collaboration is encouraged.', 'GNU AT&T Foma xfst flex fsm 4 Σ∗aΣ15 0.216s 16.23s 17.17s 1.884s Σ∗aΣ20 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.', 'The first and second entries are short regular expressions that exhibit exponential behavior.', 'The second results in a FSM with 221 states and 222 arcs.', 'The others are scripts that can be run on both Xerox/PARC and Foma.', 'The file lexicon.lex is a LEXC format English dictionary with 38418 entries.', 'North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.']\n",
      "['We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.', 'The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.', 'Using generic approximate dynamic programming techniques, this decoder can handle “non-local” features.', 'Similar approximate inference techniques support efficient parameter estimation with hidden variables.', 'We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on non- isomorphism.', 'We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).', 'Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007).', 'Lopez (2009) recently argued for a separation between features/formalisms (and the indepen 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.).', 'Features are often implied by a choice of formalism.', 'dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.', 'Here we take first steps toward such a “universal” decoder, making the following contributions:Arbitrary feature model (§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.', 'The trees are optional and can be easily removed, allowing simulation of “string-to-tree,” “tree-to-string,” “tree- to-tree,” and “phrase-based” models, among many others.', 'We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.', 'Decoding as QG parsing (§3–4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “non- local” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.', 'Because we start with inference (the key subroutine in training), many other learning algorithms are possible.', 'Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled experiments.', 'We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, Singapore, 67 August 2009.', 'Qc 2009 ACL and AFNLP Σ, T Trans : Σ ∪ {NULL} → 2T s = (s0 , . . .', ', sn ) ∈ Σn t = (t1 , . . .', ', tm ) ∈ Tm τs : {1, . . .', ', n} → {0, . . .', ', n} τt : {1, . . .', ', m} → {0, . . .', ', m} a : {1, . . .', ', m} → 2{1,...,n} θ source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where τs (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where τt (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; ∅ denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (§2.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tj−N +1 ) language model features (§2.2): N -gram probabilities gsyn (t, τt ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (§2.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I ⊆ {1, . . .', ', m} greor (s, τs , a, t, τt ) f dist (i, j) reordering features (§2.4): distortion features for a source word at position i aligned to a target word at position j gtree 2 (τs , a, τt ) f qg (i, il, j, k) tree-to-tree syntactic features (§3): configuration features for source pair si /sil being aligned to target pair tj /tk gcov (a) f scov (a), f zth (a), f sunc (a) coverage features (§4.2) counters for “covering” each s word each time, the zth time, and leaving it “uncovered” Table 1: Key notation.', 'Feature factorings are elaborated in Tab.', '2.', 'bination of the two.', 'We quantify the effects of our approximate inference.', 'We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG.', 'We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.', '(Table 1 explains notation.)', 'Given a sentence s and its parse tree τs, we formulate the translation on the feasibility of inference, including decoding.', 'Typically these feature functions are chosen to factor into local parts of the overall structure.', 'We next define some key features used in current MT systems, explaining how they factor.', 'We will use subscripts on g to denote different groups of features, which may depend on subsets of the structures t, τt, a, s, and τs. When these features factor into parts, we will use f to denote the factored vectors, so that if x is an object that breaks into parts {xi}i, then g(x) = �i f (xi).', '4 problem as finding the target sentence t∗ (along with its parse tree τ ∗ source tree) such that3 and alignment a∗ to the 2.1 Lexical.', 'Translations Classical lexical translation features depend on s and t and the alignment a between them.', 'The sim (t∗, τ ∗, a∗) = argmax p(t, τt, a | s, τs) (1) t,τt ,aa In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model.', 'That is, p(t, τt, a | s, τs) = exp{θTg(s, τs, a, t, τt)} plest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s ∈ Σ and t ∈ T. Phrase-to-phrase features generalize these, estimated as p(tl | sl) and p(sl | tl) where sl (respectively, tl) is a substring of s (t).', 'A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into �al,tl,τ l exp{θTg(s, τs, al, tl, τ l)} (2) disjoint parts of the source and target sentences t t 4 There are two conventional definitions of feature func-.', 'where the g are arbitrary feature functions and the θ are feature weights.', 'If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables.', 'In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed.', 'In principle, we might include source-side parsing as part of decoding.', 'tions.', 'One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002).', 'These estimates are usually heuristic and inconsistent (Koehn et al., 2003).', 'An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008).', 'This offers more expressive power but may require much more training data to avoid overfitting.', 'For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.', '(Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any func g (s, a, t) = Pm i∈a(j) f lex (si , tj ) (3) tion of words and alignments, we permit features + P f (slast (i,j) , tj ) that consider phrase pairs in which a target word g (t) = P i,j:1≤i<j≤m Pm+1 phr first (i,j) i j lm N ∈{2,3} j=1 f N (tj−N +1 ) (4) outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, gsyn (t, τt ) = Pm j τ (j) , τt (j)) val t (j)) (5) 2007).', 'g (s, τs , a, t, τt ) = Pm m P i∈a(j) f dist (i, j) (6) Lexical translation features factor as in Eq. 3 (Tab.', '2).', 'We score all phrase pairs in a sentence pair that pair a target phrase with the smallest gtree 2 (τs , a, τt ) = X f qg (a(j), a(τt (j)), j, τt (j)) (7) j=1 source phrase that contains all of the alignments in Table 2: Factoring of global feature collections g into f . xj denotes (xi , . . .', 'xj ) in sequence x = (x1 , . . .).', 'the target phrase; if k:i≤k≤j a(k) = ∅, no phrase i first (i, j) = mink:i≤k≤j (min(a(k))) and last (i, j) = feature fires for tj . maxk:i≤k≤j (max(a(k))).', '2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.', 'For bigrams and trigrams (used in this paper), the factoring is in Eq. 4 (Tab.', '2).', '2.3 Target Syntax.', 'There have been many features proposed that consider source- and target-language syntax during translation.', 'Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.', 'For example, Quirk et al.', '(2005) use features involving phrases and source- side dependency trees and Mi et al.', '(2008) use features from a forest of parses of the source sentence.', 'There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008).', 'In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).', 'In this work, we focus on syntactic features of target-side dependency trees, τt, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.', 'They factor as in Eq. 5 (Tab.', '2).', 'Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008).', '5 Segmentation might be modeled as a hidden variable in future work.', '2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007).', 'In syntax-based systems, reordering is typically parameterized by grammar rules.', 'For generality we permit these features to “see” all structures and denote them greor (s, τs, a, t, τt).', 'Eq. 6 (Tab.', '2) shows a factoring of reordering features based on absolute positions of aligned words.', 'We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features.', 'Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt, a | s, τs).', 'Given a source sentence s and its parse τs, a QDG induces a probabilistic monolingual dependency grammar over sentences “inspired” by the source sentence and tree.', 'We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Each word generated by Gs,τs is annotated with a “sense,” which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in τt and nodes in τs. In principle, any portion of τt may align to any portion of τs, but in practice we often make restrictions on the alignments to simplify computation.', 'Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.)', 'As usual, the normalization constant is not required for decoding; it suffices to solve: t , a ) = argmax θ g(s, τ , a, t, τ ) (8)Which translations are possible depends heav ily on the configurations that the QDG permits.', '(t∗, τ ∗ ∗ T s t t,τt ,aa Formally, for a parent-child pair (tτt (j), tj ) in τt, we consider the relationship between a(τt(j)) and a(j), the source-side words to which tτt (j) and tj align.', 'If, for example, we require that, for all j, a(τt(j)) = τs(a(j)) or a(j) = 0, and that the root of τt must align to the root of τs or to NULL, then strict isomorphism must hold between τs and τt, and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005).', 'Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.', '(“a(τt(j)) = τs(a(j))” corresponds to their “parent-child” configuration; see Fig.', '3 in Smith and Eisner (2006) for illustrations of the rest.)', 'More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab.', '2).', 'Note that the QDG instantiates the model in Eq. 2.', 'Of the features discussed in §2, f lex , f att , f val , and f dist can be easily incorporated into theQDG as described while respecting the indepen dence assumptions implied by the configuration features.', 'The others (f phr , f 2, and f 3) are non- local, or involve parts of the structure that, from the QDG’s perspective, are conditionally independent given intervening material.', 'Note that “non locality” is relative to a choice of formalism; in §2 we did not commit to any formalism, so it is only now that we can describe phrase and N -gram features as non-local.', 'Non-local features will present a challenge for decoding and training (§4.3).', 'Given a sentence s and its parse τs, at decoding time we seek the target sentence t∗, the target tree For a QDG model, the decoding problem has not been addressed before.', 'It equates to finding the most probable derivation under the s/τs-specific grammar Gs,τs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.', 'The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.', 'A major advantage of DP is that, with small modifications, summing over structures is also possible with “inside” DP algorithms.', 'We will exploit this in training(§5).', 'Efficient summing opens up many possibilities for training θ, such as likelihood and pseudo likelihood, and provides principled ways to handle hidden variables during learning.', '4.1 Translation as Monolingual Parsing.', 'We decode by performing lattice parsing on a lattice encoding the set of possible translations.', 'The lattice is a weighted “sausage” lattice that permits sentences up to some maximum length £; £ is derived from the source sentence length.', 'Let the states be numbered 0 to £; states from lρ£J to £ are final states (for some ρ ∈ (0, 1)).', 'For every position between consecutive states j − 1 and j (0 < j ≤ £), and for every word si in s, and for every word t ∈ Trans(si), we instantiate an arc annotated with t and i. The weight of such an arc is exp{θTf }, where f is the sum of feature functions that fire when si translates as t in target position j (e.g., f lex (si, t) and f dist (i, j)).', 'Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms τt , and the alignments a∗ that are most probable, (Eisner, 1997).', 'This decoder accounts for f lex ,as defined in Eq. 1.7 (In §5 we will consider kbest and all-translations variations on this prob 6 I.e., from here on, a : {1, . . .', ', m} → {0, . . .', ', n} where 0 denotes alignment to NULL.', 'f att , f val , f dist , and f qg as local features.', 'Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice repre 7 Arguably, we seek argmax.', 'p(t | s), marginalizing out senting possible translations.', 'In each bundle, the everything else.', 'Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work.', 'arcs are listed in decreasing order according to weight and for clarity only the first five are shown.', 'The output of the decoder consists of lattice arcs Source: $ konnten sie es übersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:?', 'übersetzen: ?:?', 'übersetzen: sie:you sie:you konnten:could translate übersetzen: translate übersetzen: konnten:couldn es:it sie :you translated translated konnten:might es:it sie:let sie:them ?:?', 'übersetzen: translate es:it konnten:could es:it NULL:to ...', 'Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them.', 'selected at each position and a dependency tree over them.', '4.2 Source-Side Coverage Features.', 'Most MT decoders enforce a notion of “coverage” covered the zth time (z ∈ {2, 3, 4}) and fire again all subsequent times it is covered; these are denoted f 2nd, f 3rd, and f 4th.', '• A counter of uncovered source words: of the source sentence during translation: all parts f sunc (a) = �n δ(|a−1(i)|, 0).', 'of s should be aligned to some part of t (alignment to NULL incurs an explicit cost).', 'Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated.', 'Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in τt (or a deliberate choice is made by the decoder to translate it to NULL).', 'In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder.', 'Our QDG decoder has no way to enforce coverage; it does not track any kind of state in τs apart from a single recently aligned word.', 'This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993).', 'Thissacrifice is the result of our choice to use a condi Of these, only f scov is local.', '4.3 Non-Local Features.', 'The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them.', 'Phrase lexicon features f phr , language model features f N for N > 1, and most coverage features are non-local with respect to our QDG.', 'Recently Chiang (2007) introduced “cube pruning” as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits.', 'Techniques like cube pruning can be used to include the non-local features in our decoder.8', 'Training requires us to learn values for the parameters θ in Eq. 2.', 'Given T training examples of the tional model (§2).', 'form (t (i) , τ (i), s (i) , τ (i)), for i = 1, ..., T , max The solution is to introduce a set of coverageimum likelihood estimation for this model con 9 features gcov (a).', 'Here, these include: sists of solving Eq. 9 (Tab.', '3).', 'Note that the • A counter for the number of times each source 8 A full discussion is omitted for space, but in fact we use “cube decoding,” a slightly less approximate, slightly more word is covered: f scov (a) = �n |a−1(i)|.', 'expensive method that is more closely related to the approximate inference methods we use for training, discussed in §5.', '• Features that fire once when a source word is 9 In practice, we regularize by including a term −c θ 2 ..', 'T T P exp{θTg(s(i) , τ (i) , a, t(i) , τ (i) )} T “numerator” LL(θ) = X log p(t(i) , τ (i) | s(i) , τ (i) ) = X log a s t = X log (9) i=1 t s i=1 T t,τt ,a exp{θ g(s (i) , τ (i) , a, t, τ )} T i=1 “denominator” PL(θ) = X log„X p(t(i) , a « τ (i) , s(i) , τ (i) ) X log„X p(τ (i) , a « t(i) , s(i) , τ (i) ) (10) “denominator” of i=1 a n X X 1 | t s l n T ` + t | i=1 a l l s o (11) term 1 in Eq. 10 = i=0 tl ∈Trans(si ) S(τt (0), i, t ) × exp θ f lex (si , t ) + f att ($, 0, t , k) + f qg (0, i, 0, k)´ n S(j, i, t) = Y X X S(k, il, tl) × exp θT „ lex (sil , tl) + f 1 att (t, j, tl, k)+ «ff (12) k∈τ −1 (j) il =0 tl ∈Trans(sil ) f val (t, j, τ − (j)) + f qg (i, il, j, k) S(j, i, t) = exp nθT `f (t, j, τ −1 (j))´o if τ −1 (j) = ∅ (13) val t t Table 3: Eq. 9: Log-likelihood.', 'Eq. 10: Pseudolikelihood.', 'In both cases we maximize w.r.t. θ.', 'Eqs.', '11–13: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008).', 'Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA).', 'This requires us to calculate the function’s gradient (vector of first derivatives) with respect to θ.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast “inside” DP solution is known (Smith and Eisner, 2006; Wang et al., 2007).', 'It runs in O(mn2) time and O(mn) space.', 'Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.', 'With a maximum length imposed, this is tractable using the “inside” version of the maximizing DP algorithm of Sec.', '4, but it is prohibitively expensive.', 'We therefore optimize pseudo-likelihood instead, making the following approximation (Be 10 Alignments could be supplied by automatic word alignment algorithms.', 'We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses.', '11 When the function’s value is computed by “inside” DP, the corresponding “outside” algorithm can be used to obtain the gradient.', 'Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al.', '(2005).', 'sag, 1975): p(t, τt | s, τs) ≈ p(t | τt, s, τs) × p(τt | t, s, τs) Plugging this into Eq. 9, we arrive at Eq. 10 (Tab.', '3).', 'The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown).', 'The numerators are identical to each other and to that in Eq. 9.', 'The denominators are much more manageable than in Eq. 9, never requiring summation over more than two structures at a time.', 'We must sum over target word sequences and word alignments (with fixed τt), and separately over target trees and word alignments (with fixed t).', '5.1 Summing over t and a. The summation over target word sequences and alignments given fixed τt bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992).', 'Let S(j, i, t) denote the sum of all translations rooted at position j in τt such that a(j) = i and tj = t. Tab.', '3 gives the equations for this DP: Eq. 11 is the quantity of interest, Eq. 12 is the recursion, and Eq. 13 shows the base cases for leaves of τt.Letting q = max0≤i≤n |Trans(si)|, this algo rithm runs in O(mn2q2) time and O(mnq) space.', 'For efficiency we place a hard upper bound on q during training (details in §6).', '5.2 Summing over τt and a. For the summation over dependency trees and alignments given fixed t, required for p(τt | t, s, τs), we perform “inside” lattice parsing with Gs,τs . The technique is the summing variant of the decoding method in §4, except for each state j, the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3) time and requires O(a2) space.', 'Because we use a hard upper bound on |Trans(s)| for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments.', '5.3 Handling Non-Local Features.', 'So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N > 1, f zth, f sunc ).', 'We recently proposed “cube summing,” an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).', 'Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item.', 'Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features.', 'Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudo- likelihood, as well as to compute their approximate gradients.', 'ment set of 934 sentences, and a test set of 500 sentences.', 'We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching.', '6.2 Features.', 'Our base system uses features as discussed in §2.', 'To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007).', 'We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3.', 'We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized align ments.', 'After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words × phrase conditional and “lexical smoothing” probabilities × two conditional directions.', 'Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998).Our approach permits an alternative to mini mum error-rate training (MERT; Och, 2003); it is For our target-language syntactic features g syn , discriminative but handles latent structure and regularization in more principled ways.', 'The pseudo- likelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA’s inner loop faster than MERT’s inner loop.', 'Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.', '6.1 Data and Evaluation.', 'We use the GermanEnglish portion of the Basic Travel Expression Corpus (BTEC).', 'The corpus has approximately 100K sentence pairs.', 'We filter sentences of length more than 15 words, which only removes 6% of the data.', 'We end up with a training set of 82,299 sentences, a develop we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004).', 'These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).', 'These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).', 'The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003).', 'In total, there are 7 lexical and 7 word-class syntax features.', 'For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i−j| whenever a(j) = i and i, j > 0.', '(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.)', 'The tree-to-tree syntactic features gtree 2 in our model are binary features f qg that fire for particular QG configurations.', 'We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att ∪ f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).', 'tions involving root words and NULL-alignments more finely.', 'There are 14 features in this category.', 'Coverage features gcov are as described in §4.2.', 'In all, 46 feature weights are learned.', '6.3 Experimental Procedure.', 'Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn θ on the development set.12 We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01.', 'We used £2 regularization with a fixed, untuned coefficient of 0.1.', 'Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified.', 'To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t) × p(t | s) to score target words.', 'For any training sentence (s, t) and tj for which gcov . The results are shown in Table 4.', 'The second row contains scores when adding in the eight f phr features.', 'The second column shows scores when adding the 14 target syntax features (f att and f val ), and the third column adds to them the 14 additional tree-to-tree features (f qg ).', 'We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality.', '6.5 Varying k During Decoding.', 'For models without syntactic features, we constrained the decoder to produce dependency trees in which every word’s parent is immediately to its right and ignored syntactic features while scoring structures.', 'This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate.', 'Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.', 'Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig.', '2.', 'Scores improve when we increase k up tj /∈ n Trans(si), we added tj to Trans(si) to 10, but not much beyond, and there is still a i l l substantial gap (2.5 BLEU) between using phrase for = arg ma xil∈ I p(si |tj ) × p(tj |si ), wh ere I i i n Trans(si)| < qi}.', 'features with k = 20 and using all features with We used q0 = 10 and q>0 = 5, restricting k = 5.', 'Models without syntax perform poorly |Trans(NULL)| ≤ 10 and |Trans(s)| ≤ 5 for anys ∈ Σ.', 'This made 191 of the development sentences unreachable by the model, leaving 743 sen tences for learning θ.', 'During decoding, we generated lattices with all t ∈ Trans(si) for 0 ≤ i ≤ n, for every position.', 'We used ρ = 0.9, causing states within 90% of the source sentence length to be final states.', 'Between each pair of consecutive states, we pruned edges that fell outside a beam of 70% of the sum of edge weights (see §4.1; edge weights use f lex , f dist , and f scov ) of all edges between those two states.', '6.4 Feature Set Comparison.', 'Our first set of experiments compares feature sets commonly used in phrase- and syntax-based trans when using a very small k, due to their reliance on non-local language model and phrase features.', 'By contrast, models with syntactic features, which are local in our decoder, perform relatively well even with k = 1.', '6.6 QG Configuration Comparison.', 'We next compare different constraints on isomorphism between the source and target dependency 0.55 0.50 0.45 0.40 0.35 Phrase + Syntactic lation.', 'In particular, we compare the effects of combining phrase features and syntactic features.', 'The base model contains f lex , glm , greor , and 12 We made this choice both for similarity to standard MT. 0.30 0.25 0.20 Phrase Syntactic Neither 0 5 10 15 20 Value of k for Decoding systems and a more rapid experiment cycle.', 'Figure 2: Comparison of size of k-best list for cube decoding with various feature sets.', 'QD G Co nfi gu rati on s BL E U M ET E O R sy nc hr on ou s + nul ls, root an y + child par ent , sa me no de + sib lin g + gr an dp ar ent /ch ild + c co m ma nd + oth er 0.4 00 8 0.4 10 8 0.4 33 7 0.4 88 1 0.5 01 5 0.5 15 6 0.5 14 2 0 . 6 9 4 9 0 . 6 9 3 1 0 . 6 8 1 5 0 . 7 2 1 6 0 . 7 3 6 5 0 . 7 4 4 1 0 . 7 4 7 2 Table 5: QG configuration comparison.', 'The name of each configuration, following Smith and Eisner (2006), refers to the relationship between a(τt (j)) and a(j) in τs . trees.', 'To do this, we impose harsh penalties on some QDG configurations (§3) by fixing their feature weights to −1000.', 'Hence they are permit ted only when absolutely necessary in training and rarely in decoding.13 Each model uses all phrase and syntactic features; they differ only in the sets of configurations which have fixed negative weights.', 'Tab.', '5 shows experimental results.', 'The base “synchronous” model permits parent-child (a(τt(j)) = τs(a(j))), any configuration where a(j) = 0, including both words being linked to NULL, and requires the root word in τt to be linked to the root word in τs or to NULL(5 of our 14 configurations).', 'The second row allows any configuration involving NULL, including those where tj aligns to a non-NULL word in s and its parent aligns to NULL, and allows the root in τt to be linked to any word in τs. Each subsequent row adds additional configurations (i.e., trains its θ rather than fixing it to −1000).', 'In general, wesee large improvements as we permit more con figurations, and the largest jump occurs when we add the “sibling” configuration (τs(a(τt(j))) = τs(a(j))).', 'The BLEU score does not increase, however, when we permit all configurations in the final row of the table, and the METEOR score increases only slightly.', 'While allowing certain categories of non-isomorphism clearly seems helpful, permitting arbitrary violations does not appear to be necessary for this dataset.', '6.7 Discussion.', 'We note that these results are not state-of-the- art on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3).14 Our aim has been to 13 In fact, the strictest “synchronous” model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized.', '14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation.', 'illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints.', 'Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism.', 'We have validated cube summing and decoding as practical methods for approximate inference.', 'Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses’ phrase segmentation variable), and, of course, additional feature representations.', 'The system is publicly available at www.ark.cs.', 'cmu.edu/Quipu.', 'We presented feature-rich MT using a principled probabilistic framework that separates features from inference.', 'Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle “non-local” features using generic techniques that also support efficient parameter estimation.', 'Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints.', 'We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper.', 'This research was supported by NSF IIS0836431 and IIS0844507, a grant from Google, and computational resources provided by Yahoo.']\n",
      "['We supplement WordNet entries with information on the subjectivity of its word senses.', 'Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.', 'The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.', 'We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.', 'The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.', 'There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005).', 'Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed.', 'Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005).', 'Thus, the word positive the sentence contains a favourable opinion.', 'However, such word-based indicators can be misleading for two reasons.', 'First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004).', 'Second, different word senses of a single word can actually be of different subjectivity or polarity.', 'A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive—having a positive electric charge;“protons are positive” (objective) (2) plus, positive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet.', 'This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous.', 'Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006).', 'Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use.', 'in the sentence “This deal is a positive development for our company.” gives a strong indication that 1 All examples in this paper are from WordNet 2.0..', '1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9, Boulder, Colorado, June 2009.', 'Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%.', 'In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.', 'Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets.', 'The remainder of this paper is organized as follows.', 'Section 2 discusses previous work.', 'Section 3 describes our proposed semi-supervised minimum cut framework in detail.', 'Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.', 'There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level.', 'An up-to-date overview is given in Pang and Lee (2008).', 'Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do.', 'We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level.', 'At the word level Takamura et al.', '(2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign.', 'However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability.', 'is constructed using a variety of information such as gloss co-occurrences and WordNet links.', 'Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only.', 'However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance.', 'In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.', '(2005).', 'Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification.', 'Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet.', 'However, there is no evaluation as to the accuracy of their approach.', 'They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative).', 'Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations.', 'Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification.', 'Wiebe and Mi- halcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005).', 'One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification.', 'Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNet’s relation structure.', '3.1 Minimum Cuts: The Main Idea.', 'Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items should be grouped in the same cut.', 'All items in the training/test data are seen as vertices in a graph with undirected weighted edges between them specifying how strong the similarity/association between two vertices is. We use minimum s-t cuts: the graph contains two particular vertices s (source, corresponds to subjective) and t (sink, corresponds to objective) and each vertex u is connected to s and t via a weighted edge that can express how likely u is to be classified as s or t in isolation.', 'Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s ∈ S and t ∈ T . This corresponds to removing a set of edges from the graph.', 'As similar items should be in the same part of the split, the best split is one which removes edges with low weights.', 'In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices.', 'subjective or both objective.3 An example here is the antonym relation, where two antonyms such as good—morally admirable and evil, wicked—morally bad or wrong are both subjective.', 'Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001).', 'This is essential as the existing labeled datasets for our problem are small.', 'In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations.', 'Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.', 'Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components.', 'More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories).', '3.3 Formulation of Semi-supervised Mincuts.', 'W (S, T ) = ) u∈S,v∈T w(u, v)The formulation of our semi supervised Mincut for sense subjectivity classification involves the follow Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002).', '3.2 Why might Semi-supervised Minimum.', 'Cuts Work?', 'We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons.', 'First, our problem satisfies two major conditions necessary for using minimum cuts.', 'It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components.', 'Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data.', 'Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking similarity to other senses into account, for example via considering only the sense gloss.', 'In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both ing steps, which we later describe in more detail.', '1.', 'We define two vertices s (source) and t (sink),.', 'which correspond to the “subjective” and “objective” category, respectively.', 'Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices.', 'Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge.', 'The latter guarantees that the graph is connected.', '2.', 'For the test and unlabeled examples, we see.', 'the edges to the classification vertices as the probability of them being subjective/objective disregarding other example vertices.', 'We use a supervised classifier to set these edge weights.', 'For the labeled training examples, they are connected by edges with a high constant weight to the classification vertices that they belong to.', '3.', 'WordNet relations are used to construct the.', 'edges between two example vertices.', 'Such 3 See Kamps et al.', '(2004) for an early indication of such properties for some WordNet relations.', 'edges can exist between any pair of example vertices, for example between two unlabeled examples.', 'maximum-flow algorithm to find the minimum s-t cuts of the graph.', 'The cut in which the source vertex s lies is classified as “subjective”, and the cut in which the sink vertex t lies is “objective”.', 'to reflect the degree to which they are subjectivity- preserving.', 'Therefore, we experiment with two methods of weight assignment.', 'Method 1 (NoSL) assigns the same constant weight of 1.0 to all Word- Net relations.', 'Method 2 (SL) reflects different degrees of preserving subjectivity.', 'To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008).', 'This method 5 We now describe the above steps in more detail.', 'uses a list of subjective words (SL) to classify each Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A).', 'Thus a basic principle is that the selected unlabeled senses should be related to the training/test data by WordNet relations.', 'We therefore simply scan each sense in A, and collect all senses related to it via one of the WordNet relations in Table 1.', 'All such senses that are not in A are collected in the unlabeled data set.', 'Weighting of edges to the classification vertices: The edge weight to s and t represents how likely it is that an example vertex is initially put in the cut in which s (subjective) or t (objective) lies.', 'For unlabeled and test vertices, we use a supervised classifier (SVM4) with the labeled data as training data to assign the edge weights.', 'The SVM is also used as a baseline and its features are described in Section 4.3.', 'As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex.', 'Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge.', 'Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective.', 'However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie..', 'WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective.', 'We then count how often two senses related via a given relation have the same or a different subjectivity label.', 'The weight is computed by #same/(#same+#different).', 'Results are listed in Table 1.', 'Table 1: Relation weights (Method 2) M et ho d #S a m e #D iff er en t W ei gh t An to ny m 2, 80 8 30 9 0.', '90 Si milar to 6, 88 7 1, 61 4 0.', '81 De riv ed fro m 4, 63 0 94 7 0.', '83 Dir ect Hy pe rn y m 71 ,9 15 8, 60 0 0.', '89 Dir ect Hy po ny m 71 ,9 15 8, 60 0 0.', '89 Att rib ut e 35 0 10 9 0.', '76 Al so se e 1, 03 7 33 7 0.', '75 Ex ten ded An ton ym 6, 91 7 1, 65 1 0.', '81 Do m ai n 4, 38 7 89 2 0.', '83 Do m ain m e m be r 4, 38 7 89 2 0.', '83 Example graph: An example graph is shown in Figure 1.', 'The three example vertices correspond to the senses religious—extremely scrupulous and conscientious, scrupulous—having scruples; arising from a sense of right and wrong; principled; and flicker, spark, glint—a momentary flash of light respectively.', 'The vertex “scrupulous” is unlabeled data derived from the vertex “religious”(a test item) by the relation “similar-to”.', '4 Experiments and Evaluation.', '4.1 Datasets.', 'We conduct the experiments on two different gold standard datasets.', 'One is the MicroWNOp corpus, ntu.edu.tw/˜cjlin/libsvm/.', 'Linear kernel and probability estimates are used in this work.', 'http://www.cs.pitt.edu/mpq a subjective 0.24 0.83 religio us similar-to 0.81 scrupulo us 0.76 0.17 objective baseline.8 Three different feature types are used.', 'Lexic al Feature s (L): a bag-of words represen tation of the sense glosses with stop word filtering.', 'Relati on Feature s (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjectiv e or objective part of the training set, respectiv ely.', 'This provides a non graph 0.16 0.84 flicker Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6.', 'It includes 298 words with 703 objective and 358 subjective WordNet senses.', 'The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses.', 'As the MicroWNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4.', 'In Section 4.5, we shortly discuss results on.', 'Wiebe&Mihalcea’s dataset.', '4.2 Baseline and Evaluation.', 'We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalcea’s dataset respectively.', 'We use the McNemar test at the significance level of 5% for significance statements.', 'All evaluations are carried out by 10-fold cross-validation.', '4.3 Standard Supervised Learning.', 'We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable', 'summary of subjectivity-preserving links.', 'Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNet’s hypernym tree.', 'A typical example element of SubjSet is psychological feature —a feature of the mental life of a living organism, which indicates subjectivity for its hyponyms such as hope — the general feeling that some desire will be fulfilled.', 'A binary feature describes whether a noun/verb sense is a hyponym of an element of SubjSet.', 'Monosemous Feature (M): for each sense, we scan if a monosemous word is part of its synset.', 'If so, we further check if the monosemous word is collected in the subjective word list (SL).', 'The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective.', 'For example, the sense uncompromising, inflexible—not making concessions is subjective, as “uncompromising” is a monosemous word and also in SL.', 'We experiment with different combinations of features and the results are listed in Table 2, prefixed by “SVM”.', 'All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Mark- ert (2008).', 'However, improvements by adding more features remain small.', 'In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesk’s similarity measure from Pedersen’s WordNet similarity package9.', 'We use Lesk as it is one of the few measures applicable across all parts-of-speech.', 'markert/data.', 'This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008).', 'pubs/papers/goldstandard.total.acl06.', 'classification vertices in the Mincut approach.', '9 Available at http://www.d.umn.edu/˜tpederse/.', 'similarity.html.', 'Table 2: Results of SVM and Mincuts with different settings of feature M et ho d S u b j e c t i v e O b j e c t i v e Ac cu ra cy Pr eci sio n Re cal lF sc or e Pr eci sio n Re cal lF sc or e Ba sel in e N/ A 0 N/ A 66 .3 % 10 0 % 79 .7 % 66 .3 % S & M 66 .2 % 64 .5 % 65 .3 % 82 .2 % 83 .2 % 82 .7 % 76 .9 % Le sk 65 .6 % 50 .3 % 56 .9 % 77 .5 % 86 .6 % 81 .8 % 74 .4 % S VM L 69 .6 % 37 .7 % 48 .9 % 74 .3 % 91 .6 % 82 .0 % 73 .4 %L SL 82 .0 % 43 .3 % 56 .7 % 76 .7 % 95 .2 % 85 .0 % 77 .7 %L No SL 80 .8 % 43 .6 % 56 .6 % 76 .7 % 94 .7 % 84 .8 % 77 .5 % S VM L M 68 .9 % 42 .2 % 52 .3 % 75 .4 % 90 .3 % 82 .2 % 74 .1 % LM SL 83 .2 % 44 .4 % 57 .9 % 77 .1 % 95 .4 % 85 .3 % 78 .2 % LM No SL 83 .6 % 44 .1 % 57 .8 % 77 .1 % 95 .6 % 85 .3 % 78 .2 % S VM LR 68 .4 % 45 .3 % 54 .5 % 76 .2 % 89 .3 % 82 .3 % 74 .5 % LR SL 82 .7 % 65 .4 % 73 .0 % 84 .1 % 93 .0 % 88 .3 % 83 .7 % LR No SL 82 .4 % 65 .4 % 72 .9 % 84 .0 % 92 .9 % 88 .2 % 83 .6 % S VM LR M 69 .8 % 47 .2 % 56 .3 % 76 .9 % 89 .6 % 82 .8 % 75 .3 % LRM SL 85 .5 % 65 .6 % 74 .2 % 84 .4 % 94 .3 % 89 .1 % 84 .6 % LRM No SL 84 .6 % 65 .9 % 74 .1 % 84 .4 % 93 .9 % 88 .9 % 84 .4 % 1 L, R and M correspond to the lexical, relation and monosemous features respectively.', '2 SVM-L corresponds to using lexical features only for the SVM classifier.', 'Likewise, SVM-.', 'LRM corresponds to using a combination for lexical, relation, and monosemous features for the SVM classifier.', '3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations.', 'Likewise, LMNoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list).', '4.4 Semi-supervised Graph Mincuts.', 'Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in MicroWNOp as unlabeled data.', 'We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL).', 'The results are also summarized in Table 2.', 'Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights.', 'We can also see that we achieve good results without using any other knowledge sources (setting LRNoSL).', 'The example in Figure 1 explains why semi- supervised Mincuts outperforms the supervised approach.', 'The vertex “religious” is initially assigned the subjective/objective probabilities 0.24/0.76 by the SVM classifier, leading to a wrong classification.', 'However, in our graph-based Mincut framework, the vertex “religious” might link to other vertices (for example, it links to the vertex “scrupulous” in the unlabeled data by the relation “similar-to”).', 'The mincut algorithm will put vertices “religious” and “scrupulous” in the same cut (subjective category) as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of “flicker”).', 'In other words, the edges between the vertices are likely to correct some initially wrong classification and pull the vertices into the right cuts.', 'In the following we will analyze the best minimum cut algorithm LRMSL in more detail.', 'We measure its accuracy for each part-of-speech in the MicroWNOp dataset.', 'The number of noun, adjective, adverb and verb senses in MicroWNOp is 484, 265, 31 and 281, respectively.', 'The result is listed in Table 3.', 'The significantly better performance of semi-supervised mincuts holds across all parts-of- speech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm.', 'Mincuts SVM with different sizes of labeled and unlabeled data.', 'All learning curves are generated via averaging 10 learning curves from 10-fold cross-validation.', 'Performance with different sizes of labeled data: we randomly generate subsets of labeled data A1, A2...', 'An, and guarantee that A1 ⊂ A2... ⊂ An.Results for the best SVM (LRM) and the best min imum cut (LRMSL) are listed in Table 4, and the corresponding learning curve is shown in Figure 2.', 'As can be seen, the semi-supervised Mincuts is consistently better than SVM.', 'Moreover, the semi- supervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semi- supervised framework allows for a training data reduction of more than 80%.', 'Table 4: Accuracy with different sizes of labeled data 71 68 100 200 300 400 500 600 700 800 900 1000 Size of Labeled Data Figure 2: Learning curve with different sizes of labeled data The results are listed in Table 5 and Table 6 respectively.', 'The corresponding learning curves are shown in Figure 3.', 'We see that performance improves with the increase of unlabeled data.', 'In addition, the curves seem to converge when the size of unlabeled data is larger than 3,000.', 'From the results in Tabel 5 one can also see that hyponymy is the relation accounting for the largest increase.', 'Table 6: Accuracy with different sizes of unlabeled data (random selection) # unl ab ele d da ta Ac cu ra cy 0 75 .9 % 20 0 76 .5 % 50 0 78 .6 % 10 00 80 .2 % 20 00 82 .8 % 30 00 84 .0 % 32 20 84 .6 % Performance with different sizes of unlabeled data: We propose two different settings.', 'Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices).', 'For example, we first use {antonym, similar-to} only to obtain a unlabeled dataset U1, then use a larger subset of the relations like {antonym, similar-to, direct-hyponym, direct- hypernym} to generate another unlabeled dataset U2, and so forth.', 'Obviously, Ui is a subset of Ui+1.', 'Option2: Use all the ten relations to generate the unlabeled data U . We then randomly select subsets of U , such as subset U1, U2 and U3, and guarantee that U1 ⊂ U2 ⊂ U3 ⊂ . . .', 'U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%).', 'The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small MicroWNOp dataset.', 'In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1).', '4.5 Comparison to Prior Approaches.', 'In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Re lati on # unl ab ele d da ta Ac cu ra cy {∅ } 0 75 .3 % {si milar to } 41 8 79 .1 % {si milar to, ant on ym } 51 4 79 .5 % {si milarto, antonym, direct-hypernym, direct hy po ny m } 2, 72 1 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, also se e, ext en ded ant on ym } 3, 00 4 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, al so se e, ex te nd ed an to ny m, d eri ved fr o m , at tri bu te , d o m ai n, d o m ain m e m be r} 3, 22 0 84 .6 % 89 Option1 87 Option2.', '85 83 81 79 77 75 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled Data Figure 3: Learning curve with different sizes of unlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2).', 'Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2).', 'For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description).', 'Wiebe and Mihalcea (2006) report their results in precision and recall curves for subjective senses, such as a precision of about 55% at a recall of 50% for subjective senses.', 'Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve.', 'We run our best Mincut LRMSL algorithm with two different settings on Wiebe.', 'Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset.', 'At the recall of50% we achieve a precision of 83.6% (in compari son to their precision of 55% at the same recall).', 'Our F-score is 0.63 (vs. 0.52).', 'To check whether the high performance is just due to our larger training set, we also conduct 10-fold cross-validation on Wiebe.', 'The accuracy achieved is 81.1% and the F-score 0.56 (vs. 0.52), suggesting that our algorithm performs better.', 'Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2).', 'However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus.', '5 Conclusion and Future Work.', 'We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses.', 'The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut.', 'Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%).', 'To achieve the results of standard supervised approaches with our model, we need less than 20% of their training data.', 'In addition, we compare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets.', 'Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures.', 'We will also explore other semi- supervised algorithms.']\n",
      "N09-1001 invalid syntax (<unknown>, line 1)\n",
      "['Extracting semantic relationships between entities is challenging.', 'This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.', 'Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.', 'This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.', 'We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.', 'With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text.', 'Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.', 'According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).', 'The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference.', 'In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities.', 'Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).', 'Mentions have three levels: names, nomial expressions or pronouns.', 'The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.', 'For example, we want to determine whether a person is at a location, based on the evidence in the context.', 'Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query “Who is the president of the United States?”.', 'This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).', 'Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.', 'We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.', 'Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.', 'It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),.', 'explicit relations occur in text with explicit evidence suggesting the relationships.', 'Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document.', '427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427–434, Ann Arbor, June 2005.', 'Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.', 'The rest of this paper is organized as follows.', 'Section 2 presents related work.', 'Section 3 and Section 4 describe our approach and various features employed respectively.', 'Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.', 'The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.', 'Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.', 'Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.', 'Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.', 'Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.', 'It achieves 52.8 F- measure on the 24 ACE relation subtypes.', 'Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.', 'Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.', 'Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.', 'Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.', 'This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.', 'Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.', 'We also show how semantic information like WordNet and Name List can be equipped to further improve the performance.', 'Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.', 'It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.', 'Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).', 'Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set.', 'Basically, SVMs are binary classifiers.', 'Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task.', 'For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes.', 'The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output.', 'Moreover, we only apply the simple linear kernel, although other kernels can peform better.', 'The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.', 'In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).', '2 Joachims has just released a new version of SVMLight.', 'for multi-class classification.', 'However, this paper only uses the binary-class version.', 'For details about SVMLight, please see http://svmlight.joachims.org/', 'The semantic relation is determined between two mentions.', 'In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.', 'For each pair of mentions3, we compute various lexical, syntactic and semantic features.', '4.1 Words.', 'According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2.', 'For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important.', 'The words between the two mentions are classified into three bins: the first word in between, the last word in between and other words in between.', 'Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention.', 'Since a pronominal mention (especially neutral pronoun such as ‘it’ and ‘its’) contains little information about the sense of the mention, the co- reference chain is used to decide its sense.', 'This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: • WM1: bag-of-words in M1 • HM1: head word of M1 3 In ACE, each mention has a head annotation and an.', 'extent annotation.', 'In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation.', 'This has an effect of choosing the base phrase contained in the extent annotation.', 'In addition, this also can reduce noises without losing much of information in the mention.', 'For example, in the case where the noun phrase “the former CEO of McDonald” has the head annotation of “CEO” and the extent annotation of “the former CEO of McDonald”, we only consider “the former CEO” in this paper.', '4 In this paper, the head word of a mention is normally.', 'set as the last word of the mention.', 'However, when a preposition exists in the mention, its head word is set as the last word before the preposition.', 'For example, the head word of the name mention “University of Michigan” is “University”.', '• WM2: bag-of-words in M2 • HM2: head word of M2 • HM12: combination of HM1 and HM2 • WBNULL: when no word in between • WBFL: the only word in between when only one word in between • WBF: first word in between when at least two words in between • WBL: last word in between when at least two words in between • WBO: other words in between except first and last words when at least three words in between • BM1F: first word before M1 • BM1L: second word before M1 • AM2F: first word after M2 • AM2L: second word after M2 4.2 Entity Type.', 'This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE: • ET12: combination of mention entity types 4.3 Mention Level.', 'This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN: • ML12: combination of mention levels 4.4 Overlap.', 'This category of features includes: • #MB: number of other mentions in between • #WB: number of words in between • M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2.', 'Normally, the above overlap features are too general to be effective alone.', 'Therefore, they are HM12+M1>M2; 4) HM12+M1<M2.', '4.5 Base Phrase Chunking.', 'It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998).', 'The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees.', 'In this paper, we separate the features of base phrase chunking from those of full parsing.', 'In this way, we can separately evaluate the contributions of base phrase chunking and full parsing.', 'Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collins’ parser (Collins 1999) is employed for full parsing.', 'Most of the chunking features concern about the head words of the phrases between the two mentions.', 'Similar to word features, three categories of phrase heads are considered: 1) the phrase heads in between are also classified into three bins: the first phrase head in between, the last phrase head in between and other phrase heads in between; 2) the phrase heads before M1 are classified into two bins: the first phrase head before and the second phrase head before; 3) the phrase heads after M2 are classified into two bins: the first phrase head after and the second phrase head after.', 'Moreover, we also consider the phrase path in between.', '• CPHBNULL when no phrase in between • CPHBFL: the only phrase head when only one phrase in between • CPHBF: first phrase head in between when at least two phrases in between • CPHBL: last phrase head in between when at least two phrase heads in between • CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between • CPHBM1F: first phrase head before M1 • CPHBM1L: second phrase head before M1 • CPHAM2F: first phrase head after M2 • CPHAM2F: second phrase head after M2 • CPP: path of phrase labels connecting the two mentions in the chunking • CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree.', 'This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.', 'The dependency tree is built by using the phrase head information returned by the Collins’ parser and linking all the other fragments in a phrase to its head.', 'It also includes flags indicating whether the two mentions are in the same NP/PP/VP.', '• ET1DW1: combination of the entity type and the dependent word for M1 • H1DW1: combination of the head word and the dependent word for M1 • ET2DW2: combination of the entity type and the dependent word for M2 • H2DW2: combination of the head word and the dependent word for M2 • ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP • ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP • ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree.', 'This category of features concerns about the information inherent only in the full parse tree.', '• PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree • PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path.', '4.8 Semantic Resources.', 'Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.', 'Country Name List This is to differentiate the relation subtype “ROLE.Citizen-Of”, which defines the relationship between a person and the country of the person’s citizenship, from other subtypes, especially “ROLE.Residence”, where defines the relationship between a person and the location in which the person lives.', 'Two features are defined to include this information: • ET1Country: the entity type of M1 when M2 is a country name • CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal.', 'This trigger word list is first gathered from WordNet by checking whether a word has the semantic class “person|…|relative”.', 'Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes.', 'We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.', 'Two features are defined to include this information: • ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype.', '• SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype.', 'This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.', 'The ACE corpus is gathered from various newspapers, newswire and broadcasts.', 'In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.', '5.1 Experimental Setting.', 'We use the official ACE corpus from LDC.', 'The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations.', 'During development, 155 of 674 documents in the training set are set aside for fine-tuning the system.', 'The testing set is held out only for final evaluation.', 'It consists of 97 documents (~50k words) and 1386 instances of relations.', 'Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set.', 'It shows that the', '“GrandParent”, “Spouse” and “Sibling” are automatically set with the same classes without change.', 'However, The remaining words that do not have above four classes are manually classified.', 'ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.', 'It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes “Based-In”, “Located” and “Residence” under the type “AT”, which are difficult even for human experts to differentiate.', 'Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.', 'For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1.', 'Note that only 6 of these 24 relation subtypes are symmetric: “Relative- Location”, “Associate”, “Other-Relative”, “Other- Professional”, “Sibling”, and “Spouse”.', 'In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a “NONE” class for the case where the two mentions are not related.', '5.2 Experimental Results.', 'In this paper, we only measure the performance of relation extraction on “true” mentions with “true” chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.', 'Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set.', 'It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.', 'Table 2 also measures the contributions of different features by gradually increasing the feature set.', 'It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data • Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.', '• Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.', '• The usefulness of mention level features is quite limited.', 'It only improves the F-measure by 0.8 due to the recall increase.', '• Incorporating the overlap features gives some balance between precision and recall.', 'It increases the F-measure by 3.6 with a big precision decrease and a big recall increase.', '• Chunking features are very useful.', 'It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively.', '• To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.', 'This may be due to the fact that most of relations in the ACE corpus are quite local.', 'Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.', 'While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.', 'However, full parsing is always prone to long distance errors although the Collins’ parser used in our system represents the state-of-the-art in full parsing.', '• Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype “ROLE.Citizen-Of” from “ROLE.', 'Residence” by distinguishing country GPEs from other GPEs.', 'The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes.', 'Table 4 separately measures the performance of different relation types and major subtypes.', 'It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.', 'It is not surprising that the performance on the relation type “NEAR” is low because it occurs rarely in both the training and testing data.', 'Others like “PART.Subsidary” and “SOCIAL.', 'Other-Professional” also suffer from their low occurrences.', 'It also shows that our system performs best on the subtype “SOCIAL.Parent” and “ROLE.', 'Citizen-Of”.', 'This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.', 'Table 4 also indicates the low performance on the relation type “AT” although it frequently occurs in both the training and testing data.', 'This suggests the difficulty of detecting and classifying the relation type “AT” and its subtypes.', 'Table 5 separates the performance of relation detection from overall performance on the testing set.', 'It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.', 'It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.', 'It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall.', 'It also shows that feature-based methods dramatically outperform kernel methods.', 'This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.', 'The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.', 'Finally, Table 6 shows the distributions of errors.', 'It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.', 'This suggests that relation detection is critical for relation extraction.', '# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.', '1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.', '6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.', '4 5 4 . 8 5 3 . 5 Reside nce 6 6 1 9 9 67.', '9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88.', '9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.', '9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73.', '1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.', '4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37.', '8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84.', '4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.', '8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.', '1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.', '6 6 4 . 2 6 1 . 8 Memb er 2 2 4 1 0 4 3 6 74.', '3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74.', '1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33.', '3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4.', '8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3.', '1 4 9.', '5 55 .5 Ka mb hat la (20 04) :fe ature bas ed 6 3.', '5 4 5.', '2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1.', '2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.', 'Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion.', 'In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.', 'Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.', 'This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other.', 'While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.', 'Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins’ parser used in our system achieves the state-of-the-art performance.', 'Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.', 'Last, effective ways need to be explored to incorporate information embedded in the full Collins M.', '(1999).', 'Head-driven statistical models for natural language parsing.', 'Ph.D. Dissertation, University of Pennsylvania.', 'Collins M. and Duffy N.', '(2002).', 'Covolution kernels for natural language.', 'In Dietterich T.G., Becker S. and Ghahramani Z. editors.', 'Advances in Neural Information Processing Systems 14.', 'Cambridge, MA.', 'Culotta A. and Sorensen J.', '(2004).', 'Dependency tree th parse trees.', 'Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.', 'The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.', 'Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.', 'Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.', 'The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.', 'In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research.', 'Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.', 'Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X96-1048 not well-formed (invalid token): line 14, column 117\n",
      "['A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', 'The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.', 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', 'Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).', 'This paper, however, provides a comprehensive overview of the data collection effort and its current state.', 'At present, the ‘Potsdam Commentary Corpus’ (henceforth ‘PCC’ for short) consists of 170 commentaries from Ma¨rkische Allgemeine Zeitung, a German regional daily.', 'The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.', 'Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.', 'The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.', '(Again, the goal of also in structural features.', 'As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers Su¨ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence.', 'The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences.', 'For illustration, an English translation of one of the commentaries is given in Figure 1.', 'The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced.', 'Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future.', 'Section 4 draws some conclusions from the present state of the effort.', 'The corpus has been annotated with six different types of information, which are characterized in the following subsections.', 'Not all the layers have been produced for all the texts yet.', 'There is a ‘core corpus’ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.', 'All annotations are done with specific tools and in XML; each layer has its own DTD.', 'This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.', 'We will briefly discuss this point in Section 3.1.', '2.1 Part-of-speech tags.', 'All commentaries have been tagged with part-of-speech information using Brants’ TnT1 tagger and the Stuttgart/Tu¨bingen Tag Set automatic analysis was responsible for this decision.)', 'This is manifest in the lexical choices but 1 www.coli.unisb.de/∼thorsten/tnt/ Dagmar Ziegler is up to her neck in debt.', 'Due to the dramatic fiscal situation in Brandenburg she now surprisingly withdrew legislation drafted more than a year ago, and suggested to decide on it not before 2003.', 'Unexpectedly, because the ministries of treasury and education both had prepared the teacher plan together.', 'This withdrawal by the treasury secretary is understandable, though.', 'It is difficult to motivate these days why one ministry should be exempt from cutbacks — at the expense of the others.', 'Reiche’s colleagues will make sure that the concept is waterproof.', 'Indeed there are several open issues.', 'For one thing, it is not clear who is to receive settlements or what should happen in case not enough teachers accept the offer of early retirement.', 'Nonetheless there is no alternative to Reiche’s plan.', 'The state in future has not enough work for its many teachers.', 'And time is short.', 'The significant drop in number of pupils will begin in the fall of 2003.', 'The government has to make a decision, and do it quickly.', 'Either save money at any cost - or give priority to education.', 'Figure 1: Translation of PCC sample commentary (STTS)2.', '2.2 Syntactic structure.', 'Annotation of syntactic structure for the core corpus has just begun.', 'We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.', '2.3 Rhetorical structure.', 'All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).', 'Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised.', 'Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.', 'Thus we opted not to take the step of creating more precise written annotation guidelines (as (Carlson, Marcu 2001) did for English), which would then allow for measuring inter-annotator agreement.', 'The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage.', 'One conclusion drawn from this annotation effort was that for humans and machines alike, 2 www.sfs.nphil.unituebingen.de/Elwis/stts/ stts.html 3 www.coli.unisb.de/sfb378/negra-corpus/annotate.', 'html 4 www.wagsoft.com/RSTTool assigning rhetorical relations is a process loaded with ambiguity and, possibly, subjectivity.', 'We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure.', '2.4 Underspecified rhetorical structure.', 'While RST (Mann, Thompson 1988) proposed that a single relation hold between adjacent text segments, SDRT (Asher, Lascarides 2003) maintains that multiple relations may hold simultaneously.', 'Within the RST “user community” there has also been discussion whether two levels of discourse structure should not be systematically distinguished (intentional versus informational).', 'Some relations are signalled by subordinating conjunctions, which clearly demarcate the range of the text spans related (matrix clause, embedded clause).', 'When the signal is a coordinating conjunction, the second span is usually the clause following the conjunction; the first span is often the clause preceding it, but sometimes stretches further back.', 'When the connective is an adverbial, there is much less clarity as to the range of the spans.', 'Assigning rhetorical relations thus poses questions that can often be answered only subjectively.', 'Our annotators pointed out that very often they made almost random decisions as to what relation to choose, and where to locate the boundary of a span.', '(Carlson, Marcu 2001) responded to this situation with relatively precise (and therefore long!)', 'annotation guidelines that tell annotators what to do in case of doubt.', 'Quite often, though, these directives fulfill the goal of increasing annotator agreement without in fact settling the theoretical question; i.e., the directives are clear but not always very well motivated.', 'In (Reitter, Stede 2003) we went a different way and suggested URML5, an XML format for underspecifying rhetorical structure: a number of relations can be assigned instead of a single one, competing analyses can be represented with shared forests.', 'The rhetorical structure annotations of PCC have all been converted to URML.', 'There are still some open issues to be resolved with the format, but it represents a first step.', 'What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly.', '2.5 Connectives with scopes.', 'For the ‘core’ portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright — but see Sections 3.2 and 3.3 below.', 'Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.', 'We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.', 'This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.', 'For effectively annotating connectives/scopes, we found that existing annotation tools were not well-suited, for two reasons: • Some tools are dedicated to modes of annotation (e.g., tiers), which could only quite un-intuitively be used for connectives and scopes.', '• Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them.', '5 ‘Underspecified Rhetorical Markup Language’ 6 This confirms the figure given by (Schauer, Hahn.', 'Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.', 'It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.', 'The annotator can then “click away” those words that are here not used as connectives (such as the conjunction und (‘and’) used in lists, or many adverbials that are ambiguous between connective and discourse particle).', 'Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like ‘for sub- junctor, mark all words up to the next comma as the first segment’), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.', 'When finished, the whole material is written into an XML-structured annotation file.', '2.6 Co-reference.', 'We developed a first version of annotation guidelines for co-reference in PCC (Gross 2003), which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet.', 'The tool we use is MMAX8, which has been specifically designed for marking co-reference.', 'Upon identifying an anaphoric expression (currently restricted to: pronouns, prepositional adverbs, definite noun phrases), the an- notator first marks the antecedent expression (currently restricted to: various kinds of noun phrases, prepositional phrases, verb phrases, sentences) and then establishes the link between the two.', 'Links can be of two different kinds: anaphoric or bridging (definite noun phrases picking up an antecedent via world-knowledge).', '• Anaphoric links: the annotator is asked to specify whether the anaphor is a repetition, partial repetition, pronoun, epithet (e.g., Andy Warhol – the PopArt artist), or is-a (e.g., Andy Warhol was often hunted by photographers.', 'This fact annoyed especially his dog...).', '• Bridging links: the annotator is asked to specify the type as part-whole, cause-effect (e.g., She had an accident.', 'The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.', '7 www.cis.upenn.edu/∼pdtb/ 8 www.eml-research.de/english/Research/NLP/ Downloads had to buy a new car.', 'The price shocked her.), or same-kind (e.g., Her health insurance paid for the hospital fees, but the automobile insurance did not cover the repair.).', '3.1 Retrieval.', 'For displaying and querying the annoated text, we make use of the Annis Linguistic Database developed in our group for a large research effort (‘Sonderforschungsbereich’) revolving around 9 2.7 Information structure.', 'information structure.', 'The implementation is In a similar effort, (G¨otze 2003) developed a proposal for the theory-neutral annotation of information structure (IS) — a notoriously difficult area with plenty of conflicting and overlapping terminological conceptions.', 'And indeed, converging on annotation guidelines is even more difficult than it is with co-reference.', 'Like in the co-reference annotation, G¨otze’s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet.', 'We use MMAX for this annotation as well.', 'Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such.', 'The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning.', 'Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).', 'They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter).', 'Finally, the focus/background partition is annotated, together with the focus question that elicits the corresponding answer.', 'Asking the annotator to also formulate the question is a way of arriving at more reproducible decisions.', 'For all these annotation taks, G¨otze developed a series of questions (essentially a decision tree) designed to lead the annotator to the ap propriate judgement.', 'Having explained the various layers of annotation in PCC, we now turn to the question what all this might be good for.', 'This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1).', 'On the other hand, we are interested in the application of rhetorical analysis or ‘discourse parsing’ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).', 'basically complete, yet some improvements and extensions are still under way.', 'The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window).', 'Figure 2 shows a screenshot (which is of somewhat limited value, though, as color plays a major role in signalling the different statuses of the information).', 'In the small window on the left, search queries can be entered, here one for an NP that has been annotated on the co-reference layer as bridging.', 'The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) • the full text, • the annotation values for the activated annotation set (co-reference), • the actual annotation tiers, and • the portion of text currently ‘in focus’ (which also appears underlined in the full text).', 'Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels.', 'Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.', '3.2 Stochastic rhetorical analysis.', 'In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.', 'Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method.', 'For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.', 'Future work along these lines will incorporate other layers of annotation, in particular the syntax information.', '9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.', 'rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.', 'The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations.', 'In the rhetorical tree, nuclearity information is then used to extract a “kernel tree” that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).', 'Thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity.', 'In order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information — as in PCC.', 'That is, we can use the discourse parser on PCC texts, emulating for instance a “co-reference oracle” that adds the information from our co-reference annotations.', 'The knowledge base then can be tested for its relation-inference capabilities on the basis of full-blown co-reference information.', 'Conversely, we can use the full rhetorical tree from the annotations and tune the co-reference module.', 'The general idea for the knowledge- based part is to have the system use as much information as it can find at its disposal to produce a target representation as specific as possible and as underspecified as necessary.', 'For developing these mechanisms, the possibility to feed in hand-annotated information is very useful.', '3.4 Salience-based text generation.', 'Text generation, or at least the two phases of text planning and sentence planning, is a process driven partly by well-motivated choices (e.g., use this lexeme X rather than that more colloquial near-synonym Y ) and partly by con tation like that of PCC can be exploited to look for correlations in particular between syntactic structure, choice of referring expressions, and sentence-internal information structure.', 'A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports).', 'And then there are decisions that systems typically hard-wire, because the linguistic motivation for making them is not well understood yet.', 'Preferences for constituent order (especially in languages with relatively free word order) often belong to this group.', 'Trying to integrate constituent ordering and choice of referring expressions, (Chiarcos 2003) developed a numerical model of salience propagation that captures various factors of author’s intentions and of information structure for ordering sentences as well as smaller constituents, and picking appropriate referring expressions.10 Chiarcos used the PCC annotations of co-reference and information structure to compute his numerical models for salience projection across the generated texts.', '3.5 Improved models of discourse.', 'structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure.', 'One key issue here is to seek a discourse-based model of information structure.', 'Since Daneˇs’ proposals of ‘thematic development patterns’, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.', '(Hartmann 1984), for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).', '(Brandt 1996) extended these ideas toward a conception of kommunikative Gewichtung (‘communicative-weight assignment’).', 'A different notion of information structure, is used in work such as that of (?), who tried to characterize felicitous constituent ordering (theme choice, in particular) that leads to texts presenting information in a natural, “flowing” way rather than with abrupt shifts of attention.', '—ested in correlations between prosody and dis course structure.', 'A number of PCC commentaries will be read by professional news speakers and prosodic features be annotated, so that the various annotation layers can be set into correspondence with intonation patterns.', 'In focus is in particular the correlation with rhetorical structure, i.e., the question whether specific rhetorical relations — or groups of relations in particular configurations — are signalled by speakers with prosodic means.', 'Besides information structure, the second main goal is to enhance current models of rhetorical structure.', 'As already pointed out in Section 2.4, current theories diverge not only on the number and definition of relations but also on apects of structure, i.e., whether a tree is sufficient as a representational device or general graphs are required (and if so, whether any restrictions can be placed on these graph’s structures — cf.', '(Webber et al., 2003)).', 'Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at one’s disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported.', 'The PCC is not the result of a funded project.', 'Instead, the designs of the various annotation layers and the actual annotation work are results of a series of diploma theses, of students’ work in course projects, and to some extent of paid assistentships.', 'This means that the PCC cannot grow particularly quickly.', 'After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth.', 'Hence we decided to select ten commentaries to form a ‘core corpus’, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence.', 'Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).', '11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here.', 'rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.', 'The kind of annotation work presented here would clearly benefit from the emergence of standard formats and tag sets, which could lead to sharable resources of larger size.', 'Clearly this poses a number of research challenges, though, such as the applicability of tag sets across different languages.', 'Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.']\n",
      "['Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems.', 'However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena.', 'We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output.', 'Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.', 'Most natural languages construct words by concatenating morphemes together in strict orders.', 'Such “concatenative morphotactics” can be impressively productive, especially in agglutinative languages like Aymara (Figure 11) or Turkish, and in agglutinative/polysynthetic languages like Inuktitut (Figure 2)(Mallon, 1999, 2).', 'In such languages a single word may contain as many morphemes as an average-length English sentence.', 'Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example.', 'Finnish, Turkish and Hungarian.', 'However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes: “Only restricted infixation and reduplication can be handled adequately with the present system.', 'Some extensions or revisions will be necessary for an adequate description of languages possessing extensive infixation or reduplication” (Koskenniemi, 1983, 27).', 'This limitation has of course not escaped the notice of various reviewers, e.g. Sproat(1992).', 'We shall argue that the morphotactic limitations of the traditional implementations are the direct result of relying solely on the concatenation operation in morphotactic description.', 'We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description.', 'Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network.', 'This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below.', 'Before illustrating these applications, we will first outline our general approach to finite-state morphology.', 'Lexical: uta+ma+naka+p+xa+samacha-i+wa Surface: uta ma n ka p xa samach i wa uta = house (root) +ma = 2nd person possessive +na = in -ka = (locative, verbalizer) +p = plural +xa = perfect aspect +samacha = \"apparently\" -i = 3rd person +wa = topic marker Figure 1: Aymara: utamankapxasamachiwa = ”it appears that they are in your house” Lexical: Paris+mut+nngau+juma+niraq+lauq+sima+nngit+junga Surface: Pari mu nngau juma nira lauq sima nngit tunga Paris = (root = Paris) +mut = terminalis case ending +nngau = go (verbalizer) +juma = want +niraq = declare (that) +lauq = past +sima = (added to -lauq- indicates \"distant past\") +nngit = negative +junga = 1st person sing.', 'present indic (nonspecific) Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = “I never said I wanted to go to Paris” 2.1 Analysis and Generation.', 'In the most theory- and implementation-neutral form, morphological analysis and generation of written words can be modeled as a relation between the words themselves and analyses of those words.', 'Computationally, as shown in Figure 3, a black-box module maps from words to analyses to effect Analysis, and from analyses to words to effect Generation.', 'ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages.', 'The surface language consists of strings (= words = sequences of symbols) written according to some defined orthography.', 'In a commercial application for a natural language, the surface language to be modeled is usually a given, e.g. the set of valid French words as written according to standard French orthography.', 'The lexical language again consists of strings, but strings designed according to the needs and taste of the linguist, representing analyses of the surface words.', 'It is sometimes convenient to design these lexical strings to show all the constituent morphemes in their morphophonemic form, separated and identified as in Figures 1 and 2.', 'In other applications, it may be useful to design the lexical strings to contain the traditional dictionary citation form, together with linguist-selected “tag” sym Analysis Strings Regular Expression Compiler F S T Word Strings Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages bols like +Noun, +Verb, +SG, +PL, that convey category, person, number, tense, mood, case, etc. Thus the lexical string representing paie, the first-person singular, present indicative form of the French verb payer (“to pay”), might be spelled payer+IndP+SG+P1+Verb.', 'The tag symbols are stored and manipulated just like alphabetic symbols, but they have multicharacter print names.', 'If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally.', 'Following convention, we will often refer to the upper projection of the fst, representing analyses, as the lexical language, a set of lexical strings; and we will refer to the lower projection as the surface language, consisting of surface strings.', 'There are compelling advantages to computing with such finite-state machines, including mathematical elegance, flexibility, and for most natural-language applications, high efficiency and data-compaction.', 'One computes with fsts by applying them, in either direction, to an input string.', 'When one such fst that was written for French is applied in an upward direction to the surface word maisons (“houses”), it returns the related string maison+Fem+PL+Noun, consisting of the citation form and tag symbols chosen by a linguist to convey that the surface form is a feminine noun in the plural form.', 'A single surface string can be related to multiple lexical strings, e.g. applying this fst in an upward direction to surface string suis produces the four related lexical strings shown in Figure 5.', 'Such ambiguity of surface strings is very common.', '^etre+IndP+SG+P1+Verb suivre+IndP+SG+P2+Verb suivre+IndP+SG+P1+Verb suivre+Imp+SG+P2+Verb Figure 5: Multiple Analyses for suis Conversely, the very same fst can be applied in a downward direction to a lexical string like ^etre+IndP+SG+P1+Verb to return the related surface string suis ; such transducers are inherently bidirectional.', 'Ambiguity in the downward direction is also possible, as in the relation of the lexical string payer+IndP+SG+P1+Verb (“I pay”) to the surface strings paie and paye, which are in fact valid alternate spellings in standard French orthography.', '2.2 Morphotactics and Alternations.', 'There are two challenges in modeling natural language morphology: • Morphotactics • Phonological/Orthographical Alternations Finite-state morphology models both using regular expressions.', 'The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks.', 'In practice, the most commonly separated modules are a lexicon fst, containing lexical strings, and a separately written set of Lexicon Regular Expression Rule Regular Expression Compiler Lexicon FST .o. Rule FST Lexical Transducer (a single FST) Figure 6: Creation of a Lexical Transducer rule fsts that map from the strings in the lexicon to properly spelled surface strings.', 'The lexicon description defines the morphotactics of the language, and the rules define the alternations.', 'The separately compiled lexicon and rule fsts can subsequently be composed together as in Figure 6 to form a single “lexical transducer” (Karttunen et al., 1992) that could have been defined equivalently, but perhaps less perspicuously and less efficiently, with a single regular expression.', 'In the lexical transducers built at Xerox, the strings on the lower side of the transducer are inflected surface forms of the language.', 'The strings on upper side of the transducer contain the citation forms of each morpheme and any number of tag symbols that indicate the inflections and derivations of the corresponding surface form.', 'For example, the information that the comparative of the adjective big is bigger might be represented in the English lexical transducer by the path (= sequence of states and arcs) in Figure 7 where the zeros represent epsilon symbols.2 The gemination of g and Lexical side: For the sake of clarity, Figure 7 represents the upper (= lexical) and the lower (= surface) side of the arc label separately on the opposite sides of the arc. In the remaining diagrams, we use a more compact notation: the upper and the lower symbol are combined into a single label of the form upper:lower if the symbols are distinct.', 'A single symbol is used for an identity pair.', 'In the standard notation, the path in Figure 7 is labeled as b i g 0:g +Adj:0 0:e +Comp:r. Lexical transducers are more efficient for analysis and generation than the classical two- level systems (Koskenniemi, 1983) because the morphotactics and the morphological alternations have been precompiled and need not be consulted at runtime.', 'But it would be possible in principle, and perhaps advantageous for some purposes, to view the regular expressions defining the morphology of a language as an un- compiled “virtual network”.', 'All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime.', 'Most languages build words by simply stringing morphemes (prefixes, roots and suffixes) b i g b i g 0 +Adj g 0 0 +Comp e r together in strict orders.', 'The morphotactic (word building) processes of prefixation and suffixation can be straightforwardly Surface side: Figure 7: A Path in a Transducer for English the epenthetical e in the surface form bigger result from the composition of the original lexicon fst with the rule fst representing the regular morphological alternations in English.', '2 The epsilon symbols and their placement in the string are not significant.', 'We will ignore them whenever it is convenient.', 'modeled in finite state terms as concatenation.', 'But some natural languages also exhibit non-concatenative morphotactics.', 'Some times the languages themselves are called “non- concatenative languages”, but most employ significant concatenation as well, so the term “not completely concatenative” (Lavie et al., 1988) is usually more appropriate.', 'In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication.', 'Although Arabic and Malay also include prefixation and suffixation that are modeled straightforwardly by concatenation, a complete lexicon cannot be a a:0 *:a *:0 *:0 0:a obtained without non-concatenative processes.', 'We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.', 'The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions.', 'The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation.', 'To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network.', 'Until an opening delimiter ^[ is encountered, the algorithm constructs a copy of the path it is following.', 'If the network contains no regular-expression substrings, the result will be a copy of the original network.', 'When a ^[ is encountered, the algorithm looks for a closing ^] and extracts the path between the delimiters to be handled in a special way: 1.', 'The symbols along the indicated side of the.', 'path are concatenated into a string and eliminated from the path leaving just the symbols on the opposite side.', '2.', 'A separate network is created that contains the modified path.', '3.', 'The extracted string is compiled into a. second network with the standard regular- expression compiler.', 'gle one using the crossproduct operation.', 'resenting the origin and the destination of 0:^[ a * 0:^] the regular-expression path.', 'Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation.', 'The network created by the operation is shown in Figure 9.', 'When applied in the “upward” direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled.', 'The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine.', 'It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10.', 'a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced.', 'In the linguistic applications presented Lexical: b a g i +Noun +Plural Surface: ^[ { b a g i } ^ 2 ^] Lexical: p e l a b u h a n +Noun +Plural Surface: ^[ { p e l a b u h a n } ^ 2 ^] Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation in the next sections, the two sides of a regular- expression path contain different strings.', 'The upper side contains morphological information; the regular-expression operators appear only on the lower side and are not present in the final result.', '3.1 Reduplication.', 'Traditional Two-Level implementations are already capable of describing some limited reduplication and infixation as in Tagalog (Antworth, 1990, 156–162).', 'The more challenging phenomenon is variable-length redupli- cation, as found in Malay and the closely related Indonesian language.', 'An example of variable-length full-stem reduplication occurs with the Malay stem bagi, which means “bag” or “suitcase”; this form is in fact number-neutral and can translate as the plural.', 'Its overt plural is phonologically bagibagi,3 formed by repeating the stem twice in a row.', 'Although this pluralization process may appear concatenative, it does not involve concatenating a predictable pluralizing morpheme, but rather copying the preceding stem, whatever it may be and however long it may be.', 'Thus the overt plural of pelabuhan (“port”), itself a derived form, is phonologically pelabuhanpelabuhan.', 'Productive reduplication cannot be described by finite-state or even context-free formalisms.', 'It is well known that the copy language, {ww | w ǫ L}, where each word contains two copies of the same string, is a context-sensitive language.', 'However, if the “base” language L is finite, we can construct a finite-state network that encodes L and the reduplications of all the strings in L. On the assumption that there are only a finite number of words subject to reduplication (no free compounding), it is possible to construct a lexical transducer for languages 3 In the standard orthography, such reduplicated words are written with a hyphen, e.g. bagibagi, that we will ignore for this example.', 'such as Malay.', 'We will show a simple and elegant way to do this with strictly finite-state operations.', 'To understand the general solution to full- stem reduplication using the compile-replace algorithm requires a bit of background.', 'In the regular expression calculus there are several operators that involve concatenation.', 'For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself.', 'There are also operators that express a fixed number of concatenations.', 'In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings.', 'The reduplication of any string w can then be notated as {w}^2, and we start by defining a network where the lower-side strings are built by simple concatenation of a prefix ^[, a root enclosed in braces, and an overt-plural suffix ^2 followed by the closing ^].', 'Figure 11 shows the paths for two Malay plurals in the initial network.', 'The compile-replace algorithm, applied to the lower-side of this network, recognizes each individual delimited regular-expression substring like ^[{bagi}^2^], compiles it, and replaces it with the result of the compilation, here bagibagi.', 'The same process applies to the entire lower-side language, resulting in a network that relates pairs of strings such as the ones in Figure 12.', 'This provides the desired solution, still finite-state, for analyzing and generating full- stem reduplication in Malay.4 4 It is well-known (McCarthy and Prince, 1995) that reduplication can be a more complex phenomenon than it is in Malay.', 'In some languages only a part of the stem is reduplicated and there may be systematic differences between the reduplicate and the base form.', 'We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired.', 'There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix).', 'The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output.', '3.2 Semitic Stem Interdigitation.', '3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation.', 'An example of interdigitation occurs with the Arabic stem katab, which means “wrote”.', 'According to an influential autosegmental analysis (McCarthy, 1981), this stem consists of an all-consonant root ktb whose general meaning has to do with writing, an abstract consonant-vowel template CVCVC, and a voweling or vocalization that he symbolized simply as a, signifying perfect aspect and active voice.', 'The root consonants are associated with the C slots of the template and the vowel or vowels with the V slots, producing a complete stem katab.', 'If the root and the vocalization are thought of as morphemes, neither morpheme occurs continuously in the stem.', 'The same root ktb can combine with the template CVCVC and a different vocalization ui, signifying perfect aspect and passive voice, producing the stem kutib, which means “was written”.', 'Similarly, the root ktb can combine with template CVVCVC and ui to produce kuutib, the root drs can combine with CVCVC and ui to form duris, and so forth.', 'tiers of McCarthy (1981) as projections of a multi-level transducer and wrote a small Prolog- based prototype that handled the interdigitation of roots, CV-templates and vocalizations into abstract Arabic stems; this general approach, with multi-tape transducers, has been explored and extended by Kiraz in several papers (1994a; 1996; 1994b; 2000) with respect to Syriac and Arabic.', 'The implementation is described in Kiraz and GrimleyEvans (1999).', 'In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages.', 'Thus Akkadian words were formalized as consisting of morphemes, some of which were combined together by intersection and others of which were combined via concatenation.', 'This was the key insight: morphotactic description could employ various finite-state operations, not just concatenation; and languages that required only concatenation were just special cases.', 'By extension, the widely noticed limitations of early finite-state implementations in dealing with non-concatenative morphotactics could be traced to their dependence on the concatenation operation in morphotactic descriptions.', 'This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime.', 'Kay (1987) reformalized the autosegmental 5 These patterns combine what McCarthy (1981).', 'issue here due to lack of space.', 'would call templates and vocalizations.', 'The 1996 algorithm that intersected roots and patterns into stems, and substituted the original roots and patterns on just the lower side with the intersected stem, was admittedly rather ad hoc and computationally intensive, taking over two hours to handle about 90,000 stems on a SUN Ultra workstation.', 'The compile-replace algorithm is a vast improvement in both generality and efficiency, producing the same result in a few minutes.', 'Following the lines of Kataja and Koskenniemi (1988), we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)).', 'However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge.', '3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one.', 'The strings of the filler language consist of ordinary symbols such as d, r, s, u, i. The template expressions may contain special class symbols such as C (= consonant) or V (= vowel) that represent a predefined set of ordinary symbols.', 'The objective of the merge operation is to align the template strings with the filler strings and to instantiate the class symbols of the template as the matching filler symbols.', 'Like intersection, the merge algorithm operates by following two paths, one in the template network, the other in the filler network, and it constructs the corresponding single path in the result network.', 'Every state in the result corresponds to two original states, one in template, the other in the filler.', 'If the original states are both final, the resulting state is also final; otherwise it is non-final.', 'In other words, in order to construct a successful path, the algorithm must reach a final state in both of the original networks.', 'If the new path terminates in a non-final state, it represents a failure and will eventually be pruned out.', 'The operation starts in the initial state of the original networks.', 'At each point, the algorithm tries to find all the successful matches between the template arcs and filler arcs.', 'A match is successful if the filler arc symbol is included in the class designated by the template arc symbol.', 'The main difference between merge and classical intersection is in Conditions 1 and 2 below: 1.', 'If a successful match is found, a new arc is. added to the current result state.', 'The arc is labeled with the filler arc symbol; its destination is the result state that corresponds to the two original destinations.', '2.', 'If no successful match is found for a given.', 'template arc, the arc is copied into the current result state.', 'Its destination is the result state that corresponds to the destination of the template arc and the current filler state.', 'In effect, Condition 2 preserves any template arc that does not find a match.', 'In that case, the path in the template network advances to a new state while the path in the filler network stays at the current state.', 'We use the networks in Figure 13 to illustrate the effect of the merge algorithm.', 'Figure 13 shows a linear template network and two filler networks, one of which is cyclic.', 'C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14.', 'The three symbols of the filler string are instantiated in the three consonant slots in the CVVCVC template.', 'd V V r V s Figure 14: Intermediate Result.', 'Figure 15 presents the final result in which the second filler network in Figure 13 is merged with the intermediate result shown in Figure 14.', 'Lexical: k t b =Root C V C V C =Template a + =Voc Surface: ^[ k t b .m>.', 'C V C V C .<m. a + ^] Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: ^[ k t b .m>.', 'C V C V C .<m. u * i ^] Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: ^[ d r s .m>.', 'C V V C V C .<m. u * i ^] Figure 16: Initial paths d u u r i s Figure 15: Final Result In this case, the filler language contains an infinite set of strings, but only one successful path can be constructed.', 'Because the filler string ends with a single i, the first two V symbols can be instantiated only as u. Note that ordinary symbols in the partially filled template are treated like the class symbols that do not find a match.', 'That is, they are copied into the result in their current position without consuming a filler symbol.', 'To introduce the merge operation into the Xerox regular expression calculus we need to choose an operator symbol.', 'Because merge, like subtraction, is a non-commutative operation, we also must distinguish between the template and the filler.', 'For example, we could choose .m. as the operator and decide by convention which of the two operands plays which role in expressions such as [A .m. B].', 'What we actually have done, perhaps without a sufficiently good motivation, is to introduce two variants of the merge operator, .<m. and .m>., that differ only with respect to whether the template is to the left (.<m.) or to the right (.m>.)', 'of the filler.', 'The expression [A .<m. B] represents the same merge operation as [B .m>.', 'A].', 'In both cases, A denotes the template, B denotes the filler, and the result is the same.', 'With these new operators, the network in Figure 15 can be compiled from an expression such as d r s .m>.', 'C V V C V C .<m. u* i As we have defined them, .<m. and .m>.', 'are weakly binding left-associative operators.', 'In this example, the first merge instantiates the filler consonants, the second operation fills the vowel slots.', 'However, the order in which the merge operations are performed is irrelevant in this case because the two filler languages do not provide competing instantiations for the same class symbols.', '3.2.3 Merging Roots and Vocalizations with Templates Following the tradition, we can represent the lexical forms of Arabic stems as consisting of three components, a consonantal root, a CV template and a vocalization, possibly preceded and followed by additional affixes.', 'In contrast to McCarthy, Kay, and Kiraz, we combine the three components into a single projection.', 'In a sense, McCarthy’s three tiers are conflated into a single one with three distinct parts.', 'In our opinion, there is no substantive difference from a computational point of view.', 'For example, the initial lexical representation of the surface forms katab, kutib, and duuris, may be represented as a concatenation of the three components shown in Figure 16.', 'We use the symbols =Root, =Template, and =Voc to designate the three components of the lexical form.', 'The corresponding initial surface form is a regular-expression substring, containing two merge operators, that will be compiled and replaced by the interdigitated surface form.', 'The application of the compile-replace operation to the lower side of the initial lexicon yields a transducer that maps the Arabic interdigitated forms directly into their corresponding tripartite analyses and vice versa, as illustrated in Figure 17.', 'Alternation rules are subsequently composed on the lower side of the result to map the in- terdigitated, but still morphophonemic, strings into real surface strings.', 'Although many Arabic templates are widely considered to be pure CV-patterns, it has been argued that certain templates also contain Lexical: k t b =Root C V C V C =Template a + =Voc Surface: k a t a b Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: k u t i b Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: d u u r i s Figure 17: After Applying Compile-Replace to the Lower Side “hard-wired” specific vowels and consonants.6 For example, the so-called “FormVIII” template is considered, by some linguists, to contain an embedded t: CtVCVC.', 'The presence of ordinary symbols in the template does not pose any problem for the analysis adopted here.', 'As we already mentioned in discussing the intermediate representation in Figure 14, the merge operation treats ordinary symbols in a partially filled template in the same manner as it treats unmatched class symbols.', 'The merge of a root such as ktb with the presumed FormVIII template and the a+ vocal- ism, k t b .m>.', 'C t V C V C .<m. a+ produces the desired result, ktatab, without any additional mechanism.', '4 Status of the Implementations.', '4.1 Malay Morphological.', 'Analyzer/Generator Malay and Indonesian are closely-related languages characterized by rich derivation and little or nothing that could be called inflection.', 'The Malay morphological analyzer prototype, written using lexc, Replace Rules, and compile-replace, implements approximately 50 different derivational processes, including pre- fixation, suffixation, prefix-suffix pairs (circum- fixation), reduplication, some infixation, and combinations of these processes.', 'Each root is marked manually in the source dictionary to indicate the idiosyncratic subset of derivational processes that it undergoes.', 'The small prototype dictionary, stored in an XML format, contains approximately 1000 roots, with about 1500 derivational subentries (i.e. an average of 1.5 derivational processes per root).', 'At compile time, the XML dictionary is parsed and “downtranslated” into the source format required for the lexc compiler.', 'The XML dictionary could be expanded by any competent Malay lexicographer.', '4.2 Arabic Morphological.', 'Analyzer/Generator The current Arabic system has been described in some detail in previous publications (Beesley, 1996; Beesley, 1998a; Beesley, 1998b) and is available for testing on the Internet.7 The modification of the system to use the compile-replace algorithm has not changed the size or the behavior of the system in any way, but it has reduced the compilation time from hours to minutes.', '5 Conclusion.', 'The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description.', 'The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation.', 'Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation.', 'Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics.', 'The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific', 'versial issue.', '7 http://www.x rce.xerox.com /research/mlt t/arabic/ morphotactic problems we have discussed.', 'We expect that they will have many other useful applications.', 'One illustration is given in the Appendix.', '6 Appendix: Palindrome Extraction.', 'To demonstrate the power of the compile- replace method, let us show how it can be applied to solve another “hard” problem: identifying and extracting all the palindromes from a lexicon.', 'Like reduplication, palindrome identification appears at first to require more powerful tools than a finite-state calculus.', 'But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique.', 'Let us assume that L is a simple network constructed from an English wordlist.', 'We start by extracting from L all the words with a property that is necessary but not sufficient for being a palindrome, namely, the words whose inverse is also an English word.', 'This step can be accomplished by redefining L as [L & L.r] where & represents intersection and .r is the reverse operator.', 'The resulting network contains palindromes such as madam as well non-palindromes such as dog and god.', 'The remaining task is to eliminate all the words like dog that are not identical to their own inverse.', 'This can be done in three steps.', 'We first apply the technique used for Malay reduplication.', 'That is, we redefine L as \"^[\" \"[\" L XX \"]\" \"^\" 2 \"^]\", and apply the compile-replace operation.', 'At this point the lower-side of L contains strings such as dogXXdogXX and madamXXmadamXX where XX is a specially introduced symbol to mark the middle (and the end) of each string.', 'The next, and somewhat delicate, step is to replace the XX markers by the desired operators, intersection and reverse, and to wrap the special regular expression delimiters ^[ and ^] around the whole lexicon.', 'This can be done by composing L with one or several replace transducers to yield a network consisting of expressions such as ^[ d o g & [d o g].r ^] and ^[ m a d a m & [m a d a m].r ^] In the third and final step, the application of compile-replace eliminates words like dog because the intersection of dog with the inverted form god is empty.', 'Only the palindromes survive the operation.', 'The extrac tion of all the palindromes from the 25K Unix /usr/dict/words file by this method takes a couple of seconds.']\n",
      "['Part-of-speech (POS) tag distributions are known to exhibit sparsity — a word is likely to take a single predominant tag in a corpus.', 'Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.', 'However, in existing systems, this expansion come with a steep increase in model complexity.', 'This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments.', 'In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training.', 'Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.', 'On several languages, we report performance exceeding that of more complex state-of-the art systems.1', 'Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words are likely to select a single predominant tag in a corpus, even when several tags are possible.', 'Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.', 'This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.', '— similar results have been observed across multiple languages.', 'Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.', 'In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).', 'These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable.', 'By design, they readily capture regularities at the token-level.', 'However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity.', 'Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi and Knight, 2009).', 'In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time.', 'In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.', 'The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', 'Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.', 'In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.', 'Across all languages, high performance can be attained by selecting a single tag per word type.', 'token-level HMM to reflect lexicon sparsity.', 'This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).', 'There are two key benefits of this model architecture.', 'First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).', 'Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference.', 'We evaluate our model on seven languages exhibiting substantial syntactic variation.', 'On several languages, we report performance exceeding that of state-of-the art systems.', 'Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na¨ıveBayes approach to incorporate features.', 'The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts.', 'Recent work has made significant progress on unsupervised POS tagging (Me´rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009).', 'Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.', 'This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.', 'The extent to which this constraint is enforced varies greatly across existing methods.', 'On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).', 'These clusters are computed using an SVD variant without relying on transitional structure.', 'While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.', 'Other approaches encode sparsity as a soft constraint.', 'For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags.', 'This design does not guarantee “structural zeros,” but biases towards sparsity.', 'A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Grac¸a et al., 2009).', 'This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive.', 'A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types.', 'The use of ILP in learning the desired grammar significantly increases the computational complexity of this method.', 'In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.', 'This design leads to a significant reduction in the computational complexity of training and inference.', 'Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).', 'These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training.', 'In our work, we demonstrate that using a simple na¨ıveBayes approach also yields substantial performance gains, without the associated training complexity.', 'We consider the unsupervised POS induction problem without the use of a tagging dictionary.', 'A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1.', 'As is standard, we use a fixed constant K for the number of tagging states.', 'Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.', 'Conditioned on T , features of word types W are drawn.', 'We refer to (T , W ) as the lexicon of a language and ψ for the parameters for their generation; ψ depends on a single hyperparameter β.', 'Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters θ are generated conditioned on tag assignments T . We also draw transition parameters φ.', 'Both parameters depend on a single hyperparameter α.', 'Once HMM parameters (θ, φ) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from φ.', 'The corresponding token words w are drawn conditioned on t and θ.2 Our full generative model is given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 The transition distribution φt for each tag t is drawn according to DIRICHLET(α, K ), where α is the shared transition and emission distribution hyperparameter.', 'In total there are O(K 2) parameters associated with the transition parameters.', 'In contrast to the Bayesian HMM, θt is not drawn from a distribution which has support for each of the n word types.', 'Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then θt is drawn from DIRICHLET(α, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4).', 'Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (φ, θ) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon] \\uf8eb n n \\uf8ed (w,t)∈(w,t) j \\uf8f6 P (tj |φtj−1 )P (wj |tj , θtj )\\uf8f8 P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively.', 'Since the parameter and token components will remain fixed throughout experiments, we briefly describe each.', 'Parameter Component As in the standard Bayesian HMM (Goldwater and Griffiths, 2007), all distributions are independently drawn from symmetric Dirichlet distributions: 2 Note that t and w denote tag and word sequences respectively, rather than individual tokens or tags.', 'Note that in our model, conditioned on T , there is precisely one t which has nonzero probability for the token component, since for each word, exactly one θt has support.', '3.1 Lexicon Component.', 'We present several variations for the lexical component P (T , W |ψ), each adding more complex pa rameterizations.', 'Uniform Tag Prior (1TW) Our initial lexicon component will be uniform over possible tag assignments as well as word types.', 'Its only purpose is 3 This follows since each θt has St − 1 parameters and.', 'P St = n. β T VARIABLES ψ Y W : Word types (W1 ,.', '.., Wn ) (obs) P T : Tag assigns (T1 ,.', '.., Tn ) T W φ E w : Token word seqs (obs) t : Token tag assigns (det by T ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ T tm O K θ E wN m N N Figure 1: Graphical depiction of our model and summary of latent variables and parameters.', 'The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters θ.', 'The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.', 'The hyperparameters α and β represent the concentration parameters of the token- and type-level components of the model respectively.', 'They are set to fixed constants.', 'to explore how well we can induce POS tags using only the one-tag-per-word constraint.', 'Specifically, the lexicon is generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).', 'Past work however, has typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated these features with token occurrences, typically in an HMM.', 'In our model, we associate these features at the type-level in the lexicon.', 'Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.', 'Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution ψ over tag assignments drawn from DIRICHLET(β, K ).', 'This alters generation of T as follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note that this distribution captures the frequency of a tag across word types, as opposed to tokens.', 'The P (T |ψ) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary.', 'In contrast, NNP (proper nouns) form a large portion of vocabulary.', 'Note that these observa sider suffix features, capitalization features, punctuation, and digit features.', 'While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al.', '(2010), we adopt a simpler na¨ıve Bayes strategy, where all features are emitted independently.', 'Specifically, we assume each word type W consists of feature-value pairs (f, v).', 'For each feature type f and tag t, a multinomial ψtf is drawn from a symmetric Dirichlet distribution with concentration parameter β.', 'The P (W |T , ψ) term in the lexicon component now decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n \\uf8eb \\uf8f6 tions are not modeled by the standard HMM, which = n \\uf8ed n P (v|ψTi f )\\uf8f8 instead can model token-level frequency.', 'i=1 (f,v)∈Wi', 'For inference, we are interested in the posterior probability over the latent variables in our model.', 'During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.', 'Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i).', 'The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).', 'Performance typically stabilizes across languages after only a few number of iterations.', 'to represent the ith word type emitted by the HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) i i (−i) P (Ti|W , T −i, β)P (t |Ti, t , w, α) All terms are Dirichlet distributions and parameters can be analytically computed from counts in t(−i)where T −i denotes all type-level tag assignment ex cept Ti and t(−i) denotes all token-level tags except and w (−i) (Johnson, 2007).', 't(i).', 'The terms on the right-hand-side denote the type-level and token-level probability terms respectively.', 'The type-level posterior term can be computed according to, P (Ti|W , T −i, β) ∝ Note that each round of sampling Ti variables takes time proportional to the size of the corpus, as with the standard token-level HMM.', 'A crucial difference is that the number of parameters is greatly reduced as is the number of variables that are sampled during each iteration.', 'In contrast to results reported in Johnson (2007), we found that the per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, T −i, β) formance of our Gibbs sampler on the basic 1TW model stabilized very quickly after about 10 full it All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts.', 'The token-level term is similar to the standard HMM sampling equations found in Johnson (2007).', 'The relevant variables are the set of token-level tags that appear before and after each instance of the ith word type; we denote these context pairs with the set {(tb, ta)} and they are contained in t(−i).', 'We use w erations of sampling (see Figure 2 for a depiction).', 'We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish.', 'On each language we investigate the contribution of each component of our model.', 'For all languages we do not make use of a tagging dictionary.', 'Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45.', '2 62.6 45.', '1 61.7 37.', '2 56.2 32.', '1 53.8 47.', '4 53.7 43.', '9 61.0 44.', '2 62.2 39.', '3 68.4 49.', '0 68.4 48.', '5 68.1 34.', '3 54.4 33.', '36.', '0 55.3 34.', '9 50.2 +P RI OR be st me dia n 47.', '9 65.5 46.', '5 64.7 42.', '3 58.3 40.', '0 57.3 51.', '4 65.9 48.', '3 60.7 50.', '41.', '7 68.3 56.', '2 70.7 52.', '0 70.9 42.', '37.', '1 55.8 38.', '36.', '8 57.3 +F EA TS be st me dia n 50.', '9 66.4 47.', '8 66.4 52.', '1 61.2 43.', '2 60.7 56.', '4 69.0 51.', '5 67.3 55.', '4 70.4 46.', '2 61.7 64.', '1 74.5 56.', '5 70.1 58.', '3 68.9 50.', '0 57.2 43.', '3 61.7 38.', '5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).', 'For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies.', 'For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 11 metric.', 'The second row represents the performance of the median hyperparameter setting.', 'Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3).', 'La ng ua ge # To ke ns # W or d Ty pe s # Ta gs E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics for various corpora utilized in experiments.', 'See Section 5.', 'The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task.', '5.1 Data Sets.', 'Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English.', 'For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation).', 'We train and test on the CoNLL-X training set.', 'Statistics for all data sets are shown in Table 2.', '5.2 Setup.', 'Models To assess the marginal utility of each component of the model (see Section 3), we incremen- tally increase its sophistication.', 'Specifically, we (+FEATS) utilizes the tag prior as well as features (e.g., suffixes and orthographic features), discussed in Section 3, for the P (W |T , ψ) component.', 'Hyperparameters Our model has two Dirichlet concentration hyperparameters: α is the shared hyperparameter for the token-level HMM emission and transition distributions.', 'β is the shared hyperparameter for the tag assignment prior and word feature multinomials.', 'We experiment with four values for each hyperparameter resulting in 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations In each run, we performed 30 iterations of Gibbs sampling for the type assignment variables W .4 We use the final sample for evaluation.', 'Evaluation Metrics We report three metrics to evaluate tagging performance.', 'As is standard, we report the greedy one-to-one (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags.', 'We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.', 'encodes the one tag per word constraint and is uni form over type-level tag assignments.', 'The second model (+PRIOR) utilizes the independent prior over type-level tag assignments P (T |ψ).', 'The final model tions.', '5 We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008).', 'we perform five runs with different random initialization of sampling state.', 'Hyperparameter settings are sorted according to the median one-to-one metric over runs.', 'We report results for the best and median hyperparameter settings obtained in this way.', 'Specifically, for both settings we report results on the median run for each setting.', 'Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set.', 'The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.', 'We tokenize MWUs and their POS tags; this reduces the tag set size to 12.', 'See Table 2 for the tag set size of other languages.', 'With the exception of the Dutch data set, no other processing is performed on the annotated tags.', '6 Results and Analysis.', 'We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings.', 'Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness.', 'Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.', '(2010) and the posterior regular- ization HMM of Grac¸a et al.', '(2009).', 'The system of Berg-Kirkpatrick et al.', '(2010) reports the best unsupervised results for English.', 'We consider two variants of Berg-Kirkpatrick et al.', '(2010)’s richest model: optimized via either EM or LBFGS, as their relative performance depends on the language.', 'Our model outperforms theirs on four out of five languages on the best hyperparameter setting and three out of five on the median setting, yielding an average absolute difference across languages of 12.9% and 3.9% for best and median settings respectively compared to their best EM or LBFGS performance.', 'While Berg-Kirkpatrick et al.', '(2010) consistently outperforms ours on English, we obtain substantial gains across other languages.', 'For instance, on Spanish, the absolute gap on median performance is 10%.', 'Top 5 Bot to m 5 Go ld NN P NN JJ CD NN S RB S PD T # ” , 1T W CD W RB NN S VB N NN PR P$ W DT : MD . +P RI OR CD JJ NN S WP $ NN RR B- , $ ” . +F EA TS JJ NN S CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting.', 'Our second point of comparison is with Grac¸a et al.', '(2009), who also incorporate a sparsity constraint, but does via altering the model objective using posterior regularization.', 'We can only compare with Grac¸a et al.', '(2009) on Portuguese (Grac¸a et al.', '(2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours).', 'Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result.', 'However, our full model takes advantage of word features not present in Grac¸a et al.', '(2009).', 'Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming Grac¸a et al.', '(2009).', 'Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3.', 'A novel element of our model is the ability to capture type-level tag frequencies.', 'For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR).', 'Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively.', 'Similar behavior is observed when adding features.', 'The difference between the featureless model (+PRIOR) and our full model (+FEATS) is 13.6% and 7.7% average error reduction on best and median settings respectively.', 'Overall, the difference between our most basic model (1TW) and our full model (+FEATS) is 21.2% and 13.1% for the best and median settings respectively.', 'One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively.', 'We observe similar trends when using another measure – type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods.', 'Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.', 'La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1.', '1 1 0.', '1 2 3.', '8 1 2.', '8 1 8.', '4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type.', 'The state-to-tag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3.', 'our full model yields 39.3% average error reduction across languages when compared to the basic configuration (1TW).', 'Table 5 provides insight into the behavior of different models in terms of the tagging lexicon they generate.', 'The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard.', '7 Conclusion and Future Work.', 'We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', 'This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model.', 'The resulting model is compact, efficiently learnable and linguistically expressive.', 'Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.', 'In this paper, we make a simplifying assumption of one-tag-per-word.', 'This assumption, however, is not inherent to type-based tagging models.', 'A promising direction for future work is to explicitly model a distribution over tags for each word type.', 'We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.', 'The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684).', 'We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.', 'We thank members of the MIT NLP group for their suggestions and comments.', 'Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.']\n",
      "['In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations.', 'Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances.', 'Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems.', 'Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources.', 'With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources.', 'Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision.', 'In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision.', 'From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts.', 'The system architecture is designed with generality in mind, avoiding any relation-specific inference technique.', 'Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora.', 'From the other side, Espresso requires only weak human supervision.', 'In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.)', 'In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required.', 'To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.', 'To date, most research on lexical relation harvesting has focused on is-a and part-of relations.', 'Approaches fall into two main categories: pattern- and clustering-based.', 'Most common are pattern-based approaches.', 'Hearst [12] pioneered using patterns to extract hyponym (is-a) relations.', 'Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms.', 'Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12].', 'Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures.', 'While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms.', 'Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s part- NP].', 'This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances.', 'In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)].', 'While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations.', 'Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data.', 'A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences.', 'The frequencies of the substrings in the corpus are then used to retain the best patterns.', 'The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of.', 'Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency.', 'Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations.', 'Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks.', 'Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction.', 'These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.', 'Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters.', 'Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun.', 'The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.', 'The Espresso algorithm is based on a similar framework to the one adopted in [12].', 'For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist).', 'Starting from these seeds, the algorithm begins a four-phase loop.', \"In the first phase, the algorithm infers a set of patterns P that captures as many of the seed instances as possible in C. In the second phase, we define a reliability measure to select the best set of patterns P'⊆P. In phase three, the patterns in P' are used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and then selects the best instances I' as input seeds for the next iteration.\", 'The algorithm terminates when a predefined stopping condition is met (for our preliminary experiments, the stopping condition is set according to the size of the corpus).', 'For each induced pattern p and instance i, the information theoretic scores, rπ(p) and rι(i) respectively, aim to express their reliability.', 'Below, Sections 3.2–3.5 describe in detail these different phases of Espresso.', '3.1.', 'Term definition.', 'Before one can extract relation instances from a corpus, it is necessary to define a tokenization procedure for extracting terms.', 'Terms are commonly defined as surface representations of stable and key domain concepts [19].', 'Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms.', 'We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point).', 'Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points.', 'Hence, we can capture relations between complex terms, such as “record of a criminal conviction” part-of “FBI report”.', '3.2.', 'Phase 1: Pattern discovery.', \"The pattern discovery phase takes as input a set of instances I' and produces as output a set of lexical patterns P. For the first iteration I' = Is, the set of initial seeds.\", 'In order to induce P, we apply a slight modification to the approach presented in [20].', 'For each input instance i = {x, y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then generalized into a set of new sentences SGx,y by replacing all terminological expressions by a terminological label (TR).', 'For example: “Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y” is generalized as: “Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y” All substrings linking terms x and y are then extracted from the set SGx,y, and overall frequencies are computed.', 'The most frequent substrings then represent the set of new patterns P, where the frequency cutoff is experimentally set.', 'Term generalization is particularly useful for small corpora, where generalization is vital to ease the data sparseness.', 'However, the generalized patterns are naturally less precise.', 'Hence, when dealing with bigger corpora, the system allows the use of Sx,y∪SGx,y in order to extract substrings.', 'For our experiments, we used the set SGx,y . 3.3.', 'Phase 2: Pattern filtering.', 'In this phase, Espresso selects among the patterns P those that are most reliable.', 'Intuitively, a reliable pattern is one that is both highly precise and one that extracts many instances.', \"The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision).\", \"We thus prefer patterns that are highly associated with the input patterns I'.\", \"Pointwise mutual information [4] is a commonly used metric for measuring the strength of association between two events x and y: pmi(x, y ) = log P(x, y ) P(x)P(y ) We define the reliability of a pattern p, rπ(p), as its average strength of association across each input instance i in I', weighted by the reliability of each instance i: ⎛ ⎞ ∑⎜ pmi(i, p) ∗ r (i )⎟ ⎜ r ( p ) = i∈I ′ ⎝ max ι ⎟ pmi ⎠ π I ′ where rι(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances.\", 'rπ(p) ranges from [0,1].', 'The reliability of the manually supplied seed instances are rι(i) = 1.', 'The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: pmi(i, p) = log x, p, y x,*, y *, p,* where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard.', 'A well-known problem is that pointwise mutual information is biased towards infrequent events.', 'To address this, we multiply pmi(i, p) with the discounting factor suggested in [16].', \"The set of highest n scoring patterns P', according to rπ(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1.\", 'In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one.', 'Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered.', 'Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves.', 'Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus.', 'In future work, this parameter can be learned using a development corpus.', 'Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded.', 'However, we are currently exploring a web-expansion algorithm that could both help detect generic patterns and also filter out their incorrect instances.', 'We estimate the precision of the instance set generated by a new pattern p by looking at the number of these instances that are instantiated on the Web by previously accepted patterns.', 'Generic patterns will generate instances with higher Web counts than incorrect patterns.', 'Then, the Web counts can also be used to filter out incorrect instances from the generic patterns’ instantiations.', 'More details are discussed in Section 4.3.', '3.4.', 'Phase 3: Instance discovery.', \"In this phase, Espresso retrieves from the corpus the set of instances I that match any of the lexical patterns in P'.\", 'In small corpora, the number of extracted instances can be too low to guarantee sufficient statistical evidence for the pattern discovery phase of the next iteration.', 'In such cases, the system enters a web expansion phase, in which new instances for the given patterns are retrieved from the Web, using the Google search engine.', \"Specifically, for each instance i∈ I, the system creates a set of queries, using each pattern in P' with its y term instantiated with i’s y term.\", 'For example, given the instance “Italy ; country” and the pattern [Y such as X] , the resulting Google query will be “country such as *”.', 'New instances are then created from the retrieved Web results (e.g. “Canada ; country”) and added to I. We are currently exploring filtering mechanisms to avoid retrieving too much noise.', 'Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out.', 'A set of new instances is created for each instance i∈ I by extracting sub-terminological expressions from x corresponding to the syntactic head of terms.', 'For example, expanding the relation “new record of a criminal conviction” part-of “FBI report”, the following new instances are obtained: “new record” part-of “FBI report”, and “record” part-of “FBI report”.', '3.5.', 'Phase 4: Instance filtering.', 'Estimating the reliability of an instance is similar to estimating the reliability of a pattern.', 'Intuitively, a reliable instance is one that is highly associated with as many reliable patterns as possible (i.e., we have more confidence in an instance when multiple reliable patterns instantiate it.)', 'Hence, analogous to our pattern reliability measure in Section 3.3, we define the reliability of an instance i, rι(i), as: ∑ pmi(i, p) ∗ r (p) r (i) = p∈P′ max pmi ι P′ where rπ(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum pointwise mutual information between all patterns and all instances, as in Section 3.3.', \"Espresso finally selects the highest scoring m instances, I', and retains them as input for the subsequent iteration.\", 'In this paper, we experimentally set m = 200.', '4.1.', 'Experimental Setup.', 'In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations.', '4.1.1.', 'Datasets We perform our experiments using the following two datasets: \\x83 TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection.', 'The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319.', '\\x83 CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2].', 'We preprocess the corpora using the Alembic Workbench POStagger [5].', '4.1.2.', 'Systems We compare the results of Espresso with the following two state of the art extraction systems: \\x83 RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)', '\\x83 PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label.', 'For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one.', 'To apply this algorithm, both datasets were first analyzed using the Minipar parser [14].', '\\x83 ESP: This is the algorithm described in this paper (details in Section 3).', '4.1.3.', 'Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances.', 'For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: \\x83 succession: This relation indicates that one proper noun succeeds another in a position or title.', 'For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II.', 'We evaluate this relation on the TREC9 corpus.', '\\x83 reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction.', 'For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid.', 'We evaluate this relation on the CHEM corpus.', '\\x83 production: This relation occurs when a process or element/object produces a result.', 'For example, ammonia produces nitric oxide.', 'We evaluate this relation on the CHEM corpus.', 'For each semantic relation, we manually extracted a set of seed examples.', 'The seeds were used for both Espresso as well as RH021.', 'Table 1 lists a sample of the seeds as well as sample outputs from Espresso.', '4.2.', 'Precision and Recall.', 'We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets.', 'For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds..', 'Table 1.', 'Sample seeds used for each semantic relation and sample outputs from Espresso.', 'The number in the parentheses for each relation denotes the total number of seeds.', 'E CHEM corpus) and evaluating their quality manually using one human judge2.', 'For each instance, the judge may assign a score of 1 for correct, 0 for incorrect, and ½ for partially correct.', 'Example instances that were judged partially correct include “analyst is-a manager” and “pilot is-a teacher”.', 'The precision for a given set of relation instances is the sum of the judge’s scores divided by the number of instances.', 'Although knowing the total number of instances of a particular relation in any nontrivial corpus is impossible, it is possible to compute the recall of a system relative to another system’s recall.', 'The recall of a system A, RA, is given by the following formula: C R A = C where CA is the number of correct instances of a particular relation extracted by A and C is the total number of correct instances in the corpus.', 'Following [17], we define the relative recall of system A given system B, RA|B, as: RA|B = RA = C A P × A = A RB CB PB × B Using the precision estimates, PA, from our precision experiments, we can estimate CA ≈ PA × |A|, where A is the total number of instances of a particular relation discovered by system A. 2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and.', 'agreement scores.', 'Table 2.', 'System performance on the is-a relation on the TREC9 dataset.', 'Table 3.', 'System performance on the is-a relation on the CHEM dataset.', 'SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 5 7 , 5 2 5 2 8 . 0 % 5 . 3 1 RH 02 2 5 5 6 2 5 . 0 % 3 . 7 6 PR 04 1 , 5 0 4 4 7 . 0 % 0 . 2 3 PR 04 1 0 8 4 0 . 0 % 0 . 2 5 ES P 4 , 1 5 4 7 3 . 0 % 1 . 0 0 ES P 2 0 0 8 5 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.', '† Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.', '† Relative recall is given in relation to ESP. Table 4.', 'System performance on the part-of relation on the TREC9 dataset.', 'Table 5.', 'System performance on the part-of relation on the CHEM dataset.', 'SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 1 2 , 8 2 8 3 5 . 0 % 4 2 . 5 2 RH 02 1 1 , 5 8 2 3 3 . 8 % 5 8 . 7 8 ES P 1 3 2 8 0 . 0 % 1 . 0 0 ES P 1 1 1 6 0 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.', '† Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.', '† Relative recall is given in relation to ESP. Table 6.', 'System performance on the succession relation on the TREC9 dataset.', 'Table 7.', 'System performance on the reaction relation on the CHEM dataset.', 'SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LL† RH 02 4 9 , 7 9 8 2 . 0 % 3 6 . 9 6 RH 02 6 , 0 8 3 3 0 % 5 3 . 6 7 ES P 5 5 4 9 . 0 % 1 . 0 0 ES P 4 0 8 5 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances.', '† Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances.', '† Relative recall is given in relation to ESP. Tables 2 – 8 reports the total number of instances, precision, and relative recall of each system on the TREC9 and CHEM corpora.', 'The relative recall is always given in relation to the Espresso system.', 'For example, in Table 2, RH02 has a relative recall of 5.31 with Espresso, which means that the RH02 system output 5.31 times more correct relations than Espresso (at a cost of much Table 8.', 'System performance on the production relation on the CHEM dataset.', 'SYSTEM INSTANCES PRECISION* REL RECALL† RH02 197 57.5% 0.80 ESP 196 72.5% 1.00 * Precision estimated from 20 randomly sampled instances.', '† Relative recall is given in relation to ESP. lower precision).', 'Similarly, PR04 has a relative recall of 0.23 with Espresso, which means that PR04 outputs 4.35 fewer correct relations than Espresso (also with a smaller precision).', '4.3.', 'Discussion.', 'Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision.', 'However, Espresso fails to match the recall level of RH02 in all but the experiment on the production relation.', 'Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous – hence resulting in a loss of recall.', 'As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise in the system (e.g, the pattern [X of Y] can ambiguously refer to a part-of, is-a or possession relation).', 'However, generic patterns, while having low precision, yield a high recall, as also reported by [11].', 'We ran an experiment on the reaction relation, retaining the generic patterns produced during Espresso’s selection process.', 'As expected, we obtained 1923 instances instead of the 40 reported in Table 7, but precision dropped from 85% to 30%.', 'The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso.', 'We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns.', 'Unlike Girju et al. [11] that propose a highly supervised machine learning approach based on selectional restriction, ours is an unsupervised method based on statistical evidence obtained from the Web.', 'At a given iteration in Espresso, the intuition behind our solution is that the Web is large enough that correct instances will be instantiated by many of the currently accepted patterns P. Hence, we can distinguish between generic patterns and incorrect patterns by inspecting the relative frequency distribution of their instances using the patterns in P. More formally, given an instance i produced by a generic or incorrect pattern, we count how many times i instantiates on the Web with every pattern in P, using Google.', 'The instance i is then considered correct if its web count surpasses a given threshold.', 'The pattern in question is accepted as a generic pattern if a sufficient number of its instances are considered correct, otherwise it is rejected as an incorrect pattern.', 'Although our results in Section 4.2 do not include this algorithm, we performed a small experiment by adding an a-posteriori generic pattern recovery phase to Espresso.', 'We tested the 7,634 instances extracted by the generic pattern [X of Y] on the CHEM corpus for the part-of relation.', 'We randomly sample 200 of these instances and then queried Google for these instances using the pattern [X consists of Y].', 'Manual evaluation of the 25 instances that occurred at least once on Google showed 50% precision.', 'Adding these instances to the results from Table 5 decreases the system precision from 60% to 51%, but dramatically increases Espresso’s recall by a factor of 8.16.', 'Furthermore, it is important to note that there are several other generic patterns, like [X’s Y], from which we expect a similar precision of 50% with a continual increase of recall.', 'This is a very exciting avenue of further investigation.', 'We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text.', 'Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances.', 'There are many avenues of future work.', 'Preliminary results show that Espresso generates highly precise relations, but at the expense of lower recall.', 'As mentioned above in Section 4.3, we are working on improving system recall with a web-based method to identify generic patterns and filter their instances.', 'Early results appear very promising.', 'We also plan to investigate the use of WordNet selectional constraints, as proposed by [11].', 'We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns.', 'Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes.', 'It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA.', 'The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.']\n",
      "W06-3909 invalid syntax (<unknown>, line 1)\n",
      "['This paper presents a maximum entropy-based named entity recognizer (NER).', 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.', 'Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.', 'In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.', 'Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC).', 'A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.', 'In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task.', 'Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).', 'We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.', 'By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.', 'We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).', 'As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.', 'The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors).', 'These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999).', 'We believe it is natural for authors to use abbreviations in subsequent mentions of a named entity (i.e., first “President George Bush” then “Bush”).', 'As such, global information from the whole context of a document is important to more accurately recognize named entities.', 'Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages.', 'Recently, statistical NERs have achieved results that are comparable to hand-coded systems.', \"Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance.\", \"MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data.\", 'MENE (Maximum Entropy Named Entity) (Borth- wick, 1999) was combined with Proteus (a hand- coded system), and came in fourth among all MUC 7 participants.', 'MENE without Proteus, however, did not do very well and only achieved an F measure of 84.22% (Borthwick, 1999).', 'Among machine learning-based NERs, Identi- Finder has proven to be the best on the official MUC6 and MUC7 test data.', 'MENE (without the help of hand-coded systems) has been shown to be somewhat inferior in performance.', 'By using the output of a hand-coded system such as Proteus, MENE can improve its performance, and can even outperform IdentiFinder (Borthwick, 1999).', 'Mikheev et al.', '(1998) did make use of information from the whole document.', 'However, their system is a hybrid of hand-coded rules and machine learning methods.', 'Another attempt at using global information can be found in (Borthwick, 1999).', 'He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution.', 'Reference resolution involves finding words that co-refer to the same entity.', 'In order to train this error-correction model, he divided his training corpus into 5 portions of 20% each.', 'MENE is then trained on 80% of the training corpus, and tested on the remaining 20%.', 'This process is repeated 5 times by rotating the data appropriately.', 'Finally, the concatenated 5 * 20% output is used to train the reference resolution component.', \"We will show that by giving the first model some global features, MENERGI outperforms Borthwick' s reference resolution classifier.\", 'On MUC6 data, MENERGI also achieves performance comparable to IdentiFinder when trained on similar amount of training data.', 'both MENE and IdentiFinder used more training data than we did (we used only the official MUC 6 and MUC7 training data).', 'On the MUC6 data, Bikel et al.', '(1997; 1999) do have some statistics that show how IdentiFinder performs when the training data is reduced.', 'Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data.', 'The system described in this paper is similar to the MENE system of (Borthwick, 1999).', 'It uses a maximum entropy framework and classifies each word given its features.', 'Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique.', 'Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class).', '3.1 Maximum Entropy.', 'The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed.', 'Such constraints are derived from training data, expressing some relationship between features and outcome.', 'The probability distribution that satisfies the above property is the one with the highest entropy.', 'It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra et al., 1997): where refers to the outcome, the history (or context), and is a normalization function.', 'In addition, each feature function is a binary function.', 'For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972).', 'This is an iterative method that improves the estimation of the parameters at each iteration.', 'We have used the Java-based opennlp maximum entropy package1.', 'In Section 5, we try to compare results of MENE, IdentiFinder, and MENERGI.', 'However, 1 http://maxent.sourceforge.net 3.2 Testing.', 'During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique).', 'To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise.', 'The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.', 'A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.', 'The features we used can be divided into 2 classes: local and global.', 'Local features are features that are based on neighboring tokens, as well as the token itself.', 'Global features are extracted from other occurrences of the same token in the whole document.', \"The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999).\", 'However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).', 'This might be because our features are more comprehensive than those used by Borthwick.', 'In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used.', 'In the maximum entropy framework, there is no such constraint.', 'Multiple features can be used for the same token.', 'Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.', 'We group the features used into feature groups.', 'Each feature group can be made up of many binary features.', 'For each token , zero, one, or more of the features in each feature group are set to 1.', '4.1 Local Features.', 'The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.', 'This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.', 'Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones).', 'The zone to which a token belongs is used as a feature.', 'For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD).', 'Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.', 'Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1.', 'If it is made up of all capital letters, then (allCaps, zone) is set to 1.', 'If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.', 'A token that is allCaps will also be initCaps.', 'This group consists of (3 total number of possible zones) features.', 'Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.', 'For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.', 'If the token is the first word of a sentence, then this feature is set to 1.', 'Otherwise, it is set to 0.', 'Lexicon Feature: The string of the token is used as a feature.', 'This group contains a large number of features (one for each token string present in the training data).', 'At most one feature in this group will be set to 1.', 'If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0.', 'Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.', 'If is not initCaps, then (not-initCaps, ) is set to 1.', 'Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1.', 'This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter).', 'Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1.', 'Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task.', 'The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999).', 'The sources of our dictionaries are listed in Table 2.', 'For all lists except locations, the lists are processed into a list of tokens (unigrams).', 'Location list is processed into a list of unigrams and bigrams (e.g., New York).', 'For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.', 'A list of words occurring more than 10 times in the training data is also collected (commonWords).', 'Only tokens with initCaps not found in commonWords are tested against each list in Table 2.', 'If they are found in a list, then a feature for that list will be set to 1.', 'For example, if Barry is not in commonWords and is found in the list of person first names, then the feature PersonFirstName will be set to 1.', 'Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1.', 'For example, if is found in the list of person first names, the feature PersonFirstName is set to 1.', 'Month Names, Days of the Week, and Numbers: If is initCaps and is one of January, February, . . .', ', December, then the feature MonthName is set to 1.', 'If is one of Monday, Tuesday, . . .', ', Sun day, then the feature DayOfTheWeek is set to 1.', 'If is a number string (such as one, two, etc), then the feature NumberString is set to 1.', 'Suffixes and Prefixes: This group contains only two features: Corporate-Suffix and Person-Prefix.', 'Two lists, Corporate-Suffix-List (for corporate suffixes) and Person-Prefix-List (for person prefixes), are collected from the training data.', 'For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data.', 'Frequency is calculated by counting the number of distinct previous tokens that each token has (e.g., if Electric Corp. is seen 3 times, and Manufacturing Corp. is seen 5 times during training, and Corp. is not seen with any other preceding tokens, then the “frequency” of Corp. is 2).', 'The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List.', 'A Person-Prefix-List is compiled in an analogous way.', 'For MUC6, for example, Corporate- Suffix-List is made up of ltd., associates, inc., co, corp, ltd, inc, committee, institute, commission, university, plc, airlines, co., corp. and Person-Prefix- List is made up of succeeding, mr., rep., mrs., secretary, sen., says, minister, dr., chairman, ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1.', 'If any of the tokens from to is in Person-Prefix- List, then another feature Person-Prefix is set to 1.', 'Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr., etc are not part of person names, whereas corporate suffixes like Corp., Inc., etc are part of corporate names.', '4.2 Global Features.', 'Context from the whole document can be important in classifying a named entity.', 'A name already mentioned previously in a document may appear in abbreviated form when it is mentioned again later.', 'Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al., 1998).', 'We often encounter sentences that are highly ambiguous in themselves, without some prior knowledge of the entities concerned.', 'For example: McCann initiated a new global system.', '(1) CEO of McCann . . .', '(2) Description Source Location Names http://www.timeanddate.com http://www.cityguide.travel-guides.com http://www.worldtravelguide.net Corporate Names http://www.fmlx.com Person First Names http://www.census.gov/genealogy/names Person Last Names Table 2: Sources of Dictionaries The McCann family . . .', '(3)In sentence (1), McCann can be a person or an orga nization.', 'Sentence (2) and (3) help to disambiguate one way or the other.', 'If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided.', 'The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.', 'For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.', 'For example, in the sentence that starts with “Bush put a freeze on . . .', '”, because Bush is the first word, the initial caps might be due to its position (as in “They put a freeze on . . .', '”).', 'If somewhere else in the document we see “restrictions put in place by President Bush”, then we can be surer that Bush is a name.', 'Corporate Suffixes and Person Prefixes of Other Occurrences (CSPP): If McCann has been seen as Mr. McCann somewhere else in the document, then one would like to give person a higher probability than organization.', 'On the other hand, if it is seen as McCann Pte.', 'Ltd., then organization will be more probable.', 'With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.', 'Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM).', 'The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.', 'Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.', 'For example, if FCC and Federal Communications Commission are both found in a document, then Federal has A begin set to 1, Communications has A continue set to 1, Commission has A end set to 1, and FCC has A unique set to 1.', 'Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name.', 'However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even.', 'This group of features attempts to capture such information.', 'For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified.', 'For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1.', 'Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.', 'needs to be in initCaps to be considered for this feature.', 'If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.', 'As we will see from Table 3, not much improvement is derived from this feature.', 'The baseline system in Table 3 refers to the maximum entropy system that uses only local features.', 'As each global feature group is added to the list of features, we see improvements to both MUC6 and MUC6 MUC7 Baseline 90.75% 85.22% + ICOC 91.50% 86.24% + CSPP 92.89% 86.96% + ACRO 93.04% 86.99% + SOIC 93.25% 87.22% + UNIQ 93.27% 87.24% Table 3: F-measure after successive addition of each global feature group Table 5: Comparison of results for MUC6 Systems MUC6 MUC7 No.', 'of Articles No.', 'of Tokens No.', 'of Articles No.', 'of Tokens MENERGI 318 160,000 200 180,000 IdentiFinder – 650,000 – 790,000 MENE – – 350 321,000 Table 4: Training Data MUC7 test accuracy.2 For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.', 'ICOC and CSPP contributed the greatest im provements.', 'The effect of UNIQ is very small on both data sets.', 'All our results are obtained by using only the official training data provided by the MUC conferences.', 'The reason why we did not train with both MUC6 and MUC7 training data at the same time is because the task specifications for the two tasks are not identical.', 'As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.', \"In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999).\", \"IdentiFinder ' 99' s results are considerably better than IdentiFinder ' 97' s. IdentiFinder' s performance in MUC7 is published in (Miller et al., 1998).\", 'MENE has only been tested on MUC7.', 'For fair comparison, we have tabulated all results with the size of training data used (Table 5 and Table 6).', 'Besides size of training data, the use of dictionaries is another factor that might affect performance.', 'Bikel et al.', '(1999) did not report using any dictionaries, but mentioned in a footnote that they have added list membership features, which have helped marginally in certain domains.', 'Borth 2MUC data can be obtained from the Linguistic Data Consortium: http://www.ldc.upenn.edu 3Training data for IdentiFinder is actually given in words (i.e., 650K & 790K words), rather than tokens Table 6: Comparison of results for MUC7 wick (1999) reported using dictionaries of person first names, corporate names and suffixes, colleges and universities, dates and times, state abbreviations, and world regions.', 'In MUC6, the best result is achieved by SRA (Krupka, 1995).', 'In (Bikel et al., 1997) and (Bikel et al., 1999), performance was plotted against training data size to show how performance improves with training data size.', \"We have estimated the performance of IdentiFinder ' 99 at 200K words of training data from the graphs.\", 'For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.', 'In fact, training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters, and the test data is about air vehicle launching.', 'Both BBN and NYU have tagged their own data to supplement the official training data.', \"Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999).\", 'Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.', 'The effect of a second reference resolution classifier is not entirely the same as that of global features.', 'A secondary reference resolution classifier has information on the class assigned by the primary classifier.', 'Such a classification can be seen as a not-always-correct summary of global features.', 'The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.', 'We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.', 'Moreover, if we want to test on a huge test corpus, indexing the whole corpus might prove computationally expensive.', 'Hence we decided to restrict ourselves to only information from the same document.', 'Mikheev et al.', '(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.', 'The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules.', 'We have shown that the maximum entropy framework is able to use global information directly.', 'This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).', 'Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs.', 'Information from a sentence is sometimes insufficient to classify a name correctly.', 'Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.', 'We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources.', 'Borth- wick (1999) successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.', 'However, such an approach requires a number of hand-coded systems, which may not be available in languages other than English.', 'We believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.', 'We see verb classes as the key to making gen\\xad eralizations about regular extensions of mean\\xad ing.', 'Current approaches to English classifica\\xad tion, Levin classes and WordNet, have limita\\xad tions in their applicability that impede their utility as general classification schemes.', 'We present a refinement of Levin classes, intersec\\xad tive sets, which are a more fine-grained clas\\xad sification and have more coherent sets of syn\\xad tactic frames and associated semantic compo\\xad nents.', 'We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the orig\\xad inal Levin classes.', 'We also have begun to ex\\xad amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.', 'The difficulty of achieving adequate hand\\xad crafted semantic representations has limited the field of natural language processing to applica\\xad tions that can be contained within well-defined subdomains.', 'The only escape from this lim\\xad itation will be through the use of automated or semi-automated methods of lexical acquisi\\xad tion.', 'However, the field has yet to develop a clear consensus on guidelines for a computa\\xad tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and San\\xad filippo, 1993), (Lowe et al., 1997), (Dorr, 1997).', 'The authors would like to acknowledge the sup\\xad port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952.', 'One of the most controversial areas has to do with polysemy.', 'What constitutes a clear sepa\\xad ration into senses for any one verb, and how can these senses be computationally characterized and distinguished?', 'The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single great\\xad est limitation on the general application of nat\\xad ural language processing techniques.', 'In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases.', 'We base these regular extensions on a fine-grained variation on Levin classes, inter\\xad sective Levin classes, as a source of semantic components associated with specific adjuncts.', 'We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components.', 'The difficulty of determining a suitable lexical representation becomes multi\\xad plied when more than one language is involved and attempts are made to map between them.', 'Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996).', 'However, we have found interesting par\\xad allels in how Portuguese and English treat reg\\xad ular sense extensions.', 'Two current approaches to English verb classi\\xad fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).', 'WordNet is an on\\xad line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each rep\\xad resenting a lexicalized concept.', 'A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence.', 'Words and synsets are interrelated by means of lexical and semantic-conceptual links, respec\\xad tively.', 'Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets.', 'WordNet was de\\xad signed principally as a semantic network, and contains little syntactic information.', 'Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntac\\xad tic frames that are in some sense meaning pre\\xad serving (diathesis alternations) (Levin, 1993).', 'The distribution of syntactic frames in which a verb can appear determines its class member\\xad ship.', 'The fundamental assumption is that the syntactic frames are a direct reflection of the un\\xad derlying semantics.', 'Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes.', 'The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect un\\xad derlying semantic components that constrain al\\xad lowable arguments.', 'For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the mid\\xad dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily.', 'However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut.', 'In addition, cut verbs can oc\\xad cur in the conative, John valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it, whereas break verbs cannot, *John broke at the window.', 'The explanation given is that cut describes a series of actions di\\xad rected at achieving the goal of separating some object into pieces.', 'It is possible for these ac\\xad tions to be performed without the end result being achieved, but where the cutting manner can still be recognized, i.e., John cut at the loaf.', 'Where break is concerned, the only thing speci\\xad fied is the resulting change of state where the object becomes separated into pieces.', 'If the result is not achieved, there are no attempted breaking actions that can still be recognized.', '2.1 Ambiguities in Levin classes.', 'It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997).', 'However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman\\xad tic components.', 'Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap be\\xad tween Levin and WordNet.', 'The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose.', 'For in\\xad stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the cona\\xad tive.', 'This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin.', 'Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed?', 'The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best.', 'We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to\\xad gether subsets of existing classes with over\\xad lapping members.', 'All subsets were included which shared a minimum of three members.', 'If only one or two verbs were shared between two classes, we assumed this might be due to ho\\xad mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship in\\xad volving coherent sets of verbs.', 'This filter al\\xad lowed us to reject the potential intersective class that would have resulted from combining the re\\xad move verbs with the scribble verbs, for example.', 'The sole member of this intersection is the verb draw.', 'On the other hand, the scribble verbs do form an intersective class with the perfor\\xad mance verbs, since paint and write are also in both classes, in addition to draw.', 'The algorithm we used is given in Figure 1.', '1. Enumerate all sets S = {c1, ...', ', Cn} of se-.', 'mantic classes such that let n ... n enI e, where e is a relevance cutoff.', '2.', 'For each such S = {ct, ...', ',en}, define an.', \"intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..• ,c} such that S C S' and v E ci n ... n dm (subset criterion).\", 'Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows.', 'A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.', 'Simultaneously, the verb was removed from the membership lists of those existing classes.', '3.1 Using intersective Levin classes to.', 'isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.', 'The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous seman\\xad tic class.', 'Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as \"separate by V-ing,\" where \"V\" is the basic meaning of that verb (Levin, 1993).', 'Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of \"separating\" belong to this class because of the component of force in their meaning.', 'They are interpretable as verbs of splitting or separating only in particular syn\\xad tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch).', 'The ad\\xad junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.', 'These fringe split verbs appear in several other inter\\xad sective classes that highlight the force aspect of their meaning.', 'Figure 2 depicts the intersection of split, carry and push/pull.', 'Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compo\\xad nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion.', 'Depending on the par\\xad ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compo\\xad nent Levin classes.', '1.', 'Nora pushed the package to Pamela..', '(carry verb implies causation of accompa\\xad nied motion, no separation) 2.', 'Nora pushed at/against the package..', '• Although kick is not listed as a verb of exerting force, it displays all the alternations that define this class.', 'Sim\\xad ilarly, draw and yank can be viewed as carry verbs al\\xad though they are not listed as such.', 'The list of members for each Levin verb class is not always complete, so to check if a particular verb belongs to a class it is better to check that the verb exhibits all the alternations that de\\xad fine the class.', 'Since intersective classes were built using membership lists rather than the set of defining alterna\\xad tions, they were similarly incomplete.', 'This is an obvious shortcoming of the current implementation of intersec\\xad tive classes, and might affect the choice of 3 as a relevance cutoff in later implementations.', '(verb of exerting force, no separation or causation of accompanied motion implied) 3.', 'Nora pushed the branches apart..', '(split verb implies separation, no causation of accompanied motion)', '{verb of exerting force; no separation im\\xad plied, but causation of accompanied motion possible) 5.', '*Nora pushed at the package to Pamela.', 'Although the Levin classes that make up an intersective class may have conflicting alterna\\xad tions {e.g., verbs of exerting force can take the conative alternation, while carry verbs cannot), this does not invalidate the semantic regularity of the intersective class.', 'As a verb of exerting force, push can appear in the conative alterna\\xad tion, which emphasizes its force semantic com\\xad ponent and ability to express an \"attempted\" action where any result that might be associ\\xad ated- with the verb (e.g., motion) is not nec\\xad essarily achieved; as a carry verb (used with a goal or directional phrase), push cannot take the conative alternation, which would conflict with the core meaning of the carry verb class (i.e., causation of motion).', 'The critical point is that, while the verb\\'s meaning can be extended to either \"attempted\" action or directed motion, these two extensions cannot co-occur - they are mutually exclusive.', 'However the simultaneous potential of mutually exclusive extensions is not a problem.', 'It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative.', 'The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more \"pure\" examples of the carry class and always imply the achievement of causation of motion.', 'Thus they cannot take the conative al\\xad ternation.', '3.2 Comparisons to WordNet.', 'Even though the Levin verb classes are defined by their syntactic behavior, many reflect seman\\xad tic distinctions made by WordNet, a classifica\\xad tion hierarchy defined in terms of purely se\\xad mantic word relations (synonyms, hypernyms, etc.).', 'When examining in detail the intersec\\xad tive classes just described, which emphasize not only the individual classes, but also their rela\\xad tion to other classes, we see a rich semantic lat\\xad tice much like WordNet.', 'This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs.', 'The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs.', 'WordNet distinguishes two subclasses of cut, differentiated by the type of result: 1.', 'Manner of cutting that results in separa\\xad.', 'tion into pieces (chip, clip, cut, hack, hew, saw, slash, snip), having cut, separate with an instrument as an immediate hypernym.', '2.', \"Manner of cutting that doesn't separate.\", 'completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an in\\xad strument as an immediate hypernym.', 'This distinction appears in the second-order Levin classes as membership vs. nonmember\\xad ship in the intersective class with split.', 'Levin verb classes are based on an underlying lat\\xad tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations.', 'Whereas high level semantic relations (syn\\xad onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class.', 'However, other intersective classes, such as the split/push/carry class, are no more con\\xad sistent with WordNet than the original Levin classes.', 'The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection.', 'In addition, only one verb (pull) has a WordNet sense cor\\xad responding to the change of state - separation semantic component associated with the split class.', 'The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs, and separation is inferred only when the verb occurs with an appropriate adjunct, such as apart.', 'However, apart can also be used with other classes of verbs, including many verbs of motion.', 'To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be gener\\xad ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb.', 'WordNet does not currently provide a consistent treatment of regular sense exten\\xad sion (some are listed as separate senses, others are not mentioned at all).', 'It would be straight\\xad forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and in\\xad clude corresponding syntactic information.', '3.3 Sense extension for manner of.', 'motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (me\\xad ander verbs).', 'Roll and run verbs have seman\\xad tic components describing a manner of motion that typically, though not necessarily, involves change of location.', 'In the absence of a goal or path adjunct they do not specify any direction of motion, and in some cases (e.g., float, bounce) require the adjunct to explicitly specify any dis\\xad placement at all.', 'The two classes differ in that roll verbs relate to manners of motion charac\\xad teristic of inanimate entities, while run verbs describe manners in which animate entities can move.', 'Some manner of motion verbs allow a transitive alternation in addition to the basic in\\xad transitive.', 'When a roll verb occurs in the tran\\xad sitive (Bill moved the box across the room), the subject physically causes the object to move, whereas the subject of a transitive run verb merely induces the object to move (the coach ran the athlete around the track).', 'Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes.', 'The slide class parti\\xad tions this roll/run intersection into verbs that can take the transitive alternation and verbs that cannot (drift and glide cannot be causative, because they are not typically externally con\\xad trollable).', 'Verbs in the slide/roll/run intersec\\xad tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actu\\xad ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley).', 'These verbs are listed in the intersective classes with meander verbs of existence.', '\"Meander Verbs\" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes.', 'The Portuguese verbs we examined behaved much more similarly to their English counter\\xad parts than we expected.', 'Many of the verbs participate in alternations that are direct trans\\xad lations of the English alternations.', 'However, there are some interesting differences in which sense extensions are allowed.', '4.1 Similar sense extensions.', 'We have made a preliminary study of the Por\\xad tuguese translation of the carry verb class.', 'As in English, these verbs seem to take different alter\\xad nations, and the ability of each to participate in an alternation is related to its semantic content.', 'Table 1 shows how these Portuguese verbs natu\\xad rally cluster into two different subclasses, based on their ability to take the conative and apart alternations as well as path prepositions.', 'These subclasses correspond very well to the English subclasses created by the intersective class.', 'The conative alternation in Portuguese is mainly contra (against), and the apart alterna\\xad tion is mainly separando (separating).', \"For ex\\xad ample, Eu puxei o ramo e o galho separandoos As in English, derivar and planar are not exter\\xad nally controllable actions and thus don't take the causativejinchoative alternation common to other verbs in the roll class.\", \"Planar doesn't take a direct object in Portuguese, and it shows the induced action alternation the same way as flu\\xad tuar (by using the light verb fazer).\", 'Derivar is usually said as \"estar a deriva\" (\"to be adrift\"), showing its non-controllable action more explic\\xad itly.', 'Table 1: Portuguese carry verbs with their al\\xad ternations (I pulled the twig and the branch apart ) , and Ele empurrou contra a parede (He pushed against the walQ.', '4.2 Changing class membership.', 'We also investigated the Portuguese translation of some intersective classes of motion verbs.', 'We selected the slide/roll/run, meander/roll and roll/run intersective classes.', \"Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes.\", 'The elements of the slide/roll/run class are rebater (bounce), flutuar (float), rolar ( rolQ and deslizar (slide).', 'The resultative in Portuguese cannot be expressed in the same way as in En\\xad glish.', 'It takes a gerund plus a reflexive, as in A porta deslizou abrindose (The door slid opening itself).', 'Transitivity is also not always preserved in the translations.', 'For example, flutuar does not take a direct object, so some of the alterna\\xad tions that are related to its transitive meaning are not present.', 'For these verbs, we have the in\\xad duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat).', 'As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simi\\xad lar properties with the English verbs, including the causative/inchoative.', 'The exception to this, as just noted, is flutuar (float).', 'The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and pla\\xad nar (glide) in the closely related roll/run class.', 'We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses.', 'Whereas each WordNet synset is hierarchicalized accord\\xad ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary.', 'Intersective Levin sets partition these classes according to more co\\xad herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.', 'Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the al\\xad ternations of a class.', 'This might be approxi\\xad mated by automatically extracting the syntac\\xad tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study.', 'We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sev\\xad eral of the same properties as the corresponding verbs in English.', 'Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions.', 'There are still many questions that require further investigation.', 'First, since our experi\\xad ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete.', 'We will be using re\\xad sources such as dictionaries and online corpora to investigate potential additional members of our classes.', 'Second, since the translation map\\xad pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve • c o n a t i v e c a u s . / i n c h . m i d d l e acc ept.', 'cor e£.', 'y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect.', 'par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at.', 'inv ers.', 'me asu re •ad j. per f. •c og n. ob je ct ze ro no m. y e s y e s y e s n o n o y e s y e s y e s y e s n o n o y e s y e s y e s y e s n o n o n o y e s y e s y e s n o n o y e s n o y e s y e s n o n o y e s y e s y e s y e s n o n o y e s Table 2: Portuguese slide/roll/run and roll/run verbs with their alternations tions may depend on which translation is cho\\xad sen, potentially giving us different clusters, but it is uncertain to what extent this is a factor, and it also requires further investigation.', 'In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class.', 'We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the litera\\xad ture (Palmer and Rosenzweig, 1996), (Copes\\xad take and Sanfilippo, 1993), (Dorr, 1997).', 'In pursuing this goal, we are currently implement\\xad ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998).', 'TAGs have also been applied to Por\\xad tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994).', 'We in\\xad tend to extend this grammar, building a more robust TAG grammar for Portuguese, that will allow us to build an English/Portuguese trans\\xad fer lexicon using these features.']\n",
      "J98-2005 not well-formed (invalid token): line 36, column 116\n",
      "P98-1081 not well-formed (invalid token): line 147, column 30\n",
      "['For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences.', 'News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web.', 'We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching.', 'We show that the latter performs best on the task of aligning paraphrastic headlines.', 'In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG).', 'In contrast to traditional concept-to-text systems, text-to-text generation systems convert source text to target text, where typically the source and target text share the same meaning to some extent.', 'Applications of text-to-text generation include sum- marization (Knight and Marcu, 2002), question- answering (Lin and Pantel, 2001), and machine translation.', 'For text-to-text generation it is important to know which words and phrases are semantically close or exchangable in which contexts.', 'While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level.', 'Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation.', 'Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation.', 'In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation.', 'News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News.', 'These services collect multiple articles covering the same event.', 'Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006).', 'We use this method to collect a large amount of aligned paraphrases in an automatic fashion.', 'We aim to build a high-quality paraphrase corpus.', 'Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system.', 'This implies that we focus on obtaining a high precision in the paraphrases collection process.', 'Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles.', 'We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles.', 'For the development of our system we use data which was obtained in the DAESO-project.', 'This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122–125, Athens, Greece, 30 – 31 March 2009.', 'Qc 2009 Association for Computational Linguistics document, and each original cluster as a collection of documents.', 'For each stemmed word i in sentence j, T Fi,j is a binary variable indicating if the word occurs in the sentence or not.', 'The T F ∗I DF score is then: TF.IDFi = T Fi,j · log | Table 1: Part of a sample headline cluster, with sub-clusters and Krahmer, 2007) and will be made available through the Dutch HLT Agency.', 'Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period April–August 2006.', 'For each news article, the headline and the first 150 characters of the article were stored.', 'Roughly 13,000 clusters were retrieved.', 'Table 1 shows part of a (translated) cluster.', 'It is clear that although clusters deal roughly with one subject, the headlines can represent quite a different perspective on the content of the article.', 'To obtain only paraphrase pairs, the clusters need to be more coherent.', 'To that end 865 clusters were manually subdivided into sub-clusters of headlines that show clear semantic overlap.', 'Sub- clustering is no trivial task, however.', 'Some sentences are very clearly paraphrases, but consider for instance the last two sentences in the example.', 'They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti ∈ dj }| |D| is the total number of sentences in the cluster and |{dj : ti ∈ dj }| is the number of sen tences that contain the term ti.', 'These scores are used in a vector space representation.', 'The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity.', '2.1 Clustering.', 'Our first approach is to use a clustering algorithm to cluster similar headlines.', 'The original Google News headline clusters are reclustered into finer grained sub-clusters.', 'We use the k-means implementation in the CLUTO1 software package.', 'The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space.', 'The total intra-cluster variances is minimized by the function k V = (xj − µi)2 i=1 xj ∈Si where µi is the centroid of all the points xj ∈ Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) − mean(C r[1...∆K ]) world knowledge.', 'Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...∆K ]) three headlines shown in the example.', 'We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters.', 'We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data.', 'The headlines are stemmed using the porter stemmer for Dutch (Kraaij and Pohlmann, 1994).', 'Instead of a word overlap measure as used byHere, C r is a criterion function, which mea sures the ratio of withincluster similarity to betweencluster similarity.', 'As soon as P K 1(k) ex ceeds a threshold, k − 1 is selected as the optimum number of clusters.', 'To find the optimal threshold value for cluster- stopping, optimization is performed on the development data.', 'Our optimization function is an F - score: (1 + β2) · (precision · recall) Barzilay and Elhadad (2003), we use a modified Fβ = (β2 precision + recall) T F ∗I DF word score as was suggested by Nelken · and Shieber (2006).', 'Each sentence is viewed as a 1 http://glaros.dtc.umn.edu/gkhome/views/cluto/ We evaluate the number of aligments between possible paraphrases.', 'For instance, in a cluster of four sentences, 4) = 6 alignments can be made.', 'In our case, precision is the number of alignments retrieved from the clusters which are relevant, divided by the total number of retrieved alignments.', 'Recall is the number of relevant retrieved aligments divided by the total number of relevant alignments.', 'We use an Fβ -score with a β of 0.25 as we favour precision over recall.', 'We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar.', 'Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1.', 'For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function.', 'In each newly obtained cluster all headlines can be aligned to each other.', '2.2 Pairwise similarity.', 'Our second approach is to calculate the similarity between pairs of headlines directly.', 'If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair.', 'If it is below the threshold, it is rejected.', 'However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent.', 'Beyond that point, context is needed.', 'With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(θ) = V 1 · V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared.', 'If the similarity is higher than the upper threshold, it is accepted.', 'If it is lower than the lower theshold, it is rejected.', 'In the remaining case of a similarity between the two thresholds, similarity is calculated over the contexts of the two headlines, namely the text snippet that was retrieved with the headline.', 'If this similarity exceeds the upper threshold, it is accepted.', 'Threshold values as found by optimizing on the development data using again an F0.25-score, are T hlower = 0.2 and T hupper = 0.5.', 'An optional final step is to add alignments that are implied by previous alignments.', 'For instance, if headline A is paired with headline B, and headline B is aligned to headline C , headline A can be aligned to C as Ty pe Precision Recallk m ea ns cl us ter in g 0.91 0.43 clu ste rs on lyk m ea ns cl us ter in g 0.66 0.44 all he ad lin es pa irw ise si mi lar ity 0.93 0.39 clu ste rs on ly pa irw ise si mi lar ity 0.76 0.41 all he ad lin es Table 2: Precision and Recall for both methods Pl ay st ati on 3 m or e ex pe nsi ve th an co m pe tit or P l a y s t a t i o n 3 w i l l b e c o m e m o r e e x p e n s i v e t h a n X b o x 3 6 0 So ny po stp on es Blu Ra y m ov ie s So ny po stp on es co mi ng of blu ra y dv ds Pri ce s Pl ay st ati on 3 kn ow n: fro m 49 9 eu ro s E3 20 06 : Pl ay st ati on 3 fro m 49 9 eu ro s So ny PS 3 wi th Blu R ay for sal e fro m No ve m be r 11 th PS 3 av ail abl e in Eu ro pe fro m No ve m be r 17 th Table 3: Examples of correct (above) and incorrect (below) alignments well.', 'We do not add these alignments, because in particular in large clusters when one wrong alignment is made, this process chains together a large amount of incorrect alignments.', 'The 825 clusters in the test set contain 1,751 sub- clusters in total.', 'In these sub-clusters, there are 6,685 clustered headlines.', 'Another 3,123 headlines remain unclustered.', 'Table 2 displays the paraphrase detection precision and recall of our two approaches.', 'It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored.', 'In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision.', 'Some examples of correct and incorrect alignments are given in Table 3.', 'Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision.', 'We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76.', 'Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases.', 'Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted.', 'This is not a bad thing: we are particularly interested in extracting paraphrase patterns at the constituent level.', 'These alignments can be made with existing alignment tools such as the GIZA++ toolkit.', 'We measure the performance of our approaches by comparing to human annotation of sub- clusterings.', 'The human task in itself is hard.', 'For instance, is we look at the incorrect examples in Table 3, the difficulty of distinguishing between paraphrases and non-paraphrases is apparent.', 'In future research we would like to investigate the task of judging paraphrases.', 'The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level.', 'This task has in fact been performed by human annotators in the DAESO-project.', 'A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way.', 'Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme.', 'Thanks also to Wauter Bosma for originally mining the headlines from Google News.', 'For more information on DAESO, please visit daeso.uvt.nl.']\n",
      "['We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system.', 'On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B\\uf76c\\uf765\\uf775 and+1.1 B\\uf76c\\uf765\\uf775, respectively.', 'We analyze the impact of the new features and the performance of the learning algorithm.', 'What linguistic features can improve statistical machine translation (MT)?', 'This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have.', 'Further: • Do syntax-based translation systems have unique and effective levers to pull when designing new features?', '• Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?', 'In this paper, we address these questions by experimenting with a large number of new features.', 'We add more than 250 features to improve a syntax- based MT system—already the highest-scoring single system in the NIST 2008 ChineseEnglish common-data track—by +1.1 B\\uf76c\\uf765\\uf775. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B\\uf76c\\uf765\\uf775 improvement.', '∗This research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies.', 'Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.', 'Thus they widen the advantage that syntax- based models have over other types of models.', 'The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).', 'Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.', 'The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality.', 'However, it had a few shortcomings.', 'First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).', 'Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for.', 'By contrast, we incorporate features directly into hierarchical and syntax- based decoders.', 'A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.', 'Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009.', 'Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example.', 'Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.', 'We follow this approach here.', 'minimal rules.', 'These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007).', 'We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule)', 'P(rule) =count(LHS root(rule)) 3.1 Hiero.', 'Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.', 'Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997).', 'The baseline model includes 12 features whose weights are optimized using MERT.', 'Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.', 'This grammar can be parsed efficiently using cube pruning (Chiang, 2007).', '3.2 Syntax-based system.', 'Our syntax-based system transforms source Chinese strings into target English syntax trees.', 'Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese.', 'We represent the translation model as a tree transducer (Knight and Graehl, 2005).', 'It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed.', 'From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) ↔ x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English).', 'We follow Galley et al.', '(2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c).', 'The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.', 'For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words.', 'All features are linearly combined and their weights are optimized using MERT.', 'For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006).', 'Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences.', 'We include two other techniques in our baseline.', 'To get more general translation rules, we restructure our English training trees using expectation- maximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al.', '(2006).', '3.3 MIRA training.', 'We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).', 'Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, . . .', ', fm and decode each fi to obtain a forest of translations.', '• For each i, select from the forest a set of hypothesis translations ei1, . . .', ', ein, which are the 10-best translations according to each of: h(e) · w B\\uf76c\\uf765\\uf775(e) + h(e) · w −B\\uf76c\\uf765\\uf775(e) + h(e) · w • For each i, select an oracle translation: (1) 4.1 Target-side.', 'features String-to-tree MT offers some unique levers to pull, in terms of target-side features.', 'Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.', 'e∗ = arg max (B\\uf76c\\uf765\\uf775(e) + h(e) · w) (2) e Let ∆hi j = h(e∗) − h(ei j).', '• For each ei j, compute the loss fi j = B\\uf76c\\uf765\\uf775(e∗) − B\\uf76c\\uf765\\uf775(ei j) (3) • Update w to the value of wt that minimizes: m Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems.', 'For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position.', 'If the second rule supplies the wrong preposition, a bad translation results.', 'The IN node here is an overlap point between rules.', 'Considering that 1 2 2 \\\\lw − w\\\\l + C i=1 max (fi j − ∆hi j · wt) (4) 1≤ j≤n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01.', 'This minimization is performed by a variant of sequential minimal optimization (Platt, 1998).', 'Following Chiang et al.', '(2008), we calculate the sentence B\\uf76c\\uf765\\uf775 scores in (1), (2), and (3) in the context of some previous 1-best translations.', 'We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.', 'Since the interface between the trainer and the decoder is fairly simple—for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update—it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.', 'In this section, we describe the new features introduced on top of our baseline systems.', 'Discount features Both of our systems calculate several features based on observed counts of rules in the training data.', 'Though the syntax-based system uses Good-Turing discounting when computing the P(e, c) feature, we find, as noted above, that it uses quite a few one-count rules, suggesting that their probabilities have been overestimated.', 'We can directly attack this problem by adding features counti that reward or punish rules seen i times, or features count[i, j] for rules seen between i and j times.', 'ture for each nonterminal.', 'A rule like: IN(at) ↔ zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0.', 'Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.', 'Even though the rule root features are locally attached to individual rules—and therefore cause no additional problems for the decoder search—they are aimed at problematic rule/rule interactions.', 'Bad single-level rewrites Sometimes the decoder uses questionable rules, for example: PP(x0:VBN x1:NP-C) ↔ x0 x1 This rule is learned from 62 cases in our training data, where the VBN is almost always the word given.', 'However, the decoder misuses this rule with other VBNs.', 'So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C.', 'The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP → VBN NP-C PP-BAR → NP-C IN VP → NP-C PP CONJP → RB IN Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category.', 'For example, there may be an tendency to generate too many determiners or past-tense verbs.', 'We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols.', 'For a rule like NPB(NNP(us) NNP(president) x0:NNP) ↔ meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0.', 'Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.', 'Sample syntax- based insertion rules are: NPB(DT(the) x0:NN) ↔ x0 S(x0:NP-C VP(VBZ(is) x1:VP-C)) ↔ x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source.', 'We also notice that the-insertion rules sometimes have a good effect, as in the translation “in the bloom of youth,” but other times have a bad effect, as in “people seek areas of the conspiracy.” Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk.', 'There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk.', 'We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features.', '4.2 Source-side features.', 'We now turn to features that make use of source-side context.', 'Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder.', 'This is because the entire source sentence, being fixed, is always available to every feature.', 'Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008).', 'In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents.', 'We use separately- tunable features for each syntactic category.', 'Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings.', 'To remedy this problem, Chiang et al.', '(2008) introduce a structural distortion model, which we include in our experiment.', 'Our syntax-based baseline includes the generative version of this model already.', 'Word context During rule extraction, we retain word alignments from the training data in the extracted rules.', '(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)', 'We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, f−1) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>.', 'These features are somewhat similar to features used by Watanabe et al.', '(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.', '(2007); here, we are incorporating some of its features directly into the translation model.', 'For our experiments, we used a 260 million word Chinese/English bitext.', 'We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.', 'For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9∗ 36.', '38.', '4 37.6∗∗ Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8∗ 38 .7 39.9∗ 38.', '39.', '6 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy.', 'Scores are case-insensitive IBM B\\uf76c\\uf765\\uf775 scores.', '∗ or ∗∗ = significantly better than MERT baseline ( p < 0.05 or 0.01, respectively).', 'the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2.', 'Syntax-based rule extraction was performed on a 65 million word subset of the training data.', 'For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data.', 'We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.', 'Modified KneserNey smoothing (Chen and Goodman, 1998) was applied to all language models.', 'The language models are represented using randomized data structures similar to those of Talbot et al.', '(2007).', 'Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level).', 'For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.', 'We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.', 'We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.', 'We chose a stopping iteration based on the B\\uf76c\\uf765\\uf775 score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3–5 −0.73 3 +0.54 6–10 −0.64 4 +0.29 5+ −0.02 Table 2: Weights learned for discount features.', 'Negative weights indicate bonuses; positive weights indicate penalties.', 'ations of all learners to decode the test set.', 'The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines.', 'Adding the source-side and discount features to Hiero yields a +1.5 B\\uf76c\\uf765\\uf775 improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 B\\uf76c\\uf765\\uf775 improvement.', 'The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.', '6 Analysis.', 'How did the various new features improve the translation quality of our two systems?', 'We begin by examining the discount features.', 'For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.', 'We see in both cases that one-count rules are strongly penalized, as expected.', 'Reward −0.42 a −0.13 are −0.09 at −0.09 on −0.05 was −0.05 from −0.04 ’s −0.04 by −0.04 is −0.03 it −0.03 its . Penalty +0.67 of +0.56 the +0.47 comma +0.13 period +0.11 in +0.08 for +0.06 to +0.05 will +0.04 and +0.02 as +0.02 have . Bonus −0.50 period −0.39 VP-C −0.36 VB −0.31 SG-C −0.30 MD −0.26 VBG −0.25 ADJP−0.22LRB −0.21 VP-BAR −0.20 NPB-BAR −0.16 FRAG P en alt y +0.93 IN +0.57 NNP +0.44 NN +0.41 DT +0.34 JJ +0.24 right double quote +0.20 VBZ +0.19 NP +0.16 TO +0.15 ADJP-BAR +0.14 PRN-BAR Table 3: Weights learned for inserting target English words with rules that lack Chinese words.', '6.1 Syntax features.', 'Table 3 shows word-insertion feature weights.', 'The system rewards insertion of forms of be; examples −0.16 PRN −0.15 NPB −0.13 RB −0.12 SBAR-C −0.12 VP-C-BAR−0.11RRB . +0.14 NML +0.13 comma +0.12 VBD +0.12 NNPS +0.12 PRP +0.11 SG . 1–3 in Figure 1 show typical improved translations that result.', 'Among determiners, inserting a is rewarded, while inserting the is punished.', 'This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules.', 'Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories.', 'these fixed phrases is a risk that the generative model is too inclined to take.', 'We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data.', 'Table 4 shows weights for rule-overlap features.', 'MIRA punishes the case where rules overlap with an IN (preposition) node.', 'This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it.', 'On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished.', 'The combined effect of all weights is subtle.', 'To interpret them further, it helps to look at gross Bonus −0.73 SBAR-C −0.54 VBZ −0.54 IN −0.52 NN −0.51 PP-C −0.47 right double quote −0.39 ADJP −0.34 POS −0.31 ADVP −0.30 RP −0.29 PRT −0.27 SG-C −0.22 S-C −0.21 NNPS −0.21 VP-BAR −0.20 PRP −0.20 NPB-BAR . Penalty +1.30 comma +0.80 DT +0.58 PP +0.44 TO +0.33 NNP +0.30 NNS +0.30 NML +0.22 CD +0.18 PRN +0.16 SYM +0.15 ADJP-BAR +0.15 NP +0.15 MD +0.15 HYPH +0.14 PRN-BAR +0.14 NP-C +0.11 ADJP-C . changes in the system’s behavior.', 'For example, a major error in the baseline system is to move “X said” or “X asked” from the beginning of the Chinese input to the middle or end of the English trans Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation.', 'lation.', 'The error occurs with many speaking verbs, and each time, we trace it to a different rule.', 'The problematic rules can even be non-lexical, e.g.: S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)', '↔ x3 x4 x2 x0 x1 x5 It is therefore difficult to come up with a straightforward feature to address the problem.', 'However, when we apply MIRA with the features already listed, these translation errors all disappear, as demon 38.5 38 37.5 37 36.5 36 35.5 35 Tune Test 0 5 10 15 20 25 Epoch strated by examples 4–5 in Figure 1.', 'Why does this happen?', 'It turns out that in translation hypotheses that move “X said” or “X asked” away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear.', 'Therefore, the new features work to discourage these hypotheses.', 'Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.', 'Examples 7–8 in Figure 1 show other kinds of unanticipated improvements.', 'We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall B\\uf76c\\uf765\\uf775 improvement.', '6.2 Word context features.', 'In Table 6 are shown feature weights learned for the word-context features.', 'A surprising number of the highest-weighted features have to do with translations of dates and bylines.', 'Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, ’s, said, parentheses, and quotes).', 'Finally, we note that several of the features (the third- and eighth-ranked reward and twelfth- ranked penalty) shape the translation of shuo ‘said’, preferring translations with an overt complementizer that and without a comma.', 'Thus these features work together to attack a frequent problem that our target- syntax features also addressed.', 'Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time.', 'The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data.', 'This seems in line with the finding of Watanabe et al.', '(2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data.', 'Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest.', 'Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights.', 'Early stopping would have given +0.2 B\\uf76c\\uf765\\uf775 over the results reported in Table 1.1 7 Conclusion.', 'We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems.', 'We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality.', 'We draw three conclusions from this study.', 'First, we have shown that these new features can improve the performance even of top-scoring MT systems.', 'Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training.', 'When training over 10,000 features on a modest amount of data, we, like Watanabe et al.', '(2007), did observe overfitting, yet saw improvements on new data.', 'Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.', '1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper.', '1 MERT: the united states pending israeli clarification on golan settlement plan.', 'MIRA: the united states is waiting for israeli clarification on golan settlement plan 2 MERT: . . .', 'the average life expectancy of only 18 months , canada ’s minority goverment will . . ..', 'MIRA: . . .', 'the average life expectancy of canada’s previous minority government is only 18 months . . .', '3 MERT: . . .', 'since un inspectors expelled by north korea . . ..', 'MIRA: . . .', 'since un inspectors were expelled by north korea . . .', '4 MERT: another thing is . . .', ', \" he said , \" obviously , the first thing we need to do . . .', 'MIRA: he said : \" obviously , the first thing we need to do . . .', ', and another thing is . . .', '\" 5 MERT: the actual timing . . .', 'reopened in january , yoon said ..', 'MIRA: yoon said the issue of the timing . . .', '6 MERT: . . .', 'us - led coalition forces , said today that the crash . . ..', 'MIRA: . . .', 'us - led coalition forces said today that a us military . . .', '7 MERT: . . .', 'and others will feel the danger ..', 'MIRA: . . .', 'and others will not feel the danger . 8 MERT: in residential or public activities within 200 meters of the region , . . ..', 'MIRA: within 200 m of residential or public activities area , . . .', 'Figure 1: Improved syntax-based translations due to MIRA-trained weights.', 'Bonus f e context −1.19 <unk> <unk> f−1 = ri ‘day’ −1.01 <unk> <unk> f−1 = ( −0.84 , that f−1 = shuo ‘say’ −0.82 yue ‘month’ <unk> f+1 = <unk> −0.78 \" \" f−1 = <unk> −0.76 \" \" f+1 = <unk> −0.66 <unk> <unk> f+1 = nian ‘year’ −0.65 , that f+1 = <unk> . P e n al ty f e context +1.12 <unk> ) f+1 = <unk> +0.83 jiang ‘shall’ be f+1 = <unk> +0.83 zhengfu ‘government’ the f−1 = <unk> +0.73 <unk> ) f−1 = <unk> +0.73 <unk> ( f+1 = <unk> +0.72 <unk> ) f−1 = ri ‘day’ +0.70 <unk> ( f−1 = ri ‘day’ +0.69 <unk> ( f−1 = <unk> +0.66 <unk> for f−1 = <unk> . +0.66 <unk> ’s f−1 = , +0.65 <unk> said f−1 = <unk> +0.60 , , f−1 = shuo ‘say’ . Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f−1 to the left or f+1 to the right.', 'Glosses for Chinese words are not part of features.']\n",
      "['We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', 'These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.', 'BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.', 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.', 'The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).', 'Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.', 'The focus of our work is on the use of contextual role knowledge for coreference resolution.', 'A contextual role represents the role that a noun phrase plays in an event or relationship.', 'Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.', 'Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.', '(b) After they were released...', '(c) After they blindfolded the men...', 'In (b) “they” refers to the kidnapping victims, but in (c) “they” refers to the armed men.', 'The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases.', 'The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released.', 'The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims.', 'We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.', 'BABAR employs information extraction techniques to represent and learn role relationships.', 'Each pattern represents the role that a noun phrase plays in the surrounding context.', 'BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.', 'Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.', 'BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.', 'In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned.', 'Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.', 'Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions.', 'Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters.', 'Our results show that BABAR achieves good performance in both domains, and that the contextual role knowledge improves performance, especially on pronouns.', 'Finally, Section 5 explains how BABAR relates to previous work, and Section 6 summarizes our conclusions.', 'In this section, we describe how contextual role knowledge is represented and learned.', 'Section 2.1 describes how BABAR generates training examples to use in the learning process.', 'We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.', 'Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples.', '2.1 Reliable Case Resolutions.', 'The first step in the learning process is to generate training examples consisting of anaphor/antecedent resolutions.', 'BABAR uses two methods to identify anaphors that can be easily and reliably resolved with their antecedent: lexical seeding and syntactic seeding.', '2.1.1 Lexical Seeding It is generally not safe to assume that multiple occurrences of a noun phrase refer to the same entity.', 'For example, the company may refer to Company X in one paragraph and Company Y in another.', 'However, lexically similar NPs usually refer to the same entity in two cases: proper names and existential noun phrases.', 'BABAR uses a named entity recognizer to identify proper names that refer to people and companies.', 'Proper names are assumed to be coreferent if they match exactly, or if they closely match based on a few heuristics.', 'For example, a person’s full name will match with just their last name (e.g., “George Bush” and “Bush”), and a company name will match with and without a corporate suffix (e.g., “IBM Corp.” and “IBM”).', 'Proper names that match are resolved with each other.', 'The second case involves existential noun phrases (Allen, 1995), which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse.', 'In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.', 'For example, a story can mention “the FBI”, “the White House”, or “the weather” without any prior referent in the story.', 'Although these existential NPs do not need a prior referent, they may occur multiple times in a document.', 'By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., “the FBI” always refers to the same entity).', 'Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.', 'Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases.', 'Words and punctuation that appear in brackets are considered optional.', 'The anaphor and antecedent appear in boldface.', '1.', 'Reflexive pronouns with only 1 NP in scope..', 'Ex: The regime gives itself the right...', '2.', 'Relative pronouns with only 1 NP in scope..', 'Ex: The brigade, which attacked ...', 'Ex: Mr. Cristiani is the president ...', 'Ex: The government said it ...', 'Ex: He was found in San Jose, where ...', 'Ex: Mr. Cristiani, president of the country ...', 'Ex: Mr. Bush disclosed the policy by reading it...', 'Table 1: Syntactic Seeding Heuristics BABAR’s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.', 'For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.', 'For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.', '2.2 Contextual Role Knowledge.', 'Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.', 'First, we describe how the caseframes are represented and learned.', 'Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.', '2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too.', 'an event.', 'For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event.', 'For example, management succession systems must distinguish between a person who is fired and a person who is hired.', 'Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime.', 'We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.', 'Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.', 'For example, kidnapping victims should be extracted from the subject of the verb “kidnapped” when it occurs in the passive voice (the shorthand representation of this pattern would be “<subject> were kidnapped”).', 'The types of patterns produced by AutoSlog are outlined in (Riloff, 1996).', 'Ideally we’d like to know the thematic role of each extracted noun phrase, but AutoSlog does not generate thematic roles.', 'As a (crude) approximation, we normalize the extraction patterns with respect to active and passive voice and label those extractions as agents or patients.', 'For example, the passive voice pattern “<subject> were kidnapped” and the active voice pattern “kidnapped <direct object>” are merged into a single normalized pattern “kidnapped <patient>”.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, “<agent> kidnapped” or “kidnapped <patient>”), and (2) predicate-argument relations associated with both verbs and nouns (e.g., “kidnapped for <np>” or “vehicle with <np>”).', 'We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus.', 'The learned patterns are then normalized and applied to the corpus.', 'This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted.', 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', '2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.', 'Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.', '3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.', 'conceptual relationship in the discourse.', 'For example, co-occurring caseframes may reflect synonymy (e.g., “<patient> kidnapped” and “<patient> abducted”) or related events (e.g., “<patient> kidnapped” and “<patient> released”).', 'We do not attempt to identify the types of relationships that are found.', 'BABAR merely identifies caseframes that frequently co-occur in coreference resolutions.', 'Te rro ris m Na tur al Dis ast ers mu rde r of < NP > kill ed <p atie nt > <a ge nt > da ma ged wa s inj ure d in < NP > <a ge nt > rep ort ed <a ge nt > add ed <a ge nt > occ urr ed cau se of < NP > <a ge nt > stat ed <a ge nt > add ed <a ge nt > wr eak ed <a ge nt > cro sse d per pet rat ed <p atie nt > con de mn ed <p atie nt > dri ver of < NP > <a ge nt > car ryi ng Figure 1: Caseframe Network Examples Figure 1 shows examples of caseframes that co-occur in resolutions, both in the terrorism and natural disaster domains.', 'The terrorism examples reflect fairly obvious relationships: people who are murdered are killed; agents that “report” things also “add” and “state” things; crimes that are “perpetrated” are often later “condemned”.', 'In the natural disasters domain, agents are often forces of nature, such as hurricanes or wildfires.', 'Figure 1 reveals that an event that “damaged” objects may also cause injuries; a disaster that “occurred” may be investigated to find its “cause”; a disaster may “wreak” havoc as it “crosses” geographic regions; and vehicles that have a “driver” may also “carry” items.', 'During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent.', 'Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence.', 'For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphor’s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.', 'If so, the CF Network reports that the anaphor and candidate may be coreferent.', '2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.', 'For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.', 'For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NP’s caseframe.', 'For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame.', 'Figure 2 shows examples of lexical expectations that were learned for both domains.', 'collected too.', 'We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe.', 'Figure 3 shows examples of semantic expectations that were learned.', 'For example, BABAR learned that agents that “assassinate” or “investigate a cause” are usually humans or groups (i.e., organizations).', 'T e r r o r i s m Ca sef ra me Semantic Classes <a ge nt > ass ass ina ted group, human inv esti gat ion int o < N P> event exp lod ed out sid e < N P> building N a t u r a l D i s a s t e r s Ca sef ra me Semantic Classes <a ge nt > inv esti gat ing cau se group, human sur viv or of < N P> event, natphenom hit wit h < N P> attribute, natphenom Figure 3: Semantic Caseframe Expectations Figure 2: Lexical Caseframe Expectations To illustrate how lexical expectations are used, suppose we want to determine whether noun phrase X is the antecedent for noun phrase Y. If they are coreferent, then X and Y should be substitutable for one another in the story.4 Consider these sentences: (S1) Fred was killed by a masked man with a revolver.', '(S2) The burglar fired the gun three times and fled.', '“The gun” will be extracted by the caseframe “fired <patient>”.', 'Its correct antecedent is “a revolver”, which is extracted by the caseframe “killed with <NP>”.', 'If “gun” and “revolver” refer to the same object, then it should also be acceptable to say that Fred was “killed with a gun” and that the burglar “fireda revolver”.', 'During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.', 'If either case is true, then CFLex reports that the anaphor and candidate might be coreferent.', '2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations.', 'Semantic expectations are analogous to lexical expectations except that they represent semantic classes rather than nouns.', 'For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.', 'As with lexical expections, the semantic classes of co-referring expressions are 4 They may not be perfectly substitutable, for example one NP may be more specific (e.g., “he” vs. “John F. Kennedy”).', 'But in most cases they can be used interchangably.', 'For each domain, we created a semantic dictionary by doing two things.', 'First, we parsed the training corpus, collected all the noun phrases, and looked up each head noun in WordNet (Miller, 1990).', 'We tagged each noun with the top-level semantic classes assigned to it in Word- Net.', 'Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags.', 'This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain.', 'Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected.', 'However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse.', 'For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.', 'So we decided to use semantic class information only to rule out candidates.', 'If two nouns have mutually exclusive semantic classes, then they cannot be coreferent.', 'This solution also obviates the need to perform word sense disambiguation.', 'Each word is simply tagged with the semantic classes corresponding to all of its senses.', 'If these sets do not overlap, then the words cannot be coreferent.', 'The semantic caseframe expectations are used in two ways.', 'One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.', 'Given an anaphor and candidate, BABAR checks (1) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate, and (2) whether the semantic classes of the candidate intersect with the semantic ex pectations of the caseframe that extracts the anaphor.', 'If one of these checks fails then this knowledge source reports that the candidate is not a viable antecedent for the anaphor.', 'A different knowledge source, called CFSemCFSem, compares the semantic expectations of the caseframe that extracts the anaphor with the semantic expectations of the caseframe that extracts the candidate.', 'If the semantic expectations do not intersect, then we know that the case- frames extract mutually exclusive types of noun phrases.', 'In this case, this knowledge source reports that the candidate is not a viable antecedent for the anaphor.', '2.3 Assigning Evidence Values.', 'Contextual role knowledge provides evidence as to whether a candidate is a plausible antecedent for an anaphor.', 'The two knowledge sources that use semantic expectations, WordSemCFSem and CFSemCFSem, always return values of -1 or 0.', '-1 means that an NP should be ruled out as a possible antecedent, and 0 means that the knowledge source remains neutral (i.e., it has no reason to believe that they cannot be coreferent).', 'The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent.', 'They return a value in the range [0,1], where 0 indicates neutrality and 1 indicates the strongest belief that the candidate and anaphor are coreferent.', 'BABAR uses the log-likelihood statistic (Dunning, 1993) to evaluate the strength of a co-occurrence relationship.', 'For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the χ2 table to obtain a confidence level.', 'The confidence level is then used as the belief value for the knowledge source.', 'For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent.', '3 The Coreference Resolution Model.', 'Given a document to process, BABAR uses four modules to perform coreference resolution.', 'First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process.', 'Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.', 'Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.', 'Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.', 'In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.', '3.1 General Knowledge Sources.', 'Figure 4 shows the seven general knowledge sources (KSs) that represent features commonly used for coreference resolution.', 'The gender, number, and scoping KSs eliminate candidates from consideration.', 'The scoping heuristics are based on the anaphor type: for reflexive pronouns the scope is the current clause, for relative pronouns it is the prior clause following its VP, for personal pronouns it is the anaphor’s sentence and two preceding sentences, and for definite NPs it is the anaphor’s sentence and eight preceding sentences.', 'The semantic agreement KS eliminates some candidates, but also provides positive evidence in one case: if the candidate and anaphor both have semantic tags human, company, date, or location that were assigned via NER or the manually labeled dictionary entries.', 'The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy).', 'KS Function Ge nde r filters candidate if gender doesn’t agree.', 'Nu mb er filters candidate if number doesn’t agree.', 'Sc opi ng filters candidate if outside the anaphor’s scope.', 'Se ma nti c (a) filters candidate if its semantic tags d o n ’ t i n t e r s e c t w i t h t h o s e o f t h e a n a p h o r .', '( b ) s u p p o r t s c a n d i d a t e i f s e l e c t e d s e m a n t i c t a g s m a t c h t h o s e o f t h e a n a p h o r . Le xic al computes degree of lexical overlap b e t w e e n t h e c a n d i d a t e a n d t h e a n a p h o r . Re cen cy computes the relative distance between the c a n d i d a t e a n d t h e a n a p h o r . Sy nR ole computes relative frequency with which the c a n d i d a t e ’ s s y n t a c t i c r o l e o c c u r s i n r e s o l u t i o n s . Figure 4: General Knowledge Sources The Lexical KS returns 1 if the candidate and anaphor are identical, 0.5 if their head nouns match, and 0 otherwise.', 'The Recency KS computes the distance between the candidate and the anaphor relative to its scope.', 'The SynRole KS computes the relative frequency with which the candidates’ syntactic role (subject, direct object, PP object) appeared in resolutions in the training set.', 'During development, we sensed that the Recency and Syn- role KSs did not deserve to be on equal footing with the other KSs because their knowledge was so general.', 'Consequently, we cut their evidence values in half to lessen their influence.', '3.2 The DempsterShafer Decision Model.', 'BABAR uses a DempsterShafer decision model (Stefik, 1995) to combine the evidence provided by the knowledge sources.', 'Our motivation for using DempsterShafer is that it provides a well-principled framework for combining evidence from multiple sources with respect to competing hypotheses.', 'In our situation, the competing hypotheses are the possible antecedents for an anaphor.', 'An important aspect of the DempsterShafer model is that it operates on sets of hypotheses.', 'If evidence indicates that hypotheses C and D are less likely than hypotheses A and B, then probabilities are redistributed to reflect the fact that {A, B} is more likely to contain the answer than {C, D}.', 'The ability to redistribute belief values across sets rather than individual hypotheses is key.', 'The evidence may not say anything about whether A is more likely than B, only that C and D are not likely.', 'Each set is assigned two values: belief and plausibility.', 'Initially, the DempsterShafer model assumes that all hypotheses are equally likely, so it creates a set called θ that includes all hypotheses.', 'θ has a belief value of 1.0, indicating complete certainty that the correct hypothesis is included in the set, and a plausibility value of 1.0, indicating that there is no evidence for competing hypotheses.5 As evidence is collected and the likely hypotheses are whittled down, belief is redistributed to subsets of θ.', 'Formally, the DempsterShafer theory defines a probability density function m(S), where S is a set of hypotheses.', 'm(S) represents the belief that the correct hypothesis is included in S. The model assumes that evidence also arrives as a probability density function (pdf) over sets of hypotheses.6 Integrating new evidence into the existing model is therefore simply a matter of defining a function to merge pdfs, one representing the current belief system and one representing the beliefs of the new evidence.', 'The DempsterShafer rule for combining pdfs is: to {C}, meaning that it is 70% sure the correct hypothesis is C. The intersection of these sets is the null set because these beliefs are contradictory.', 'The belief value that would have been assigned to the intersection of these sets is .60*.70=.42, but this belief has nowhere to go because the null set is not permissible in the model.7 So this probability mass (.42) has to be redistributed.', 'DempsterShafer handles this by re-normalizing all the belief values with respect to only the non-null sets (this is the purpose of the denominator in Equation 1).', 'In our coreference resolver, we define θ to be the set of all candidate antecedents for an anaphor.', 'Each knowledge source then assigns a probability estimate to each candidate, which represents its belief that the candidate is the antecedent for the anaphor.', 'The probabilities are incorporated into the DempsterShafer model using Equation 1.', 'To resolve the anaphor, we survey the final belief values assigned to each candidate’s singleton set.', 'If a candidate has a belief value ≥ .50, then we select that candidate as the antecedent for the anaphor.', 'If no candidate satisfies this condition (which is often the case), then the anaphor is left unresolved.', 'One of the strengths of the DempsterShafer model is its natural ability to recognize when several credible hypotheses are still in play.', 'In this situation, BABAR takes the conservative approach and declines to make a resolution.', '4 Evaluation Results.', '4.1 Corpora.', 'We evaluated BABAR on two domains: terrorism and natural disasters.', 'We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuter’s text collection8 that had a subject code corresponding to natural disasters.', 'For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X ∩Y =S 1 − ) m1 (X ) ∗ m2 (Y ) m1 (X ) ∗ m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).', 'In the terrorism domain, 1600 texts were used for training and the 40 test docu X ∩Y =∅ All sets of hypotheses (and their corresponding belief values) in the current model are crossed with the sets of hypotheses (and belief values) provided by the new evidence.', 'Sometimes, however, these beliefs can be contradictory.', 'For example, suppose the current model assigns a belief value of .60 to {A, B}, meaning that it is 60% sure that the correct hypothesis is either A or B. Then new evidence arrives with a belief value of .70 assigned 5 Initially there are no competing hypotheses because all hypotheses are included in θ by definition.', '6 Our knowledge sources return some sort of probability estimate, although in some cases this estimate is not especially well-principled (e.g., the Recency KS).', 'ments contained 322 anaphoric links.', 'For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links.', 'In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998).', 'We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships.', 'Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in θ is correct, so eliminating all of the hypotheses violates this assumption.', '8 Volume 1, English language, 19961997, Format version 1, correction level 0 An ap ho r T e r r o r i s m R e c Pr F D i s a s t e r s R e c Pr F De f. NP s Pro no uns .43 .79 .55 .50 .72 .59 .42 .91 .58 .42 .82 .56 Tot al .46 .76 .57 .42 .87 .57 Table 2: General Knowledge Sources Table 4: Individual Performance of KSs for Terrorism Table 3: General + Contextual Role Knowledge Sources larger MUC4 and Reuters corpora.9 4.2 Experiments.', 'We adopted the MUC6 guidelines for evaluating coreference relationships based on transitivity in anaphoric chains.', 'For example, if {N P1, N P2, N P3} are all coreferent, then each NP must be linked to one of the other two NPs.', 'First, we evaluated BABAR using only the seven general knowledge sources.', 'Table 2 shows BABAR’s performance.', 'We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted.', 'BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.', 'We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus.', 'Table 3 shows BABAR’s performance when the four contextual role knowledge sources are added.', 'The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.', 'The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.', 'The difference in performance between pronouns and definite noun phrases surprised us.', 'Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.', 'Since pronouns carry little semantics of their own, resolving them depends almost entirely on context.', 'In contrast, even though context can be helpful for resolving definite NPs, context can be trumped by the semantics of the nouns themselves.', 'For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections.', 'Table 5: Individual Performance of KSs for Disasters (e.g., “the mayor” vs. “the journalist”).', 'We also performed experiments to evaluate the impact of each type of contextual role knowledge separately.', 'Tables 4 and 5 show BABAR’s performance when just one contextual role knowledge source is used at a time.', 'For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision.', 'For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision.', 'This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.', 'Tables 4 and 5 also show that putting all of the contextual role KSs in play at the same time produces the greatest performance gain.', 'There are two possible reasons: (1) the knowledge sources are resolving different cases of anaphora, and (2) the knowledge sources provide multiple pieces of evidence in support of (or against) a candidate, thereby acting synergistically to push the DempsterShafer model over the belief threshold in favor of a single candidate.', '5 Related Work.', 'Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR.', 'Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations.', 'Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate.', 'However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.', 'The learned information was recycled back into the resolver to improve its performance.', 'This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.', '(Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference.', 'Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).', 'These systems rely on a training corpus that has been manually annotated with coreference links.', '6 Conclusions.', 'The goal of our research was to explore the use of contextual role knowledge for coreference resolution.', 'We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability.', 'We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.', 'Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.', 'We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases.', 'This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.', 'In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.', '7 Acknowledgements.', 'This work was supported in part by the National Science Foundation under grant IRI9704240.', 'The inventions disclosed herein are the subject of a patent application owned by the University of Utah and licensed on an exclusive basis to Attensity Corporation.']\n",
      "['A method is presented for segmenting text into subtopic areas.', 'The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity.', 'The lexical cohesion relations of reiteration and collocation are used to identify related words.', 'These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.', 'This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.', 'Many examples of heterogeneous data can be found in daily life.', 'The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.', \"Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.\", 'Text segmentation could also be used as a pre-processing step in automatic summarisation.', 'Each segment could be summarised individually and then combined to provide an abstract for a document.', 'Previous work on text segmentation has used term matching to identify clusters of related text.', 'Salton and Buckley (1992) and later, Hearst (1994) extracted related text pmtions by matching high frequency terms.', 'Yaari ( 1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments.', 'Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching.', 'Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours.', 'A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994).', 'Another approach to text segmentation is the detection of semantically related words.', 'Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994).', 'Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented.', \"Another approach extracted semantic information from Roget's Thesaurus (RT).\", 'Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ).', 'It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges.', 'However, RT does not capture all types of lexical cohesion relations.', 'In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus.', 'Furthermore, this process was not automated and relied on subjective decision making.', \"Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.\", 'The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated.', 'This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.', '1 Background Theory: Lexical Cohesion.', 'Cohesion concerns how words in a text are related.', 'The major work on cohesion in English was conducted by Halliday and Hasan (1976).', 'An instance of cohesion between a pair of elements is referred to as a tie.', 'Ties can be anaphoric or cataphoric, and located at both the sentential and suprasentential level.', 'Halliday and Hasan classified cohesion under two types: grammatical and lexical.', 'Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction.', 'Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words.', 'Identifying semantic relations in a text can be a useful indicator of its conceptual structure.', 'Lexical cohesion is divided into three classes: general noun, reiteration and collocation.', \"General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role.\", 'Consequently, it was not further considered.', 'Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing).', 'The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers.', 'A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel).', 'All semantic relations not classified under the class of reiteration are attributed to the class of collocation.', 'To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights.', 'The first two methods represent lexical cohesion relations.', 'Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety.', 'The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included).', 'These types can be identified using relation weights (Jobbins and Evett, 1998).', 'Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.', 'An inflected word was reduced to its stem by look\\xad up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. \"orange oranges\").', 'Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.', 'Collocations were automatically located in a text by looking up pairwise words in this lexicon.', 'Figure 1 shows the record for the headword orange followed by its collocates.', 'For example, the pairwise words orange and peel form a collocation.', 'orange free green lemon peel red state yellow Figure 1.', 'Excerpt from the collocation lexicon.', 'Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995).', 'A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured.', 'An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex).', 'Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.', 'The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity.', 'A window size of three sentences was found to produce the best results.', 'Multiple sentences were compared because calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991).', 'Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score.', 'Word repetitions are identified between identical words and words derived from the same stem.', 'troughs placed subject change linguistic feature points located average std.', 'dev.', '(out of 42 poss.)', 'word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) 41 Collocations are located by looking up word pairs in the collocation lexicon.', 'Relation weights are word repetition 8.5 3.62 (97.6%) calculated between pairwise words according to their location in RT.', 'The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows.', 'Scores plotted on a graph show a series of peaks (high scores) and troughs (low scores).', 'Low scores indicate a weak collocation 5.8 3.70 40 relation weights (95.2%) word repetition 40 collocation 6.4 4.72 (95.2%) relation weights 39 level of cohesion.', 'Hence, a trough signals a potential subject change and texts can be relation weights 7 4.23 (92.9%) segmented at these points.', 'An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text.', 'Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web.', 'A total of 42 texts for test data were generated by concatenating pairs of these articles.', 'Hence, each generated text consisted of two articles.', 'The transition from the first article to the second represented a known subject change point.', 'Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms (Reynar, 1994; Stairmand, 1997).', 'For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text.', 'An error margin of one sentence either side of this point, determined by empirical analysis, was allowed.', 'Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points.', 'collocation 6.3 3.83 35 (83.3%) Table 1.', 'Comparison of segmentation algorithm using different linguistic features.', 'Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result.', 'A total of 41 out of a possible 42 known subject change points were identified from the least number of troughs placed per text (7.I).', 'For the text where the known subject change point went undetected, a total of three troughs were placed at sentences 6, 11 and 18.', 'The subject change point occurred at sentence 13, just two sentences after a predicted subject change at sentence 11.', 'In this investigation, word repetition alone achieved better results than using either collocation or relation weights individually.', 'The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text.', 'The objective of the current investigation was to determine whether all troughs coincide with a subject change.', 'The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts.', 'Method: Twenty texts were randomly selected for test data each consisting of approximately 500 words.', 'These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced.', 'No restriction was placed on the number of subject changes that could be identified.', 'Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman ami Passonneau, 1996).', 'Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change.', 'The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects.', 'In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points.', 'Due to that success, these five approaches were applied in this experiment.', 'To evaluate the results, the information retrieval metrics precision and recall were used.', 'These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997).', \"The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered.\", 'Allowing an error margin provides some degree of flexibility.', 'An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar ( 1994) allowed three sentences.', 'In this investigation, an error margin of two sentences was considered.', 'Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts.', 'Discussion: The segmentation algorithm usmg word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively.', 'For 9 out of the 20 texts segmented, all troughs were relevant.', 'Therefore, many of the troughs placed by the segmentation algorithm represented valid subject Table 2.', 'Comparison of troughs to segmentation points placed by the test subjects.', 'changes.', 'Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62.', 'These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation.', 'As an example, a text segmentation algorithm developed by Hearst ( 1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61.', 'In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5).', 'Each text was only 500 words in length and was related to a specific subject area.', 'These factors limited the degree of subject change that occurred.', 'Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect.', 'Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows.', 'The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69.', 'When used in isolation, the performance of each feature was inferior to a combined approach.', 'This fact provides evidence that different lexical relations are detected by each linguistic feature considered.', 'Areas for improving the segmentation algorithm include incorporation of a threshold for troughs.', 'Currently, all troughs indicate a subject change, however, minor fluctuations in scores may be discounted.', 'Future work with this algorithm should include application to longer documents.', 'With trough thresholding the segments identified in longer documents could detect significant subject changes.', 'Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.']\n",
      "['Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', 'Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.', 'The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', 'The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic.', 'Boxer’s performance on the shared task for comparing semantic represtations was promising.', 'It was able to produce complete DRSs for all seven texts.', 'Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren’t; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases.', 'Boxer is distributed with the C&C tools and freely available for research purposes.', '277', 'Boxer is an open-domain tool for computing and reasoning with semantic representations.', 'Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called “boxes” because of the way they are graphically displayed) for English sentences and texts.', 'There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).', '2.1 Combinatory Categorial Grammar.', 'As a preliminary to semantics, we need syntax.', 'Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001).', 'CCG lends itself extremely well for this task because it is lexically driven and has only few “grammar” rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression).', 'Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.', 'Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001).', 'We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.', '2.2 Discourse Representation Theory.', 'DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations (Kamp, 1981).', 'DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition.', 'We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1.', 'Discourse Representation Structures (DRSs).', '2.', 'Underspecified DRSs (DRSs + merge + alfa).', '3.', 'λ-DRSs (UDRSs + lambda + application) DRSs are the representations corresponding to natural language sentences or texts.', 'This is the core DRT language compatible with first-order logic.', 'The DRS language employed by Boxer is a subset of the one found in Kamp and Reyle (1993).', 'We define the syntax of DRSs below with the help of BackusNaur form, where non-terminal symbols are enclosed in angle brackets.', 'The non-terminal <ref> denotes a discourse referent, and <symn> an n-place predicate symbol.', '<expe > ::= <ref> <expt > ::= <drs> <ref>∗ <drs> ::= <condition>∗ <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >⇒<expt > | <expt >∨<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents.', 'Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs.', 'The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations.', 'The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989).', 'There is only one way to state that an individual is participating in an event—namely by relating it to the event using a binary relation expressing some thematic role.', 'Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear.', 'Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event.', 'We won’t show the standard translation from DRS to FOL here (Blackburn et al., 2001; Bos, 2004; Kamp and Reyle, 1993).', 'Intuitively, translating DRSs into first-order formulas proceeds as follows: each discourse referent is translated as a first-order quantifier, and all DRS-conditions are translated into a conjunctive formula of FOL.', 'Discourse referents usually are translated to existential quantifiers, with the exception of those declared in antecedents of implicational DRS-conditions, that are translated as universal quantifiers.', 'Obviously, negated DRSs are translated as negated formulas, disjunctive DRSs as disjunctive formulas, and implicational DRSs as formulas with material implication.', 'Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation.', 'This level of representation is referred to as underspecified DRS, or UDRS for short.', 'It is a small extension of the DRS language given in the previous section and is defined as follows: <expt > ::= <udrs> <udrs> ::= <drs> | (<expt >;<expt >) | (<expt >α<expt >) Note here that expressions of type t are redefined as UDRSs.', 'UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date λq.λp.( x ;q@x;p@x) λp.λx.( y record(y) nn(y,x) ;p@x) λx. date(x) [fa] N: record date y λx.( record(y) nn(y,x) ; ) date(x) . . .', '[merge] y λx. record(y) nn(y,x) date(x) [fa] NP: A record date y λp.( x ; record(y) nn(y,x) date(x) ;p@x) . . .', '[merge] x y λp. record(y) nn(y,x) date(x) ;p@x Figure 1: Derivation with λ-DRSs, including β-conversion, for “A record date”.', 'Combinatory rules are indicated by solid lines, semantic rules by dotted lines.', 'DRS composed by the α-operator.', 'The merge conjoins two DRSs into a larger DRS — semantically the merge is interpretated as (dynamic) logical conjunction.', 'Merge- reduction is the process of eliminating the merge operation by forming a new DRS resulting from the union of the domains and conditions of the argument DRSso of a merge, respectively (obeying certain constraints).', 'Figure 1 illustrates the syntax- semantics interface (and merge-reduction) for a derivation of a simple noun phrase.', 'Boxer adopts Van der Sandt’s view as presupposition as anaphora (Van der Sandt, 1992), in which presuppositional expressions are either resolved to previously established discourse entities or accommodated on a suitable level of discourse.', 'Van der Sandt’s proposal is cast in DRT, and therefore relatively easy to integrate in Boxer’s semantic formalism.', 'The α-operator indicates information that has to be resolved in the context, and is lexically introduced by anaphoric or presuppositional expressions.', 'A DRS constructed with α resembles the protoDRS of Van der Sandt’s theory of presupposition (Van der Sandt, 1992) although they are syntactically defined in a slightly different way to overcome problems with free and bound variables, following Bos (2003).', 'Note that the difference between anaphora and presupposition collapses in Van der Sandt’s theory.', 'The types are the ingredients of a typed lambda calculus that is employed to construct DRSs in a bottom-up fashion, compositional way.', 'The language of lambda DRSs is an extension of the language of (U)DRS defined before: <expe > ::= <ref> | <vare > <expt > ::= <udrs> | <vart > <expα > ::= (<exp(β,α)> @ <varβ >) | <varα > <exp(α,β)> ::= λ<varα >.<expβ > | <var(α,β)> Hence we define discourse referents as expressions of type e, and DRSs as expressions of type t . We use @ to indicate function application, and the λ-operator to bind free variables over which we wish to abstract.', '3.1 Preprocessing.', 'The input text needs to be tokenised with one sentence per line.', 'In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).', 'The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents.', 'An example of a CCG derivation is shown in Figure 2.', 'a virus --[lex] --[lex] by np:nb/n n ---------------------[lex] -----------[fa] Cervical cancer caused ((s:pss\\\\np)\\\\(s:pss\\\\np))/np np:nb ---[lex] --[lex] ---[lex] --------------------------------------[fa] n/n n is s:pss\\\\np (s:pss\\\\np)\\\\(s:pss\\\\np) ------------[fa] ----------------[lex] -----------------------------------------------[ba] n (s:dcl\\\\np)/(s:pss\\\\np) s:pss\\\\np ------------[tc] ---------------------------------------------------------------------[fa] np s:dcl\\\\np --------------------------------------------------------------------------------------[ba] s:dcl Figure 2: CCG derivation as generated by the C&C tools 3.2 Lexicon.', 'In CCG, the syntactic lexicon comprises the set of lexical categories.', 'CCGbank hosts more than a thousand different categories.', 'The semantic lexicon defines a suitable mapping from categories to semantic representations.', 'In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.', 'Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency.', 'Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations.', 'So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category.', 'For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category.', 'Although there is a one-to-one mapping between the CCG categories and semantic types — and this must be the case to ensure the semantic composition process proceeds without type clashes — the actual instantiations of a semantic type can differ even within the scope of a single CCG category.', 'For example, the category n/n can correspond to an adjective, a cardinal expression, or even common nouns and proper names (in the compound expressions).', 'In the latter two cases the lexical entry introduces a new discourse referent, in the former two it does not.', 'To account for this difference we also need to look at the part of speech that is assigned to a token.', '3.3 Resolution.', 'Boxer implements various presupposition triggers introduced by noun phrases, including personal pronouns, possessive pronouns, reflexive pronouns, emphasising pronouns, demonstrative pronouns, proper names, other-anaphora, definite descriptions.', 'In addition, some aspects of tense are implemented as presupposition triggers, too.', 'Anaphora and presupposition resolution takes place in a separate stage after building up the representation, following the resolution algorithm outlined in Bos (2003).', 'The current implementation of Boxer aims at high precision in resolution: personal pronouns are only attempted to be resolved to named entities, definite descriptions and proper names are only linked to previous discourse referents if there is overlap in the DRS-conditions of the antencedent DRS and alpha-DRS.', 'If no suitable antecedent can be found, global accommodation of the anaphoric discourse referent and conditions will take palce.', 'Because Boxer has the option to output unresolved DRSs too, it is possible to include external anaphora or coreference resolution components.', '3.4 Example Analysis.', 'We illustrate the capabilities of Boxer with the following example text shown below (aka as Text 2 of the shared task).1 The text consists of three sentences, the second being a coordinated sentence.', 'It contains a passive construction, three pronouns, relative clauses, control verbs, and a presupposition trigger other.', 'Text 2 Cervical cancer is caused by a virus.', 'That has been known for some time and it has led to a vaccine that seems to prevent it.', 'Researchers have been looking for other cancers that may be caused by viruses.', 'The output of Boxer for this text is shown in Figure 3.', 'Only the box format is shown here — Boxer is also able to output the DRSs in Prolog or XML encodings.', 'It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here).', 'As we can see from the example and Boxer’s analysis various things go right and various things go wrong.', 'Boxer deals fine with the passive construction (assigned the 1 This text was taken from the Economist Volume 387 Number 8582, page 92.', 'The third sentence has been simplified.', 'appropriate semantic role), the relative clauses, and the control construction (vaccine is the agent of the prevent event).', 'It also handles the presupposition trigger anaphorically linking the mention of other cancers in the third sentence with the phrase cervical cancer in the first sentence, and asserting an inequality condition in the DRS.', 'Boxer failed to resolve three pronouns correctly.', 'These are all accommodated at the global level of DRS, which is the DRS on the left-hand side in Figure 3.', 'All of the pronouns have textual antecedents: the abstract pronoun that in the second sentence refers to the fact declared in the first sentence.', 'The first occurrence of it in the second sentence also seems to refer to this fact — the second occurrence of it refers to cervical cancer mentioned in the first sentence.', 'bin/boxer --input working/step/text2.ccg --semantics drs --box --resolve --roles verbnet --format no %%% %%% | x0 x1 x2 | | x3 x4 x5 | | x6 x7 | | x8 x9 x10 x11 | | x13 x14 x15 x16 x17 | %%% |------------| |--------------| |--------------| |------------------------| |---------------------| %%% (| thing(x0) |+(| cancer(x3) |+(| know(x6) |+(| lead(x8) |+| researcher(x13) |)))) %%% | neuter(x1) | | cervical(x3) | | time(x7) | | vaccine(x9) | | look(x14) | %%% | neuter(x2) | | cause(x4) | | event(x6) | | seem(x10) | | agent(x14,x13) | %%% | | | virus(x5) | | theme(x6,x0) | | proposition(x11) | | cancer(x15) | %%% | event(x4) | | for(x6,x7) | | event(x10) | | | %%% | theme(x4,x3) | | | | event(x8) | | | | | %%% | by(x4,x5) | | agent(x8,x1) | | |----------| | %%% | | | agent(x10,x9) | | | | x15 = x3 | | %%% | theme(x10,x11) | | | | | %%% | to(x8,x9) | | cause(x16) | %%% | | | virus(x17) | %%% | | x12 | | | event(x16) | %%% | x11:|---------------| | | theme(x16,x15) | %%% | | prevent(x12) | | | by(x16,x17) | %%% | | event(x12) | | | for(x14,x15) | %%% | | agent(x12,x9) | | | event(x14) | %%% | | theme(x12,x2) | | | | %%% | | | | %%% | | Attempted: 3.', 'Completed: 3 (100.00%).', 'Figure 3: Boxer output for Shared Task Text 2', 'Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).', 'Boxer was able to produce semantic representation for all text without any further modifications to the software.', 'For each text we briefly say what was good and bad about Boxer’s analysis.', '(We won’t comment on the performance on the second text, as this is the text proposed by ourselves and already discussed in the previous section.)', 'Text 1: An object is thrown with a horizontal speed ...', 'Good: The resulting predicate argument structure was fine overall, including a difficult control construction (“how long does it take the object to fall ...”).', 'The definite description “the object” was correctly resolved.', 'The conditional got correctly anal- ysed.', 'Bad: The measure phrase “125 m high” got misinterpreted as noun-noun comn- pound.', 'The definite description “the fall” was not linked to the falling event mentioned before.', 'Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions.', 'Text 3: John went into a restaurant ...', 'Good: The pronouns were correctly resolved to the proper name “John” rather than “the waiter”, even though this is based on the simple strategy in Boxer to link third- person pronouns to named entities of type human.', 'The coordination construction “warm and friendly” got correctly analysed (distributively), and the control construction “began to read his book” received a proper predicate argument structure.', 'Bad: Boxer doesn’t deal with bridging references introduced by relational nouns, so expressions like “the corner” were not linked to other discourse entities.', 'Text 4: The first school for the training of leader dogs ...', 'Good: The named entities were correctly recognised and classified (locations and proper names).', 'The VP coordination in the first and later sentences was correctly analysed.', 'The expression “this school” got correctly linked to the schhol mentioned earlier in the text.', 'The time expression “1999” got the right interpretation.', 'Bad: The adjectives/determiners “first” and “several” didn’t receive a deep analysis.', 'The complex NP “Joao Pedro Fonseca and Marta Gomes” was distributively interpreted, rather than collective.', 'The pronoun “they” wasn’t resolved.', 'The preposition “In” starting the second sentence was incorrectly analysed by the parser.', 'Text 5: As the 3 guns of Turret 2 were being loaded ...', 'Good: The discourse structures invoked by the sentence initial adverbs “As” and “When” was correctly computed.', 'Predicate argument structure overall good, including treatment of the relative clauses.', 'The expression “the propellant” was correctly resolved.', 'Time expressions in the one but last sentence got a correct analysis.', 'Bad: The name “Turret 2” was incorrectly analysed (not as a compound).', 'The adverbs “yet” and “then” got a shallow analysis.', 'The first-person pronoun “I” was not resolved to the crewman.', 'Comments: The quotes were removed in the tokenisation phase, because the C&C parser, being trained on a corpus without quotes, performs badly on texts containing quotes.', 'Text 6: Amid the tightly packed row houses of North Philadelphia ...', 'Good: The named entities were correctly recognised and classified as locations.', 'The various cases of VP coordination all got properly analysed.', 'The numerical and date expressions got correct representations.', 'Bad: The occurrences of the third-person neuter pronouns were not resolved.', 'The preposition “Amid” was not correctly analysed.', 'Text 7: Modern development of wind-energy technology and applications ...', 'Good: Correct interpretation of time expressions “1930s” and “1970s”.', 'Correct pred icate argument structure overall.', 'Bad: “Modern” was recognised as a proper name.', 'The noun phrase “wind-energy technology and applications” was distributively analysed with “wind-energy” only applying to “technology”.', 'The sentence-initial adverb “Since” did not introduce proper discourse structure.', 'The units of measurement in the last two sentences were not recognised as such.', 'The tricky time expression “mid-80’s” only got a shallow interpretation.', 'Boxer is a wide-coverage system for semantic interpretation.', 'It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.', 'The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.', 'Boxer’s performance on the shared task for comparing semantic represtations was promising.', 'It was able to produce DRSs for all texts.', 'We can’t quantify the quality of Boxer’s output, as we don’t have gold standard representations at our disposal.', 'Manually inspecting the output gives us the following impression: • computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination; • discourse structure triggered by conditionals, negation or discourse adverbs is overall correctly computed; • some measure and time expressions are correctly analysed, others aren’t; • several shallow analyses are given for lexical phrases that require deep analysis; • bridging references and pronouns are not resolved in most cases; but when they are, they are mostly correctly resolved (high precision at the cost of recall).', 'Finally, a comment on availability of Boxer.', 'All sources of Boxer are available for download and free of noncommercial use.', 'It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two classes of methods have been shown to be useful for resolving lexical ambiguity.', 'The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.', 'These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.', 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', \"This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.\", 'Decision lists are found, by and large, to outperform either component method.', 'However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.', 'A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.', 'Two classes of methods have been shown useful for resolving lexical ambiguity.', 'The first tests for the presence of particular context words within a certain distance of the ambiguous target word.', 'The second tests for collocations - patterns of words and part-of-speech tags around the target word.', 'The context-word and collocation methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.', 'Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', 'Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax.', \"This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence.\", 'A method is presented for doing this, based on Bayesian classifiers.', 'The work reported here was applied not to accent restoration, but to a related lexical disam\\xad biguation task: context-sensitive spelling correction.', \"The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert.\", 'where dessert was misspelled as desert.', 'This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words.', 'We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods.', '\\\\Ve then apply each of the two component methods mentioned above\\xad context words and collocations.', '\\\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', '\\\\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.', 'The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.', 'The final section draws some conclusions.', 'Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.', 'Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).', 'These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.', '\\\\Ve treat context-sensitive spelling correction as a task of word disambiguation.', 'The ambiguity among words is modelled by confusion sets.', 'A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.', 'Thus if C = {deser·t, desser·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.', 'This treatment requires a collection of confusion sets to start with.', 'There are several ways to obtain such a collection.', 'One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations.', 'Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of \"\\\\Vords Commonly Confused\" in the back of the Random House unabridged dictionary [Fiexner, 1983].', 'A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).', 'We will make the simplifying assumption that both kinds of errors are equally bad.', 'In practice, however, false negatives are much worse, as users get irritated by programs that badger them with bogus complaints.', 'However, given the probabilistic nature of the methods that will be presented below, it would not be hard to modify them to take this into account.', \"We would merely set a confidence threshold, and report a suggested correction only if the probability of the suggested word exceeds the probability of the user's original spelling by at least the threshold amount.\", 'The reason this was not done in the work reported here is that setting this confidence threshold involves a certain subjective factor (which depends on the user\\'s \"irritability threshold\").', 'Our simplifying assumption allows us to measure performance objectively, by the single parameter of prediction accuracy.', '1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.', 'For instance, cat might have the confusion set {lwf,car·, ...', '}, hat might have {cat,had, ...', '}, and so on.', 'We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the \"confusable\" relation is no longer transitive.', 'This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of \"minimal competency\" for comparison with the other methods Context words Tests for particular words within ±k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.', 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', 'Each method will be described in terms of its operation on a single confusion set C = {Wt, ...', ', wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.', 'The methods handle multiple confusion sets by applying the same technique to each confusion set independently.', 'Each method involves a training phase and a test phase.', 'The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.', 'and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].', '3.1 Baseline method.', 'The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.', 'For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (o·r left as) desert.', 'Table 1 shows the performance of the baseline method for 18 confusion sets.', 'This collection of confusion sets will be used for evaluating the methods throughout the paper.', 'Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.', 'Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.', 'For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886.', 'Essentially the baseline method measures how accurately one can predict words using just their prior probabilities.', 'This provides a lower bound on the performance we would expect from the other methods, which use more than just the priors.', '3.2 Component method 1: Context words.', 'One clue about the identity of an ambiguous target word comes from the words around it.', 'For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert.', 'On the other hand, words such as chocolate and delicious in the context imply desser·t. This observation is the basis for the method of context words.', \"The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow.\", 'C on fu si on se t No.', 'of No.', 'of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2.', \"5 84 0 its , it' s 19 .5 1 3.\", \"57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets.\", 'The \"Most frequent word\" column gives the word in the confusion set that occurred most frequently in the training corpus.', '(In subsequent tables, confusion sets will be referred to by their most frequent word.)', 'The \"Baseline\" column gives the prediction accuracy of the baseline system on the test corpus.', 'Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.', 'The task is to pick the word Wi that is most probable, given the context words Cj observed within a ±k-word window of the target word.', \"The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k.\", ', c_ 1, c1, ...', ', cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.', 'Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.', 'This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation.', 'We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.', 'The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within ±k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way.', 'Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).', 'The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.', 'Gale et al. interpolate between the two so as to minimize the overall inaccuracy.', 'We have pursued an alternative approach to the problem of estimating the likelihood terms.', 'We start with the observation that there is no need to use every word in the ±k-word window to discriminate among the words in the confusion set.', 'If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.', 'We implement this by introducing a \"minimum occurrences\" threshold, Tmin.', 'It is currently set to 10.', 'We then ignore a context word c if: L m; < Tmin or L (Af;- m;) < Tmin l i n l5i n where m; and A{ are defined as above.', 'In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;.', 'In the former case, we have insufficient data to measure its presence; in the latter, its absence.', 'Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set.', 'For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help.', 'By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time.', 'To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set.', 'If the observed association is not judged to be significant,3 then c is discarded.', 'The significance level is currently set to 0.05.', 'Figure 1 pulls together the points of the preceding discussion into an outline of the method of context words.', 'In the training phase, it identifies a list of context words that are useful for discriminating among the words in the confusion set.', 'At run time, it estimates the probability of each word in the confusion set.', 'It starts with the prior probabilities, and multiplies them by the likelihood of each context word from its list that appears in the ±k-word window of the target word.', 'Finally, it selects the word in the confusion set with the greatest probability.', 'The main parameter to tune for the method of context words is k, the half-width of the context window.', 'Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities.', '\\\\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.', 'In the rest of this paper, this value of k will be used.', '2 We are interpreting the condition \"c. occurs within a ±k-word window of w;\" as a binary feature - either it happens, or it does not.', 'This allows us to handle context words in the same Bayesian framework as will be used later for other binary features (see Section 3.3).', 'A more conventional interpretation is to take into account the number of occurrences of each Cj within the ±k-word window, and to estimate p(cilw;) accordingly.', 'However, either interpretation is valid, as long as it is applied consistently - that is, both when estimating the likelihoods from training data, and when classifying test cases.', '3 An association is significant if the probability that it occurred by chance is low.', 'This is not a statement about.', 'the strength of the association.', 'Even a wea.k association may be judged significant if there are enough da.ta to support it.', 'Measures of the strength of association will be discussed in Section 3.4.', 'Training phase (1) Propose all words a.s candidate context words.', '(2) Count occurrences of each candidate context word in the training corpus.', '(3) Prune context words that have insufficient data or are uninformative discriminators.', '(4) Store the remaining context words (and their associated statistics) for use at run time.', 'Run time (1) Initialize the probability for each word in the confusion set to its prior probability.', '(2) Go through the list of context words that was saved during training.', 'For each context word that appears in the context of the ambiguous target word, update the probabilities.', '(3) Choose the word in the confusion set with the highest probability.', 'Figure 1: Outline of the method of context words.', 'Table 2 shows the effect of varying k for our usual collection of confusion sets.', 'It can be seen that performance generally degrades as k increases.', 'The reason is that the method starts picking up spurious correlations in the training corpus.', 'Table 4 gives some examples of the context words learned for the confusion set {peace, piece}, with k = 24.', 'The context words co1·ps, united, nations, etc., all imply peace, and appear to be plausible (although united and nations are a counterexample to our earlier assumption of independence).', 'On the other hand, consider the context word how, which allegedly also implies peace.', 'If we look back at the training corpus for the supporting data for this word, we find excerpts such a.s: But oh, how I do sometimes need just a moment of rest, and peace ..', 'No ma.tter how earnest is our quest for guaranteed peace ..', 'How best to destroy your peace ? There does not seem to be a necessary connection here between how and peace; the correlation is probably spurious.', 'Although we are using a chi-square test expressly to filter out such spurious correlations, we can only expect the test to catch 95% of them (given that the significance level was set to 0.05).', 'As mentioned above, most of the legitimate context words show up for small k; thus as k gets large, the limited number of legitimate context words gets overwhelmed by the 5% of the spurious correlations that make it through our filter.', '3.3 Component method 2: Collocations.', 'The method of context words is good at capturing generalities that depend on the presence of nearby words, but not their order.', '\\\\Vhen order matters, other more syntax-based methods, such as collocations and trigrams, are appropriate.', 'In the work reported here, the method of collocations was used to capture order dependencies.', 'A collocation expresses a pattern of syntactic elements around the target word.', 'We allow two types of syntactic elements: words, and part-of-speech tags.', 'Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords ± 3 ± 6 ± 1 2 ± 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 ..', '5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no.', 'of contex t words 2 7 . 9 36.9 55.9 92.9 Table 2: Performance of the method of context words as a function of k, the half-width of the context window.', 'The bottom line of the table shows the number of context words learned, averaged over all confusion sets, also as a function of k. This collocation would match the sentences: Travelers entering from the desert were confounded ...', 'along with some guerrilla fighting in the desert.', 'two ladies who lay pinkly nude beside him in the desert Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each word in the sentence with its set of possible part-of-speech tags, obtained from a dictionary.', \"For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set.\", 'The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ).', 'The method of collocations was implemented in much the same way as the method of context words.', 'The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.', 'An ambiguous target word is then classified by finding all collocations that match its context.', \"Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's).\", '45 for each word in the confusion set.', \"This evidence is combined using Bayes' rule.\", 'In the end, the Wi with the highest probability, given the evidence, is selected.', 'A new complication arises for collocations, however, in that collocations, unlike context words, cannot be assumed independent.', 'Consider, for example, the following collocations for desert: PREP the in the the These collocations are highly interdependent- we will say they conflict.', 'To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence.', 'If two pieces of evidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence.', 'We identify conflicts by the heuristic that two collocations conflict iff they overlap.', 'The overlapping portion is the factor they have in common, and thus represents their lack of independence.', 'This is only a heuristic because we could imagine collocations that do not overlap, but still conflict.', 'Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right.', 'Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one.', 'Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.', 'This preserves the strongest non-conflicting evidence as the basis for our answer.', 'The strength of a collocation reflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4.', 'Figure 2 ties together the preceding discussion into an outline of the method of collocations.', 'The method is described in terms of \"features\" rather than \"collocations\" to reflect its full generality; the features could be context words a.s well as collocations.', 'In fact, the method subsumes the method of context words -it does everything that method does, and resolves conflicts among its features as well.', 'To facilitate the conflict resolution, it sorts the features by decreasing strength.', 'Like the method of context words, the method of collocations has one main parameter to tune: f, the maximum number of syntactic elements in a collocation.', 'Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3.', 'We tried this on some practice confusion sets, and found that a.ll values of£ gave roughly comparable performance.', 'We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3).', 'Table 3 shows the results of varying£ for the usual confusion sets.', 'There is no clear winner; each value of£ did best for certain confusion sets.', 'Table 5 gives examples of the collocations learned for {peace, piece} with£= 2.', 'A good deal of redundancy can be seen among the collocations.', 'There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps).', 'Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant.', '3.4 Hybrid method 1: Decision lists.', 'Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while collo\\xad cations capture drder-dependent generalities.', 'Ya.rowsky proposed decision lists as a way to get the best of both methods.', 'The idea is to make one big list of all features - in this case, context words and collocations.', 'The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.', 'An ambiguous target word is then classified by running down the list and matching each feature against the target context.', 'The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features.', 'Count occurren ces of each candidat e feature in the training corpus.', 'P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time.', 'Run time (1) Initialize the probability for each word in the confusion set to its prior probability.', '(2) Go through the sorted list of features that was saved during training.', 'For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities.', '(3) Choose the word in the confusion set with the highest probability.', 'Figure 2: Outline of the method of collocations.', 'Differences from the method of context words are highlighted in boldface.', 'The method is described in terms of \"features\" rather than \"collocations\" to reflect its full generality.', 'matches is used to classify the target word.', 'Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance.', 'The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take \"features\" in that figure to include both context words and collocations.', 'The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature.', 'This obviates the need for resolving conflicts between features.', 'Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined.', 'Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)', '= abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2.', \"It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!)\", \"= max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid co\\xad occurs 10 times with desert and 1 time with dessert in the training corpus.\", \"Then reliability'(!)\", '== max(10/11, 1/11) = 10/11 = 0.909.', 'This value measures the extent to which the presence of the feature is unambiguously correlated with one particular w;.', \"It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set.\", '5Jn fact, we guarantee that this inequalit.y holds by performing smoothing before calculating strength.', 'We smooth the data by adding 1 to the count.', 'of how many times each feature was observed for each w;.', '47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.', 'of colloc ations 3 3 . 9 263.1 98.5.4 Table 3: Performance of the method of collocations as a function of f, the maximum length of a collocation.', 'The bottom line of the table shows the number of collocations learned, averaged over all confusion sets, also as a function of e. One peculiar property of the reliability metric is that it ignores the prior probabilities of the words in the confusion set.', \"For instance, in the arid example, it would award the same high score even if the total number of occurrences of desert and dessert in the training corpus were 50 and 5, respectively - in which case arid's performance of 10/11 would be exactly what one would expect by chance, and therefore hardly impressive.\", 'Besides the reliability metric, therefore, we also considered an alternative metric: the uncertainty coefficient of x, denoted U(xiy) [Press et al., 1988, p..501].', 'U(xiy) measures how much additional information we get about the presence of the feature by knowing the choice of word in the confusion set.6 U(xiy) is calculated as follows: U(xiy) H(x) H(xiy) H ( x ) H ( x i y ) H ( x )-p(f)lnp(f)- p( .J)lnp(-.J) - Lp(w;) (p(flw;)lnp(flw;) + p( .Jiw;)lnp(•flw;)) The probabilities are ca.lculated for the population consisting of all occurrences in the training corpus of any w;.', 'For instance, p(f) is the probability of feature f being present within this 6 This definition may seem backwards, but.', 'is appropriate for use on the right-hand side of Bayes\\' rule, where the choice of word in the confusion set is t.he \"given\".', '48 C on te xt w or d pe ac e pzece co rp s p e a c e u n i t e d n a t i o n s o u r h e a r t j u s t i c e s t a t e a m e r i c a n a i d i n t e r n a t i o n a l w o m e n w a r w o r l d p i e c e o v e r m u s t g r e a t u n d e r h o w 4 9 1 4 1 1 2 0 0 1 5 0 2 7 1 1 2 0 1 2 0 1 2 0 1 1 0 1 1 0 1 1 0 1 0 0 2 0 1 4 0 3 1 1 . 5 1 1 4 1 1 1 1 1 1 1 0 1 1 0 1 t w o f o r a b o u t e v e r y l i t t l e l o n g o n e t h e so 5 1 2 8 3 3 8 4 9 4 9 5 1 0 6 1 1 1 4 2 3 1 7 9 113 9 1 4 1 6 2 2 To tal oc cu rr en ce s 1 8 4 126 Table 4: Excerpts from the list of 43 context words learned for {peace, piece} with k = 24.', 'Each line gives a context word, and the num\\xad ber of peace and piece occurrences for which that context word occurred within ±k words.', 'The last line of the table gives the total num\\xad ber of occurrences of peace and piece in the training corpus.', 'Table 5: Excerpts from the sorted list of 98 collocations learned for {peace, piece} with £ = 2.', 'Each line gives a collocation, and the number of peace and piece occurrences it matched.', 'The last line of the table gives the total number of occurrences of peace and piece in the training corpus.', '49 population.', 'Applying the U(xjy) metric to the arid example, the value returned now depends on the number of occurrences of deser·t and dessert in the training corpus.', 'If these numbers are 50 and 5, then U(xly) = 0.0, reflecting the uninformativeness of the arid feature in this situation.', \"If instead the numbers are 50 and 500, then U(:rjy) = 0.402, indicating arid's better-than-chance ability to pick out desrrt (10 out of 50 occurrences) over dessert (1 out of 500 occurrences).\", 'To compare the two strength metrics, we tried both on some practice confusion sets.', 'Sometimes one metric did substantially better, sometimes the other.', 'In the balance, the reliability metric seemed to give higher performance.', 'This metric is therefore the one that will be used from here on.', 'It was also used for all experiments involving the method of collocations.', 'Table 6 shows the performance of decision lists with each metric for the usual confusion sets.', 'As with the practice confusion sets, we see sometimes dramatic performance differences between the two metrics, and no clear winner.', 'For instance, for {I, me}, the reliability metric did better than U(xjy) (0.980 versus 0.808); whereas for {between, among}, it did worse (0.659 versus 0.800).', 'Further research is needed to understand the circumstances under which each metric performs best.', 'Focusing for now on the reliability metric, Table 6 shows that the method of decision lists does, by and large, accomplish what it set out to do - namely, outperform either component method alone.', 'There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.', 'But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined.', 'This problem is addressed in the next section.', '3.5 Hybrid method 2: Bayesian classifiers.', 'The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.', 'In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.', '\\\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence.', 'This section presents a method of doing this based on Bayesian classifiers.', 'Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.', 'It classifies an ambiguous target word by matching each feature in the list in turn against the target context.', 'Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.', 'This method is essentially the same as the one for collocations (see Figure 2), except that it uses context words as well as collocations for the features.', 'The only new wrinkle is in checking for conflicts between features (in step (2) at run time), as there are now two kinds of features to consider.', 'If both features are context words, we say the features never conflict (as in the method of context words).', 'If both features are collocations, we say they conflict iff they overlap (as in the method of collocations).', 'The new case is if one feature is a context word, and the other is a collocation.', \"Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it's}.\", '50 C on fu si on se t Ba sel in e Cwords Collocs ± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914.', '0.981 0 . 8 6 3 0.862 0.945 0 . 8 6 1 0.861 0.909 0 . 8 0 7 0.931 0.965 0 . 7 8 0 0.791 0.853 0 . 7 4 1 0.747 0.821 0 . 7 2 6 0.816 0.887 0 . 6 2 7 0.646 0.646 0 . 6 1 4 0.639 0.639 0 . 5 7 . 5 0..575 0.807 0 . 5 3 8 0.759 0.730 0 . 5 3 0 0.530 0.840 0 . 4 4 2 0.695 0.789 0 . 3 9 3 0.754 0.869 0 . 3 0 6 0.726 0.932 0 . 2 9 0 0.290 0.812 0 . 1 1 4 0.455 0.318 0.', '93 5 0.829 0.', '98 0 0.808 0.', '93 1 0.805 0.', '93 2 0.892 0.', '96 7 0.961 0.', '84 2 0.933 0.', '82 1 0.654 0.', '86 8 0.896 0.', '62 9 0.667 0.', '62 7 0.651 0.', '80', '0.', '65 9 0.800 0.', '84 0 0.840 0.', '78 9 0.726 0.', '85 2 0.836 0.', '91 4 0.906 0.', '81 2 0.841 0.', '43 2 0.568 Table 6: Performance of decision lists with the reliability and U(xiy) strength metrics.', 'To some extent, all of these collocations conflict with walk.', 'Collocation (1) is the most blatant case; if it matches the target context, this logically implies that the context word walk will match.', 'If collocation (2) matches, this guarantees that one of the possible tags of walk will be present nearby the target word, thereby elevating the probability that walk will match within ±k words.', 'If collocation (3) matches, this guarantees that there are two positions nearby the target word that are incompatible with walk, thereby reducing the probability that walk will match.', 'If we were to treat all of these cases as conflicts, we would end up losing a great deal of (potentially useful) evidence.', 'Instead, we adopt the more relaxed policy of only flagging the most egregious conflicts - here, the one between collocation (1) and walk.', 'In general, we will say that a collocation and a context word conflict iff the collocation contains an explicit test for the context word.', 'Table 7 compares all methods covered so far - baseline, two component methods, and two hybrid methods.', '(A sixth method, trigrams, is included as well-it will be discussed in Section 4.)', 'The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.', 'Occasionally it scores slightly less than collocations; this appears to be due to some averaging effect where noisy context words are dragging it down.', 'Occasionally too it scores less than decision lists, but never by much; on the whole, it yields a modest but consistent improvement, and in the case of {between, among}, a sizable improvement.', 'We believe the improvement is due to considering all of the evidence, rather than just the single strongest piece, which makes the method more robust to inaccurate judgements about which piece of evidence is \"strongest\".', '51 C on fu si on se t B as eli 11 e C w or ds Collocs Dlist Bayes ± 3 : : ; 2 R e l y R e l y Trigra ms w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 5 0 ..', '5 3 8 0 ..', '5 3 0 0 . 4 4 2 0 . 3 9 3 0 . : 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.931 0.93.5 0.935 0 . 9 1 4 0.981 0.980 0.985 0 . 8 6 2 0.945 0.931 0.942 0 . 8 6 1 0.909 0.932 0.924 0 . 9 3 1 0.96.5 0.967 0.973 0 . 7 9 1 0.853 0.842 0.869 0 . 7 4 7 0.821 0.821 0.827 0 . 8 1 6 0.887 0.868 0.901 0 . 6 4 6 0.646 0.629 0.662 0 . 6 : 3 9 0.639 0.627 0.639 0 ..', '5 7 . 0 . 7 . 5 9 0.730 0.6.59 0.786 0 ..', '5 3 0 0.840 0.840 0.840 O . G 9.', '0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0..', '538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.', '4 Evaluation.', 'While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.', 'We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].', \"Schabes's method can be viewed as performing an abductive inference: given a sentence con\\xad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence.\", 'It answers this question by substituting each w; in turn into the sentence.', 'The w; that produces the highest-probability sentence is selected.', 'Sentence probabilities are calculated using a part-of-speech trigram model.', \"We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7.\", 'It can be seen that trigrams and the Bayesian hybrid method each have their better moments.', 'Trigrams are at their worst when the words in the confusion set have the same part of speech.', 'In this case, trigrams can distinguish between the words only by their prior probabilities\\xad this follows from the way the method calculates sentence probabilities.', 'Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method.', 'In such cases, the Bayesian hybrid method is clearly better.', \"On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}\\xad trigrams are often better than the Bayesian method.\", 'We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence.', 'This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.', 'This is a research direction we plan to pursue.', \"The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations.\", 'Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.', 'This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.', 'A method for doing this, based on Bayesian classifiers, was presented.', 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', \"A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise.\", 'This is a direction we plan to pursue in future research.', 'We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.']\n"
     ]
    }
   ],
   "source": [
    "# test_docs  = os.listdir(\"./scisumm-2018/Test\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_data, train_docs = get_dataset_2018(train_rps,\"/ssd_scratch/cvit/dhawals1939/scisumm-2018/Training/\")\n",
    "# val_data, val_docs = get_dataset_2018(val_rps,\"scisumm-2018/Training/\")\n",
    "# test_data, test_docs = get_dataset_2018(test_rps,\"/ssd_scratch/cvit/dhawals1939/scisumm-2018/Training/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06d44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit test passed\n"
     ]
    }
   ],
   "source": [
    "def test_if_valid_data(docs):\n",
    "    for k in docs:\n",
    "        try:\n",
    "            assert(len(docs[k]['ref_off']) == len(docs[k]['cite_text']))\n",
    "        except:\n",
    "            print(\"test failed: length of queries and gt not equal\")\n",
    "            return\n",
    "    print(\"unit test passed\")\n",
    "test_if_valid_data(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eec5bd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), #len(val_data),len(test_data), len(train_docs), len(val_docs),len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4b03fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "# val_dataloader = DataLoader(val_data, shuffle=True, batch_size=1)\n",
    "# test_dataloader = DataLoader(test_data, shuffle=True, batch_size=1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e558f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eae782489864ccba49a720e8d281d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1ceb87785346119acdc3c7daebe013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f714b84eef24a94aea45cbb038b8e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e8a0e830b74f01a15025d8a6d93e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58b10468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,'bert2018_ngrams_full_dset.pth')\n",
    "model = torch.load('bert2018_ngrams_full_dset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a957f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "972f7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def get_embed_docs(docs):\n",
    "    for doc_id in docs.keys():\n",
    "        docs[doc_id]['corpus_embed'] = model.encode(docs[doc_id]['corpus'])\n",
    "        docs[doc_id]['cite_text_embed'] = model.encode(docs[doc_id]['cite_text'])\n",
    "    return docs\n",
    "train_docs = get_embed_docs(train_docs)\n",
    "\n",
    "with open('train_docs.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_docs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eed00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ignite.metrics import Rouge  #https://pypi.org/project/pytorch-ignite/\n",
    "from rouge_score import rouge_scorer  #https://pypi.org/project/rouge-score/\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "from pythonrouge.pythonrouge import Pythonrouge\n",
    "\n",
    "\n",
    "def get_matching_sentences(model,corpus,queries,gt, thresh=0.6,topk=3,cs_based=True):\n",
    "    #f1 score will be same as precision and recall in our case: since documents\n",
    "    # Get a vector for each headline (sentence) in the corpus\n",
    "    # Define search queries and embed them to vectors as well\n",
    "    \n",
    "    query_embeddings = model.encode(queries)\n",
    "    # For each search term return 3 closest sentences\n",
    "    total_nums_correct = 0\n",
    "    total_retrieved = 0\n",
    "    total_relevent = 0\n",
    "    rouge1 = []\n",
    "    rouge2 = []\n",
    "    rouge_su4 = []\n",
    "    for i in range(len(queries)):\n",
    "        query = queries[i]\n",
    "        distances = []\n",
    "#         lis = np.intersect1d(list(citant_ngrams), list(cited_text_ngrams_lis))\n",
    "#         lis = \" \".join(lis)\n",
    "#         temp = pd.Series(cit_text_lis[i])\n",
    "#         citant_ngrams = temp.apply(lambda x: get_ngrams(x))\n",
    "#         lis = np.intersect1d(list(citant_ngrams), list(cited_text_ngrams_lis))\n",
    "#         lis = \" \".join(lis)\n",
    "        citant_ngrams = get_ngrams(query)\n",
    "        for c_sente in corpus:\n",
    "            rp_ngrams = get_ngrams(c_sente)\n",
    "            lis = np.intersect1d(list(citant_ngrams), list(rp_ngrams))\n",
    "            lis = \" \".join(lis)\n",
    "            query_embedding = model.encode(query+lis)\n",
    "            rp_embedding = model.encode(c_sente)\n",
    "            distances.append(scipy.spatial.distance.cdist([query_embedding], [rp_embedding], \"cosine\")[0])\n",
    "#         query, query_embedding  = queries[i], query_embeddings[i]\n",
    "#         distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "        distances = np.array(distances)\n",
    "        distances = 1 - distances\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: -x[1])\n",
    "\n",
    "        retrieved = []\n",
    "        if cs_based:\n",
    "            for k,dist in results:\n",
    "                if dist >= thresh:\n",
    "                    retrieved.append(k)\n",
    "        else: #retrieve topk most matching sentences\n",
    "            indexes = results[0:topk]\n",
    "            retrieved = []\n",
    "            for l,k in indexes:\n",
    "                retrieved.append(l)\n",
    "    \n",
    "        nums_correct = len(np.intersect1d(retrieved,gt[i]))\n",
    "        total_nums_correct += nums_correct\n",
    "        total_relevent += len(gt[i])\n",
    "        total_retrieved += len(retrieved)\n",
    "        for idx in retrieved:\n",
    "            scores = scorer.score(corpus[idx].strip(), query)\n",
    "            rouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=[[corpus[idx].strip()]], reference=[[[query]]],\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=False, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "    \n",
    "            score = rouge.calc_score()\n",
    "            rouge1.append(score['ROUGE-1-F'])\n",
    "            rouge2.append(score['ROUGE-2-F'])\n",
    "            rouge_su4.append(score['ROUGE-SU4-F'])\n",
    "\n",
    "            \n",
    "    return total_nums_correct, total_relevent,total_retrieved, np.mean(rouge1), np.mean(rouge2), np.mean(rouge_su4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "53830ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['corpus', 'cite_text', 'ref_off', 'corpus_embed', 'cite_text_embed'])\n",
      "use boxer semantic analyzer extract semantic predicates event date\n",
      "['3', '144']\n"
     ]
    }
   ],
   "source": [
    "doc_id = 'W08-2222'\n",
    "print(train_docs[doc_id].keys())\n",
    "print(train_docs[doc_id]['cite_text'][0])\n",
    "print(train_docs[doc_id]['ref_off'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8effd447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resulting drss translated ordinary logic formulas processing standard theorem provers logic'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[doc_id]['corpus'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bc33cc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semantic representations produced boxer known discourse representation structures drss incorporate neodavidsonian representations events using verbnet inventory thematic roles'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[doc_id]['corpus'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "63f03c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "file  A00-2018\n",
      "{0: {17, 90, 91, 5}, 1: {5}, 2: {17, 90, 91}, 3: {39, 48, 49, 51, 120}, 4: {119, 90, 91, 92, 93, 94}, 5: {0, 162, 95}, 6: {175, 90, 87}, 7: {90, 91, 92, 93, 94}, 8: {1, 38, 39, 40, 176}, 9: {126, 174}, 10: {101, 12, 85}, 11: {174, 143, 146, 155, 63}, 12: {17, 63, 78, 79}, 13: {162, 90, 78}, 14: {40, 91, 174}, 15: {180, 79}, 16: {40}, 17: {175}}\n",
      "len new_corpus 191\n",
      "18 15\n",
      "\n",
      "\n",
      "file  P05-1013\n",
      "ref  nan\n",
      "{0: {49, 20}, 1: {24, 9, 109}, 2: {104, 106, 95}, 3: {104, 86, 79}, 4: {20, 109}, 5: {36, 38, 95}, 6: {20, 109}, 7: {36, 79}, 8: {109, 62, 23}, 9: {24, 104, 109}, 10: {80, 81, 109, 23}, 11: {24, 104}, 12: {20, 86, 95}, 13: {24, 96, 95}, 14: {20, 109, 95}, 15: {40, 49, 51}, 16: {99, 109, 7}, 17: {14, 7}, 19: {104, 2, 20}, 18: {49}}\n",
      "len new_corpus 114\n",
      "20 20\n",
      "\n",
      "\n",
      "file  D10-1044\n",
      "ref  nan\n",
      "ref  nan\n",
      "{0: {144, 9, 4}, 1: {96, 132, 95}, 2: {9, 7}, 3: {62}, 4: {50, 28, 71}, 5: {96, 152, 23}, 6: {144, 71}, 8: {9, 75, 62}, 9: {28, 45}, 11: {75}, 12: {143, 31}, 13: {153, 22}, 14: {96, 144, 23}, 15: {62}, 16: {120, 42, 141, 119}, 17: {24, 96, 28, 23}, 18: {40, 37}, 7: {144, 9}, 10: {97}}\n",
      "len new_corpus 154\n",
      "19 19\n",
      "\n",
      "\n",
      "file  P87-1015\n",
      "{0: {205, 165, 118}, 1: {229, 118, 119}, 2: {146, 3, 149}, 3: {201, 118}, 4: {118, 151}, 5: {222, 133, 118}, 6: {138, 2, 22, 23}, 7: {156, 118}, 8: {221, 222, 118}, 9: {54, 119}, 10: {128, 133}, 11: {164, 207}, 12: {16, 204, 207}, 13: {16, 9, 19}, 14: {28, 214, 119}, 15: {138, 35, 118}}\n",
      "len new_corpus 233\n",
      "16 16\n",
      "\n",
      "\n",
      "file  P08-1102\n",
      "ref  nan\n",
      "ref  nan\n",
      "{0: {32, 130, 21}, 2: {42, 43, 38}, 3: {25, 92}, 4: {9, 130, 92}, 5: {33, 34, 36}, 6: {32, 12}, 8: {64, 130, 92}, 9: {130, 92, 79}, 10: {96, 121, 97}, 11: {91, 37}, 12: {16, 73}, 13: {35, 46}, 14: {34, 12}, 15: {130, 91, 92}, 1: {32, 28}}\n",
      "len new_corpus 142\n",
      "15 16\n",
      "no annotation found for  7\n",
      "\n",
      "\n",
      "file  W06-3114\n",
      "{0: {170, 108, 47}, 1: {8, 34}, 2: {9, 18}, 3: {36, 151}, 4: {16, 9, 18}, 5: {144, 172, 36}, 6: {145, 140, 36}, 7: {140, 103}, 8: {140, 167}, 9: {50, 102}, 10: {123, 84, 68}, 11: {170, 11, 34}, 12: {140, 62}, 13: {18, 126}, 14: {170, 173, 15}, 15: {8, 170}, 16: {90}, 17: {92, 5, 6, 84}, 18: {8, 145}}\n",
      "len new_corpus 179\n",
      "19 18\n",
      "\n",
      "\n",
      "file  E03-1005\n",
      "ref  nan\n",
      "ref  nan\n",
      "{0: {105, 20}, 1: {145, 74, 41}, 2: {80, 44}, 3: {143}, 4: {145, 146, 140, 141}, 5: {140, 134}, 6: {22, 102, 103}, 7: {105, 140, 133}, 9: {140, 38}, 10: {100, 46}, 11: {32, 85, 86, 30}, 13: {140}, 14: {27, 115, 30}, 8: {25}, 12: {130}}\n",
      "len new_corpus 147\n",
      "15 15\n",
      "\n",
      "\n",
      "file  W99-0623\n",
      "ref  nan\n",
      "{0: {144, 85}, 1: {120, 125, 117}, 2: {72, 25, 125}, 3: {120, 48}, 4: {139, 38}, 5: {91, 84, 134}, 6: {120, 76, 38}, 7: {120, 38}, 8: {25, 139}, 9: {72, 25, 13}, 10: {108, 103, 87}, 11: {51, 139}, 12: {98, 70, 79}, 13: {27, 140}, 14: {70, 77, 80, 81, 82}, 16: {11, 70}, 17: {98, 116}, 15: {49, 85}}\n",
      "len new_corpus 149\n",
      "18 18\n",
      "\n",
      "\n",
      "file  P08-1028\n",
      "ref  nan\n",
      "ref  nan\n",
      "{0: {65, 51, 21}, 2: {42, 189, 190}, 3: {185, 186, 21, 53}, 4: {51, 195}, 5: {25, 76, 189, 190}, 6: {48, 57, 25}, 7: {57, 53}, 8: {68, 69, 70, 51, 57}, 9: {189, 190}, 10: {29, 60, 189, 190}, 11: {24, 25, 190, 38}, 12: {64, 51, 190}, 13: {176, 177, 64, 73}, 15: {189, 191}, 16: {24, 185, 191}, 1: {75, 27}, 14: {99, 163}, 17: {190}}\n",
      "len new_corpus 203\n",
      "18 17\n",
      "\n",
      "\n",
      "file  W99-0613\n",
      "{0: {9, 121}, 1: {35, 36, 252, 159}, 2: {137, 91, 134}, 3: {91, 79}, 4: {10, 236, 213}, 5: {8, 18, 250}, 6: {42, 236, 237, 39}, 7: {9, 10, 202, 236, 213}, 8: {39, 137, 9, 61, 222}, 9: {36, 176, 26, 27, 30}, 10: {18, 108}, 11: {85, 26, 27, 28, 29}, 12: {8, 9, 7}, 13: {32, 172}, 15: {95, 47}, 16: {213, 214, 127}, 14: {85}}\n",
      "len new_corpus 257\n",
      "17 13\n",
      "\n",
      "\n",
      "file  A97-1014\n",
      "{0: {72, 168}, 1: {144, 165, 168}, 2: {145, 14, 151}, 3: {15}, 4: {41, 36, 47}, 5: {24, 47, 167}, 6: {160, 168, 167}, 7: {167, 4, 151}, 8: {160, 4, 39}, 9: {160, 127}, 10: {39, 166, 167}, 11: {72, 166, 39}, 12: {143, 4, 151}, 13: {4, 71}, 14: {144, 167}, 15: {160, 151}}\n",
      "len new_corpus 176\n",
      "16 16\n",
      "\n",
      "\n",
      "file  P11-1060\n",
      "ref  nan\n",
      "ref  nan\n",
      "ref  nan\n",
      "{0: {112, 8, 11}, 1: {112, 2, 132}, 2: {9, 25}, 3: {25, 45, 21}, 4: {51, 21}, 7: {112, 11, 148}, 8: {115, 11, 13}, 9: {112, 132, 21}, 10: {25, 21}, 11: {10, 94, 47}, 12: {112, 132, 36}, 13: {42, 44, 157}, 14: {106}, 15: {21, 45, 167}, 16: {88, 138, 172}, 18: {171}, 5: {25, 166}, 17: {40, 21}, 6: {8}}\n",
      "len new_corpus 175\n",
      "19 19\n",
      "\n",
      "\n",
      "file  P11-1061\n",
      "ref  nan\n",
      "{0: {40, 9, 144}, 2: {10, 44}, 3: {16, 70, 15}, 4: {25, 44, 52}, 5: {83, 29, 110}, 6: {113, 18, 115}, 7: {3, 18, 115}, 8: {18, 158}, 9: {13, 21, 23}, 10: {24, 3, 158}, 11: {120, 10, 21}, 12: {10, 2}, 13: {24, 16, 19}, 14: {17, 153, 23}, 15: {18, 158, 111}, 16: {161, 10}, 17: {56, 29, 23}, 18: {161, 23}, 1: {47}}\n",
      "len new_corpus 165\n",
      "19 19\n",
      "\n",
      "\n",
      "file  W06-2932\n",
      "ref  nan\n",
      "{0: {19, 5, 86}, 1: {36, 79}, 2: {57, 61, 79}, 3: {76, 79}, 4: {45, 54, 22}, 5: {104, 106, 54}, 6: {12}, 7: {104, 57, 22}, 8: {41, 58}, 9: {24, 64, 22}, 10: {41, 79}, 11: {12, 21}, 12: {64, 43}, 13: {104, 21}, 15: {57, 41}, 16: {43}, 14: {33}}\n",
      "len new_corpus 111\n",
      "17 17\n",
      "\n",
      "\n",
      "file  A00-2030\n",
      "{0: {18, 11, 4}, 1: {100, 33, 34, 52}, 2: {49, 50, 52}, 3: {33, 34}, 4: {1, 10, 106}, 5: {16, 61, 6}, 6: {104, 33, 34, 6}, 7: {32, 33, 34}, 8: {24, 2, 12, 23}, 9: {24, 3, 61, 23}, 10: {24, 33, 34, 23}, 11: {32, 33, 34}, 12: {10, 3, 60, 61}, 13: {33, 52, 94}, 14: {104, 33, 34, 6}, 15: {104, 33, 34}, 16: {2, 11, 12, 16, 26}, 17: {104, 105, 6}, 18: {2, 19, 60, 61}, 19: {16, 61}}\n",
      "len new_corpus 113\n",
      "20 20\n",
      "\n",
      "\n",
      "file  P08-1043\n",
      "{0: {156, 94}, 1: {4}, 2: {70, 19, 94}, 3: {3, 19}, 4: {51, 19, 4}, 5: {107, 19, 4}, 6: {105, 69, 70, 14}, 7: {33, 85}, 8: {19, 53}, 9: {48, 19}, 10: {19, 163, 141}, 11: {188, 100, 21}, 12: {94}, 13: {188, 133, 134}, 14: {156, 85, 86}, 15: {97, 155, 5}}\n",
      "len new_corpus 198\n",
      "16 16\n",
      "\n",
      "\n",
      "file  D09-1092\n",
      "{0: {32, 17, 193}, 1: {114, 20, 39}, 2: {138}, 3: {10, 18, 196}, 4: {111, 55}, 5: {192, 9, 128}, 6: {122, 118, 119}, 7: {35, 148}, 8: {193, 35, 110}, 9: {184, 30, 55}, 10: {193, 146, 102}, 11: {163, 77, 38}, 12: {184, 156}, 13: {148, 36, 31}, 14: {184, 170, 19}, 15: {193, 131, 29}, 16: {192, 170, 196}, 17: {168, 105, 192}, 18: {10, 195, 29}, 19: {110, 6}}\n",
      "len new_corpus 200\n",
      "20 20\n",
      "\n",
      "\n",
      "file  J01-2004\n",
      "ref  nan\n",
      "{0: {372}, 1: {40, 41, 42, 15, 17}, 2: {25, 21, 31}, 3: {364, 21, 31}, 4: {302, 31}, 5: {31}, 6: {80, 215, 231, 79}, 7: {297, 21, 302}, 8: {138, 133}, 9: {291}, 10: {355, 372}, 11: {209, 210, 59, 31}, 12: {100, 372}, 14: {32, 108, 20, 21}, 15: {31}, 16: {33, 21, 31}}\n",
      "len new_corpus 415\n",
      "16 17\n",
      "no annotation found for  13\n",
      "\n",
      "\n",
      "file  W11-2123\n",
      "{0: {1, 12, 7}, 1: {1, 45, 7}, 2: {136, 131, 7}, 3: {1, 21}, 4: {1, 21, 7}, 5: {8, 108, 52}, 6: {1, 129}, 7: {1, 205, 199}, 8: {274, 52, 7}, 9: {177, 204, 263}, 10: {274, 52, 7}, 11: {1}, 12: {200, 1, 145}, 13: {1, 182, 7}, 14: {200, 1, 274}, 15: {200, 12, 229}, 16: {1, 7}, 17: {140, 93, 278}, 18: {1, 7, 199}, 19: {7, 199}}\n",
      "len new_corpus 288\n",
      "20 20\n",
      "\n",
      "\n",
      "file  P04-1036\n",
      "{0: {8}, 1: {82, 15}, 2: {64, 41, 68, 45}, 3: {172, 66, 83, 4}, 4: {153, 126, 15}, 5: {1, 101, 180, 181}, 6: {165, 126}, 7: {48, 8, 115, 171}, 8: {89, 169, 83}, 9: {137, 171, 172, 126}, 10: {75, 171, 189, 46}, 11: {115, 155, 126, 87}, 12: {8, 1}, 13: {89, 105, 155, 126}, 14: {68, 137, 13, 180, 181}, 15: {180, 13, 63}, 16: {8, 41, 159}}\n",
      "len new_corpus 194\n",
      "17 17\n"
     ]
    }
   ],
   "source": [
    "path='/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "def ref_off(d, path='/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'):\n",
    "    ref_off_union = {}\n",
    "    for name in names:\n",
    "        f_n = d+'_'+name+'.csv'\n",
    "        if f_n in files:\n",
    "            df = pd.read_csv(path+f_n)\n",
    "            ref_offs = list(df['Reference Offset'])\n",
    "            if(df['Reference Offset'].dtype!=np.float64):\n",
    "                for i,r in enumerate(ref_offs):\n",
    "                    r = str(r).replace(\"'\",\"\")\n",
    "                    if ',' in r:\n",
    "                        r = r.split(',')\n",
    "                        for a in r:\n",
    "                            try:\n",
    "                                if i in ref_off_union:\n",
    "                                    ref_off_union[i].add(int(a))\n",
    "                                else:\n",
    "                                    ref_off_union[i] = {int(a)}\n",
    "                            except:\n",
    "                                pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if i in ref_off_union:\n",
    "                                ref_off_union[i].add(int(r))\n",
    "                            else:\n",
    "                                ref_off_union[i] = {int(r)}\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                for i,r in enumerate(ref_offs):\n",
    "                    if i in ref_off_union:\n",
    "                        try:\n",
    "                            ref_off_union[i].add(int(r))\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            ref_off_union[i] = {int(r)}\n",
    "                        except:\n",
    "                            pass\n",
    "    print(ref_off_union)\n",
    "    return ref_off_union            \n",
    "#     return list(pd.Series(ref_off_union))\n",
    "\n",
    "def get_Test_dataset_2018(files, folder):\n",
    "    pairs_lis = []\n",
    "    prev_corpus = []\n",
    "    docs_wise = {}\n",
    "    titles = []\n",
    "    for z,f in enumerate(files):\n",
    "        print(\"\\n\\nfile \",f)\n",
    "        cit_text_lis = []\n",
    "        ref_text_lis = []\n",
    "        cit_off_lis = []\n",
    "#         ref_off_lis = []\n",
    "        new_corpus = []\n",
    "\n",
    "        a = folder+f+\"/Reference_XML/\"+f+\".xml\"\n",
    "        tree = ET.parse(a)\n",
    "        root = tree.getroot()\n",
    "        final =[]\n",
    "        total = len(root)\n",
    "        \n",
    "        for a in root:\n",
    "            title = a.text\n",
    "            titles.append(title)\n",
    "            break\n",
    "            \n",
    "        final.append(title)   \n",
    "        for a in root:\n",
    "            for b in a:\n",
    "                final.append(b.text)\n",
    "                \n",
    "        d={'col1':final}\n",
    "        rp = pd.DataFrame(data=d)\n",
    "        corpus = rp.col1\n",
    "        new_corpus = corpus.apply(lambda x: preprocess(x))\n",
    "\n",
    "        path = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "        files = [f_ for f_ in listdir(path) if isfile(join(path, f_))]\n",
    "        \n",
    "        fi = None\n",
    "        df = None\n",
    "        for name in names:\n",
    "            fi = f+'_'+name+'.csv'\n",
    "            if fi in files:\n",
    "                df = pd.read_csv(path+fi)\n",
    "                break\n",
    "        \n",
    "        cit_text = list(df['Citation Text Clean'])\n",
    "        pattern = r'\\<.*?\\>'\n",
    "        pattern2 = r'\\(.*?\\)'\n",
    "        for c in cit_text:\n",
    "            c = re.sub(pattern2,'',re.sub(pattern, '', c))\n",
    "            c = preprocess(c)\n",
    "            cit_text_lis.append(c)\n",
    "\n",
    "\n",
    "        ref_text = list(df['Reference Text'])\n",
    "        pattern = r'\\<.*?\\>'\n",
    "        pattern2 = r'\\(.*?\\)'\n",
    "\n",
    "        for ref in ref_text:\n",
    "            try:\n",
    "                ref = re.sub(pattern2,'',re.sub(pattern, '', ref))\n",
    "            except:\n",
    "                print(\"ref \",ref)\n",
    "            ref = preprocess(ref)\n",
    "            ref_text_lis.append(ref)\n",
    "\n",
    "        ref_off_lis = ref_off(f)\n",
    "        print(\"len new_corpus\",len(new_corpus))\n",
    "        print(len(ref_off_lis),len(cit_text_lis))\n",
    "        for i in range(len(cit_text_lis)):\n",
    "            if i in ref_off_lis:\n",
    "                for j in ref_off_lis[i]:\n",
    "                    cited_text_spans = new_corpus[int(j)]\n",
    "                    if int(j) < len(new_corpus):\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[int(j)]],label=1.0)) #positive pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3)) #negative pairs\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "                        pairs_lis.append(InputExample(texts=[cit_text_lis[i],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "    #                         pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "    #                         pairs_lis.append(InputExample(texts=[new_corpus[random.randint(0,len(new_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.3))\n",
    "            else:\n",
    "                print(\"no annotation found for \",i)\n",
    "        if (z!=0 and len(new_corpus)!=0 and len(prev_corpus)!=0):\n",
    "            pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "#                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "#                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "#                 pairs_lis.append(InputExample(texts = [prev_corpus[random.randint(0,len(prev_corpus)-1)],new_corpus[random.randint(0,len(new_corpus)-1)]],label=0.0))\n",
    "\n",
    "        docs_wise[f] = {'corpus':new_corpus.values,  'cite_text':cit_text_lis, 'ref_off':ref_off_lis}\n",
    "        prev_corpus = new_corpus\n",
    "\n",
    "            \n",
    "    return pairs_lis, docs_wise\n",
    "\n",
    "test_path = \"/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test/\"\n",
    "test_rps = os.listdir(test_path)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "test_data, test_docs = get_Test_dataset_2018(test_rps,test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f1a92b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "sorted(files)\n",
    "lis_missing = []\n",
    "files_missing = []\n",
    "doc_ids = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(path+f)\n",
    "    if df['Reference Offset'].isnull().values.any():\n",
    "#         print(\"f \",f)\n",
    "        lis_missing.append('_'+f.split('_')[1])\n",
    "        files_missing.append(f)\n",
    "        doc_ids.append(f.split('_')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b372b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = set(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "68222fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_missing\n",
    "names, counts = np.unique(lis_missing,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c5a1e0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_aakansha.csv', 8),\n",
       " ('_swastika.csv', 12),\n",
       " ('_sweta.csv', 5),\n",
       " ('_vardha.csv', 5)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(names,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c9aaf230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_off(d, path='/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'):\n",
    "    ref_off_union = {}\n",
    "    for name in names:\n",
    "        f_n = d+'_'+name+'.csv'\n",
    "        if f_n in files:\n",
    "            df = pd.read_csv(path+f_n)\n",
    "            ref_offs = list(df['Reference Offset'])\n",
    "            if(df['Reference Offset'].dtype!=np.float64):\n",
    "                for i,r in enumerate(ref_offs):\n",
    "                    r = str(r).replace(\"'\",\"\")\n",
    "                    if ',' in r:\n",
    "                        r = r.split(',')\n",
    "                        for a in r:\n",
    "                            try:\n",
    "                                if i in ref_off_union:\n",
    "                                    ref_off_union[i].add(int(a))\n",
    "                                else:\n",
    "                                    ref_off_union[i] = {int(a)}\n",
    "                            except:\n",
    "                                pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if i in ref_off_union:\n",
    "                                ref_off_union[i].add(int(r))\n",
    "                            else:\n",
    "                                ref_off_union[i] = {int(r)}\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                for i,r in enumerate(ref_offs):\n",
    "                    if i in ref_off_union:\n",
    "                        try:\n",
    "                            ref_off_union[i].add(int(r))\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            ref_off_union[i] = {int(r)}\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "    return list(pd.Series(ref_off_union))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a58f2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "names = ['aakansha','vardha','swastika','sweta']\n",
    "\n",
    "ref_off_dic = {}\n",
    "for d in doc_ids:\n",
    "    ref_off_dic[d] = ref_off(d, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a34d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c5c5c23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P05-1013': [{20, 49},\n",
       "  {9, 24, 109},\n",
       "  {95, 104, 106},\n",
       "  {79, 86, 104},\n",
       "  {20, 109},\n",
       "  {36, 38, 95},\n",
       "  {20, 109},\n",
       "  {36, 79},\n",
       "  {23, 62, 109},\n",
       "  {24, 104, 109},\n",
       "  {23, 80, 81, 109},\n",
       "  {24, 104},\n",
       "  {20, 86, 95},\n",
       "  {24, 95, 96},\n",
       "  {20, 95, 109},\n",
       "  {40, 49, 51},\n",
       "  {7, 99, 109},\n",
       "  {7, 14},\n",
       "  {2, 20, 104},\n",
       "  {49}],\n",
       " 'J01-2004': [{372},\n",
       "  {15, 17, 40, 41, 42},\n",
       "  {21, 25, 31},\n",
       "  {21, 31, 364},\n",
       "  {31, 302},\n",
       "  {31},\n",
       "  {79, 80, 215, 231},\n",
       "  {21, 297, 302},\n",
       "  {133, 138},\n",
       "  {291},\n",
       "  {355, 372},\n",
       "  {31, 59, 209, 210},\n",
       "  {100, 372},\n",
       "  {20, 21, 32, 108},\n",
       "  {31},\n",
       "  {21, 31, 33}],\n",
       " 'P08-1043': [{94, 156},\n",
       "  {4},\n",
       "  {19, 70, 94},\n",
       "  {3, 19},\n",
       "  {4, 19, 51},\n",
       "  {4, 19, 107},\n",
       "  {14, 69, 70, 105},\n",
       "  {33, 85},\n",
       "  {19, 53},\n",
       "  {19, 48},\n",
       "  {19, 141, 163},\n",
       "  {21, 100, 188},\n",
       "  {94},\n",
       "  {133, 134, 188},\n",
       "  {85, 86, 156},\n",
       "  {5, 97, 155}],\n",
       " 'A00-2030': [{4, 11, 18},\n",
       "  {33, 34, 52, 100},\n",
       "  {49, 50, 52},\n",
       "  {33, 34},\n",
       "  {1, 10, 106},\n",
       "  {6, 16, 61},\n",
       "  {6, 33, 34, 104},\n",
       "  {32, 33, 34},\n",
       "  {2, 12, 23, 24},\n",
       "  {3, 23, 24, 61},\n",
       "  {23, 24, 33, 34},\n",
       "  {32, 33, 34},\n",
       "  {3, 10, 60, 61},\n",
       "  {33, 52, 94},\n",
       "  {6, 33, 34, 104},\n",
       "  {33, 34, 104},\n",
       "  {2, 11, 12, 16, 26},\n",
       "  {6, 104, 105},\n",
       "  {2, 19, 60, 61},\n",
       "  {16, 61}],\n",
       " 'W06-3114': [{47, 108, 170},\n",
       "  {8, 34},\n",
       "  {9, 18},\n",
       "  {36, 151},\n",
       "  {9, 16, 18},\n",
       "  {36, 144, 172},\n",
       "  {36, 140, 145},\n",
       "  {103, 140},\n",
       "  {140, 167},\n",
       "  {50, 102},\n",
       "  {68, 84, 123},\n",
       "  {11, 34, 170},\n",
       "  {62, 140},\n",
       "  {18, 126},\n",
       "  {15, 170, 173},\n",
       "  {8, 170},\n",
       "  {90},\n",
       "  {5, 6, 84, 92},\n",
       "  {8, 145}],\n",
       " 'P11-1061': [{9, 40, 144},\n",
       "  {10, 44},\n",
       "  {15, 16, 70},\n",
       "  {25, 44, 52},\n",
       "  {29, 83, 110},\n",
       "  {18, 113, 115},\n",
       "  {3, 18, 115},\n",
       "  {18, 158},\n",
       "  {13, 21, 23},\n",
       "  {3, 24, 158},\n",
       "  {10, 21, 120},\n",
       "  {2, 10},\n",
       "  {16, 19, 24},\n",
       "  {17, 23, 153},\n",
       "  {18, 111, 158},\n",
       "  {10, 161},\n",
       "  {23, 29, 56},\n",
       "  {23, 161},\n",
       "  {47}],\n",
       " 'E03-1005': [{20, 105},\n",
       "  {41, 74, 145},\n",
       "  {44, 80},\n",
       "  {143},\n",
       "  {140, 141, 145, 146},\n",
       "  {134, 140},\n",
       "  {22, 102, 103},\n",
       "  {105, 133, 140},\n",
       "  {38, 140},\n",
       "  {46, 100},\n",
       "  {30, 32, 85, 86},\n",
       "  {140},\n",
       "  {27, 30, 115},\n",
       "  {25},\n",
       "  {130}],\n",
       " 'D10-1044': [{4, 9, 144},\n",
       "  {95, 96, 132},\n",
       "  {7, 9},\n",
       "  {62},\n",
       "  {28, 50, 71},\n",
       "  {23, 96, 152},\n",
       "  {71, 144},\n",
       "  {9, 62, 75},\n",
       "  {28, 45},\n",
       "  {75},\n",
       "  {31, 143},\n",
       "  {22, 153},\n",
       "  {23, 96, 144},\n",
       "  {62},\n",
       "  {42, 119, 120, 141},\n",
       "  {23, 24, 28, 96},\n",
       "  {37, 40},\n",
       "  {9, 144},\n",
       "  {97}],\n",
       " 'W99-0613': [{9, 121},\n",
       "  {35, 36, 159, 252},\n",
       "  {91, 134, 137},\n",
       "  {79, 91},\n",
       "  {10, 213, 236},\n",
       "  {8, 18, 250},\n",
       "  {39, 42, 236, 237},\n",
       "  {9, 10, 202, 213, 236},\n",
       "  {9, 39, 61, 137, 222},\n",
       "  {26, 27, 30, 36, 176},\n",
       "  {18, 108},\n",
       "  {26, 27, 28, 29, 85},\n",
       "  {7, 8, 9},\n",
       "  {32, 172},\n",
       "  {47, 95},\n",
       "  {127, 213, 214},\n",
       "  {85}],\n",
       " 'P08-1102': [{21, 32, 130},\n",
       "  {38, 42, 43},\n",
       "  {25, 92},\n",
       "  {9, 92, 130},\n",
       "  {33, 34, 36},\n",
       "  {12, 32},\n",
       "  {64, 92, 130},\n",
       "  {79, 92, 130},\n",
       "  {96, 97, 121},\n",
       "  {37, 91},\n",
       "  {16, 73},\n",
       "  {35, 46},\n",
       "  {12, 34},\n",
       "  {91, 92, 130},\n",
       "  {28, 32}],\n",
       " 'P08-1028': [{21, 51, 65},\n",
       "  {42, 189, 190},\n",
       "  {21, 53, 185, 186},\n",
       "  {51, 195},\n",
       "  {25, 76, 189, 190},\n",
       "  {25, 48, 57},\n",
       "  {53, 57},\n",
       "  {51, 57, 68, 69, 70},\n",
       "  {189, 190},\n",
       "  {29, 60, 189, 190},\n",
       "  {24, 25, 38, 190},\n",
       "  {51, 64, 190},\n",
       "  {64, 73, 176, 177},\n",
       "  {189, 191},\n",
       "  {24, 185, 191},\n",
       "  {27, 75},\n",
       "  {99, 163},\n",
       "  {190}],\n",
       " 'P11-1060': [{8, 11, 112},\n",
       "  {2, 112, 132},\n",
       "  {9, 25},\n",
       "  {21, 25, 45},\n",
       "  {21, 51},\n",
       "  {11, 112, 148},\n",
       "  {11, 13, 115},\n",
       "  {21, 112, 132},\n",
       "  {21, 25},\n",
       "  {10, 47, 94},\n",
       "  {36, 112, 132},\n",
       "  {42, 44, 157},\n",
       "  {106},\n",
       "  {21, 45, 167},\n",
       "  {88, 138, 172},\n",
       "  {171},\n",
       "  {25, 166},\n",
       "  {21, 40},\n",
       "  {8}],\n",
       " 'W06-2932': [{5, 19, 86},\n",
       "  {36, 79},\n",
       "  {57, 61, 79},\n",
       "  {76, 79},\n",
       "  {22, 45, 54},\n",
       "  {54, 104, 106},\n",
       "  {12},\n",
       "  {22, 57, 104},\n",
       "  {41, 58},\n",
       "  {22, 24, 64},\n",
       "  {41, 79},\n",
       "  {12, 21},\n",
       "  {43, 64},\n",
       "  {21, 104},\n",
       "  {41, 57},\n",
       "  {43},\n",
       "  {33}],\n",
       " 'P87-1015': [{118, 165, 205},\n",
       "  {118, 119, 229},\n",
       "  {3, 146, 149},\n",
       "  {118, 201},\n",
       "  {118, 151},\n",
       "  {118, 133, 222},\n",
       "  {2, 22, 23, 138},\n",
       "  {118, 156},\n",
       "  {118, 221, 222},\n",
       "  {54, 119},\n",
       "  {128, 133},\n",
       "  {164, 207},\n",
       "  {16, 204, 207},\n",
       "  {9, 16, 19},\n",
       "  {28, 119, 214},\n",
       "  {35, 118, 138}],\n",
       " 'W99-0623': [{85, 144},\n",
       "  {117, 120, 125},\n",
       "  {25, 72, 125},\n",
       "  {48, 120},\n",
       "  {38, 139},\n",
       "  {84, 91, 134},\n",
       "  {38, 76, 120},\n",
       "  {38, 120},\n",
       "  {25, 139},\n",
       "  {13, 25, 72},\n",
       "  {87, 103, 108},\n",
       "  {51, 139},\n",
       "  {70, 79, 98},\n",
       "  {27, 140},\n",
       "  {70, 77, 80, 81, 82},\n",
       "  {11, 70},\n",
       "  {98, 116},\n",
       "  {49, 85}],\n",
       " 'A00-2018': [{5, 17, 90, 91},\n",
       "  {5},\n",
       "  {17, 90, 91},\n",
       "  {39, 48, 49, 51, 120},\n",
       "  {90, 91, 92, 93, 94, 119},\n",
       "  {0, 95, 162},\n",
       "  {87, 90, 175},\n",
       "  {90, 91, 92, 93, 94},\n",
       "  {1, 38, 39, 40, 176},\n",
       "  {126, 174},\n",
       "  {12, 85, 101},\n",
       "  {63, 143, 146, 155, 174},\n",
       "  {17, 63, 78, 79},\n",
       "  {78, 90, 162},\n",
       "  {40, 91, 174},\n",
       "  {79, 180},\n",
       "  {40},\n",
       "  {175}]}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_off_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "names = ['aakansha','vardha','swastika','sweta']\n",
    "\n",
    "\n",
    "for d in doc_ids:\n",
    "    df = pd.read_csv(path+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8f370420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citance Number</th>\n",
       "      <th>Reference Article</th>\n",
       "      <th>Citing Article</th>\n",
       "      <th>Citation Marker Offset</th>\n",
       "      <th>Citation Marker</th>\n",
       "      <th>Citation Offset</th>\n",
       "      <th>Citation Text</th>\n",
       "      <th>Citation Text Clean</th>\n",
       "      <th>Reference Offset</th>\n",
       "      <th>Reference Text</th>\n",
       "      <th>Discourse Facet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>W05-1505</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Recent work by Nivre and Nilsson introduces a ...</td>\n",
       "      <td>Recent work by Nivre and Nilsson introduces a ...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>&lt;S sid=\"49\" ssid=\"20\"&gt;The baseline simply reta...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>P08-1006</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>1http: //sourceforge.net/projects/mstparser Fi...</td>\n",
       "      <td>Sagae and Tsujii (2007)'s dependency parser, b...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>&lt;S sid=\"109\" ssid=\"1\"&gt;We have presented a new ...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>W10-1401</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Bengoetxea and Gojenola (2010) discuss non-pro...</td>\n",
       "      <td>Bengoetxea and Gojenola (2010) discuss non-pro...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>&lt;S sid=\"95\" ssid=\"6\"&gt;The second main result is...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>P12-3029</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>For tree banks with non-projective trees weuse...</td>\n",
       "      <td>For tree banks with non-projective trees we us...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>&lt;S sid=\"79\" ssid=\"6\"&gt;In the first part of the ...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>W10-1403</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>It uses graph transformation to handle non-pro...</td>\n",
       "      <td>It uses graph transformation to handle non-pro...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>&lt;S sid=\"109\" ssid=\"1\"&gt;We have presented a new ...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>D08-1008</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>To simplify implementation, we instead opted f...</td>\n",
       "      <td>To simplify implementation, we instead opted f...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>&lt;S sid=\"38\" ssid=\"9\"&gt;Projectivizing a dependen...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>D07-1013</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>,wn in O (n) time, producing a projective depe...</td>\n",
       "      <td>Nivre and Nilsson (2005) showed how the restri...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>&lt;S sid=\"109\" ssid=\"1\"&gt;We have presented a new ...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>D07-1119</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>For handling non-projective relations, Nivre a...</td>\n",
       "      <td>For handling non-projective relations, Nivre a...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>&lt;S sid=\"79\" ssid=\"6\"&gt;In the first part of the ...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>N07-1050</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Whereas most of the early approaches were limi...</td>\n",
       "      <td>The most popular strategy for capturing non pr...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>&lt;S sid=\"109\" ssid=\"1\"&gt;We have presented a new ...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>W09-1207</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>troduced in (Nivre and Nilsson, 2005) to handl...</td>\n",
       "      <td>We adopt the pseudo-projective approach introd...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>S sid=\"109\" ssid=\"1\"&gt;We have presented a new m...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>E09-1034</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>non projective (Nivre and Nilsson, 2005), we c...</td>\n",
       "      <td>However, just as it has been noted that most n...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>S sid=\"109\" ssid=\"1\"&gt;We have presented a new m...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>W09-1218</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>In order to avoid losing the benefits of highe...</td>\n",
       "      <td>In order to avoid losing the benefits of highe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>C08-1081</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Pseudo-projective parsing for recovering non p...</td>\n",
       "      <td>Pseudo-projective parsing for recovering non p...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>&lt;S sid=\"86\" ssid=\"13\"&gt;As expected, the mos...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>C08-1081</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Although the parser only derives projective gr...</td>\n",
       "      <td>Although the parser only derives projective gr...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>&lt;S sid=\"95\" ssid=\"6\"&gt;The second main result is...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>C08-1081</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Pseudo-projective parsing was proposed by Nivr...</td>\n",
       "      <td>Pseudo-projective parsing was proposed by Nivr...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>&lt;S sid=\"109\" ssid=\"1\"&gt;We have presented a new ...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>C08-1081</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Weprojectivize training data by a minimal tran...</td>\n",
       "      <td>We projectivize training data by a minimal tra...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>&lt;S sid=\"51\" ssid=\"22\"&gt;In the second scheme, H...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>D11-1006</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>For tree banks with non-projective trees we us...</td>\n",
       "      <td>For tree banks with non-projective trees we us...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>&lt;S sid=\"99\" ssid=\"10\"&gt;This may seem surprising...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>P11-2121</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>Since the number of non-projective dependencie...</td>\n",
       "      <td>Since the number of non-projective dependencie...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>&lt;S sid=\"7\" ssid=\"3\"&gt;From the point of view of ...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>E06-1010</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Itshould be noted that the proportion of lost ...</td>\n",
       "      <td>It should be noted that the proportion of lost...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>P05-1013</td>\n",
       "      <td>D07-1111</td>\n",
       "      <td>0</td>\n",
       "      <td>Nivre and Nilsson, 2005</td>\n",
       "      <td>0</td>\n",
       "      <td>The resulting algorithm is projective, and non...</td>\n",
       "      <td>The resulting algorithm is projective, and non...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>&lt;S sid=\"2\" ssid=\"2\"&gt;We show how a datadriven d...</td>\n",
       "      <td>Aim_Citation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citance Number Reference Article Citing Article  Citation Marker Offset  \\\n",
       "0                1          P05-1013       W05-1505                       0   \n",
       "1                2          P05-1013       P08-1006                       0   \n",
       "2                3          P05-1013       W10-1401                       0   \n",
       "3                4          P05-1013       P12-3029                       0   \n",
       "4                5          P05-1013       W10-1403                       0   \n",
       "5                6          P05-1013       D08-1008                       0   \n",
       "6                7          P05-1013       D07-1013                       0   \n",
       "7                8          P05-1013       D07-1119                       0   \n",
       "8                9          P05-1013       N07-1050                       0   \n",
       "9               10          P05-1013       W09-1207                       0   \n",
       "10              11          P05-1013       E09-1034                       0   \n",
       "11              12          P05-1013       W09-1218                       0   \n",
       "12              13          P05-1013       C08-1081                       0   \n",
       "13              14          P05-1013       C08-1081                       0   \n",
       "14              15          P05-1013       C08-1081                       0   \n",
       "15              16          P05-1013       C08-1081                       0   \n",
       "16              17          P05-1013       D11-1006                       0   \n",
       "17              18          P05-1013       P11-2121                       0   \n",
       "18              19          P05-1013       E06-1010                       0   \n",
       "19              20          P05-1013       D07-1111                       0   \n",
       "\n",
       "            Citation Marker  Citation Offset  \\\n",
       "0   Nivre and Nilsson, 2005                0   \n",
       "1   Nivre and Nilsson, 2005                0   \n",
       "2   Nivre and Nilsson, 2005                0   \n",
       "3   Nivre and Nilsson, 2005                0   \n",
       "4   Nivre and Nilsson, 2005                0   \n",
       "5   Nivre and Nilsson, 2005                0   \n",
       "6   Nivre and Nilsson, 2005                0   \n",
       "7   Nivre and Nilsson, 2005                0   \n",
       "8   Nivre and Nilsson, 2005                0   \n",
       "9   Nivre and Nilsson, 2005                0   \n",
       "10  Nivre and Nilsson, 2005                0   \n",
       "11                      NaN                0   \n",
       "12  Nivre and Nilsson, 2005                0   \n",
       "13  Nivre and Nilsson, 2005                0   \n",
       "14  Nivre and Nilsson, 2005                0   \n",
       "15  Nivre and Nilsson, 2005                0   \n",
       "16  Nivre and Nilsson, 2005                0   \n",
       "17  Nivre and Nilsson, 2005                0   \n",
       "18                      NaN                0   \n",
       "19  Nivre and Nilsson, 2005                0   \n",
       "\n",
       "                                        Citation Text  \\\n",
       "0   Recent work by Nivre and Nilsson introduces a ...   \n",
       "1   1http: //sourceforge.net/projects/mstparser Fi...   \n",
       "2   Bengoetxea and Gojenola (2010) discuss non-pro...   \n",
       "3   For tree banks with non-projective trees weuse...   \n",
       "4   It uses graph transformation to handle non-pro...   \n",
       "5   To simplify implementation, we instead opted f...   \n",
       "6   ,wn in O (n) time, producing a projective depe...   \n",
       "7   For handling non-projective relations, Nivre a...   \n",
       "8   Whereas most of the early approaches were limi...   \n",
       "9   troduced in (Nivre and Nilsson, 2005) to handl...   \n",
       "10  non projective (Nivre and Nilsson, 2005), we c...   \n",
       "11  In order to avoid losing the benefits of highe...   \n",
       "12  Pseudo-projective parsing for recovering non p...   \n",
       "13  Although the parser only derives projective gr...   \n",
       "14  Pseudo-projective parsing was proposed by Nivr...   \n",
       "15  Weprojectivize training data by a minimal tran...   \n",
       "16  For tree banks with non-projective trees we us...   \n",
       "17  Since the number of non-projective dependencie...   \n",
       "18  Itshould be noted that the proportion of lost ...   \n",
       "19  The resulting algorithm is projective, and non...   \n",
       "\n",
       "                                  Citation Text Clean  Reference Offset  \\\n",
       "0   Recent work by Nivre and Nilsson introduces a ...              49.0   \n",
       "1   Sagae and Tsujii (2007)'s dependency parser, b...             109.0   \n",
       "2   Bengoetxea and Gojenola (2010) discuss non-pro...              95.0   \n",
       "3   For tree banks with non-projective trees we us...              79.0   \n",
       "4   It uses graph transformation to handle non-pro...             109.0   \n",
       "5   To simplify implementation, we instead opted f...              38.0   \n",
       "6   Nivre and Nilsson (2005) showed how the restri...             109.0   \n",
       "7   For handling non-projective relations, Nivre a...              79.0   \n",
       "8   The most popular strategy for capturing non pr...             109.0   \n",
       "9   We adopt the pseudo-projective approach introd...             109.0   \n",
       "10  However, just as it has been noted that most n...             109.0   \n",
       "11  In order to avoid losing the benefits of highe...               NaN   \n",
       "12  Pseudo-projective parsing for recovering non p...              86.0   \n",
       "13  Although the parser only derives projective gr...              95.0   \n",
       "14  Pseudo-projective parsing was proposed by Nivr...             109.0   \n",
       "15  We projectivize training data by a minimal tra...              51.0   \n",
       "16  For tree banks with non-projective trees we us...              99.0   \n",
       "17  Since the number of non-projective dependencie...               7.0   \n",
       "18  It should be noted that the proportion of lost...               NaN   \n",
       "19  The resulting algorithm is projective, and non...               2.0   \n",
       "\n",
       "                                       Reference Text  Discourse Facet  \n",
       "0   <S sid=\"49\" ssid=\"20\">The baseline simply reta...  Method_Citation  \n",
       "1   <S sid=\"109\" ssid=\"1\">We have presented a new ...  Method_Citation  \n",
       "2   <S sid=\"95\" ssid=\"6\">The second main result is...  Result_Citation  \n",
       "3   <S sid=\"79\" ssid=\"6\">In the first part of the ...  Method_Citation  \n",
       "4   <S sid=\"109\" ssid=\"1\">We have presented a new ...  Result_Citation  \n",
       "5   <S sid=\"38\" ssid=\"9\">Projectivizing a dependen...  Method_Citation  \n",
       "6   <S sid=\"109\" ssid=\"1\">We have presented a new ...  Result_Citation  \n",
       "7   <S sid=\"79\" ssid=\"6\">In the first part of the ...  Method_Citation  \n",
       "8   <S sid=\"109\" ssid=\"1\">We have presented a new ...  Result_Citation  \n",
       "9   S sid=\"109\" ssid=\"1\">We have presented a new m...  Result_Citation  \n",
       "10  S sid=\"109\" ssid=\"1\">We have presented a new m...  Result_Citation  \n",
       "11                                                NaN              NaN  \n",
       "12      <S sid=\"86\" ssid=\"13\">As expected, the mos...  Method_Citation  \n",
       "13  <S sid=\"95\" ssid=\"6\">The second main result is...  Method_Citation  \n",
       "14  <S sid=\"109\" ssid=\"1\">We have presented a new ...  Result_Citation  \n",
       "15   <S sid=\"51\" ssid=\"22\">In the second scheme, H...  Method_Citation  \n",
       "16  <S sid=\"99\" ssid=\"10\">This may seem surprising...  Result_Citation  \n",
       "17  <S sid=\"7\" ssid=\"3\">From the point of view of ...  Method_Citation  \n",
       "18                                                NaN              NaN  \n",
       "19  <S sid=\"2\" ssid=\"2\">We show how a datadriven d...     Aim_Citation  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'\n",
    "\n",
    "f = 'P05-1013_swastika.csv'\n",
    "df = pd.read_csv(path+f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc332e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "188cf79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citance Number</th>\n",
       "      <th>Reference Article</th>\n",
       "      <th>Citing Article</th>\n",
       "      <th>Citation Marker Offset</th>\n",
       "      <th>Citation Marker</th>\n",
       "      <th>Citation Offset</th>\n",
       "      <th>Citation Text</th>\n",
       "      <th>Citation Text Clean</th>\n",
       "      <th>Reference Offset</th>\n",
       "      <th>Reference Text</th>\n",
       "      <th>Discourse Facet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>N10-1002</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>As a benchmark VPC extraction system, we use t...</td>\n",
       "      <td>As a benchmark VPC extraction system, we use t...</td>\n",
       "      <td>'90' , '91'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>W11-0610</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Each of these scores can be calculated from a ...</td>\n",
       "      <td>Each of these scores can be calculated from a ...</td>\n",
       "      <td>'5'</td>\n",
       "      <td>&lt;S sid=\"5\" ssid=\"1\"&gt;We present a new parser fo...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>W06-3119</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>We then use Charniak? s parser (Charniak, 2000...</td>\n",
       "      <td>We then use Charniak's parser (Charniak, 2000)...</td>\n",
       "      <td>'90'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>N03-2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>We were interested in the occurrence of featur...</td>\n",
       "      <td>We were interested in the occurrence of featur...</td>\n",
       "      <td>'48','49','51'</td>\n",
       "      <td>&lt;S sid=\"48\" ssid=\"17\"&gt;Maximum-entropy models h...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>N06-1039</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>After getting a set of basic clusters, we pass...</td>\n",
       "      <td>After getting a set of basic clusters, we pass...</td>\n",
       "      <td>'90','91','92','93','94'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>C04-1180</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>The levels of accuracy and robustness recently...</td>\n",
       "      <td>The levels of accuracy and robustness recently...</td>\n",
       "      <td>0</td>\n",
       "      <td>“NA”</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>W05-0638</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>In CoNLL-2005, full parsing trees are provided...</td>\n",
       "      <td>In CoNLL-2005, full parsing trees are provided...</td>\n",
       "      <td>'90'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>P05-1065</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>We also use a standard statistical parser (Cha...</td>\n",
       "      <td>We also use a standard statistical parser (Cha...</td>\n",
       "      <td>'90','91','92','93','94'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>P05-1065</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>For each article, we calculated the per cent a...</td>\n",
       "      <td>For each article, we calculated the percentage...</td>\n",
       "      <td>'38','39','40'</td>\n",
       "      <td>&lt;S sid=\"38\" ssid=\"7\"&gt;To compute a probability ...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>P04-1040</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>The evaluation of the transformed output of th...</td>\n",
       "      <td>The evaluation of the transformed output of th...</td>\n",
       "      <td>'174'</td>\n",
       "      <td>&lt;S sid=\"174\" ssid=\"1\"&gt;We have presented a lexi...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>P04-1040</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>As an alternative to hard coded heuristics, Bl...</td>\n",
       "      <td>As an alternative to hard coded heuristics, Bl...</td>\n",
       "      <td>'85'</td>\n",
       "      <td>&lt;S sid=\"85\" ssid=\"54\"&gt;As partition-function ca...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>N06-1022</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>The parser of Charniak (2000) is also a two-st...</td>\n",
       "      <td>The parser of Charniak (2000) is also a two-st...</td>\n",
       "      <td>'63','143','146'</td>\n",
       "      <td>&lt;S sid=\"63\" ssid=\"32\"&gt;As we discuss in more de...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>N06-1022</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Most recently, McDonald et al (2005) have impl...</td>\n",
       "      <td>Most recently, McDonald et al (2005) have impl...</td>\n",
       "      <td>'78','79'</td>\n",
       "      <td>&lt;S sid=\"78\" ssid=\"47\"&gt;With some prior knowledg...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>H05-1035</td>\n",
       "      <td>0</td>\n",
       "      <td>Charniak, 2000</td>\n",
       "      <td>0</td>\n",
       "      <td>The feature set contains complex information e...</td>\n",
       "      <td>The feature set contains complex information e...</td>\n",
       "      <td>'90'</td>\n",
       "      <td>&lt;S sid=\"90\" ssid=\"1\"&gt;We created a parser based...</td>\n",
       "      <td>Method_Citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>A00-2018</td>\n",
       "      <td>P04-1042</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Note that the dependency figures of Dienes lag...</td>\n",
       "      <td>Note that the dependency figures of Dienes lag...</td>\n",
       "      <td>'174'</td>\n",
       "      <td>&lt;S sid=\"174\" ssid=\"1\"&gt;We have presented a lexi...</td>\n",
       "      <td>Result_Citation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Citance Number Reference Article Citing Article  Citation Marker Offset  \\\n",
       "0                2          A00-2018       N10-1002                       0   \n",
       "1                3          A00-2018       W11-0610                       0   \n",
       "2                4          A00-2018       W06-3119                       0   \n",
       "3                5          A00-2018       N03-2024                       0   \n",
       "4                6          A00-2018       N06-1039                       0   \n",
       "5                7          A00-2018       C04-1180                       0   \n",
       "6                8          A00-2018       W05-0638                       0   \n",
       "7                9          A00-2018       P05-1065                       0   \n",
       "8               10          A00-2018       P05-1065                       0   \n",
       "9               11          A00-2018       P04-1040                       0   \n",
       "10              13          A00-2018       P04-1040                       0   \n",
       "11              17          A00-2018       N06-1022                       0   \n",
       "12              18          A00-2018       N06-1022                       0   \n",
       "13              19          A00-2018       H05-1035                       0   \n",
       "14              20          A00-2018       P04-1042                       0   \n",
       "\n",
       "   Citation Marker  Citation Offset  \\\n",
       "0   Charniak, 2000                0   \n",
       "1   Charniak, 2000                0   \n",
       "2   Charniak, 2000                0   \n",
       "3   Charniak, 2000                0   \n",
       "4   Charniak, 2000                0   \n",
       "5             2000                0   \n",
       "6   Charniak, 2000                0   \n",
       "7   Charniak, 2000                0   \n",
       "8   Charniak, 2000                0   \n",
       "9             2000                0   \n",
       "10            2000                0   \n",
       "11            2000                0   \n",
       "12            2000                0   \n",
       "13  Charniak, 2000                0   \n",
       "14            2000                0   \n",
       "\n",
       "                                        Citation Text  \\\n",
       "0   As a benchmark VPC extraction system, we use t...   \n",
       "1   Each of these scores can be calculated from a ...   \n",
       "2   We then use Charniak? s parser (Charniak, 2000...   \n",
       "3   We were interested in the occurrence of featur...   \n",
       "4   After getting a set of basic clusters, we pass...   \n",
       "5   The levels of accuracy and robustness recently...   \n",
       "6   In CoNLL-2005, full parsing trees are provided...   \n",
       "7   We also use a standard statistical parser (Cha...   \n",
       "8   For each article, we calculated the per cent a...   \n",
       "9   The evaluation of the transformed output of th...   \n",
       "10  As an alternative to hard coded heuristics, Bl...   \n",
       "11  The parser of Charniak (2000) is also a two-st...   \n",
       "12  Most recently, McDonald et al (2005) have impl...   \n",
       "13  The feature set contains complex information e...   \n",
       "14  Note that the dependency figures of Dienes lag...   \n",
       "\n",
       "                                  Citation Text Clean  \\\n",
       "0   As a benchmark VPC extraction system, we use t...   \n",
       "1   Each of these scores can be calculated from a ...   \n",
       "2   We then use Charniak's parser (Charniak, 2000)...   \n",
       "3   We were interested in the occurrence of featur...   \n",
       "4   After getting a set of basic clusters, we pass...   \n",
       "5   The levels of accuracy and robustness recently...   \n",
       "6   In CoNLL-2005, full parsing trees are provided...   \n",
       "7   We also use a standard statistical parser (Cha...   \n",
       "8   For each article, we calculated the percentage...   \n",
       "9   The evaluation of the transformed output of th...   \n",
       "10  As an alternative to hard coded heuristics, Bl...   \n",
       "11  The parser of Charniak (2000) is also a two-st...   \n",
       "12  Most recently, McDonald et al (2005) have impl...   \n",
       "13  The feature set contains complex information e...   \n",
       "14  Note that the dependency figures of Dienes lag...   \n",
       "\n",
       "            Reference Offset  \\\n",
       "0                '90' , '91'   \n",
       "1                        '5'   \n",
       "2                       '90'   \n",
       "3             '48','49','51'   \n",
       "4   '90','91','92','93','94'   \n",
       "5                          0   \n",
       "6                       '90'   \n",
       "7   '90','91','92','93','94'   \n",
       "8             '38','39','40'   \n",
       "9                      '174'   \n",
       "10                      '85'   \n",
       "11          '63','143','146'   \n",
       "12                 '78','79'   \n",
       "13                      '90'   \n",
       "14                     '174'   \n",
       "\n",
       "                                       Reference Text  Discourse Facet  \n",
       "0   <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "1   <S sid=\"5\" ssid=\"1\">We present a new parser fo...  Method_Citation  \n",
       "2   <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "3   <S sid=\"48\" ssid=\"17\">Maximum-entropy models h...  Method_Citation  \n",
       "4   <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "5                                                “NA”  Result_Citation  \n",
       "6   <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "7   <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "8   <S sid=\"38\" ssid=\"7\">To compute a probability ...  Method_Citation  \n",
       "9   <S sid=\"174\" ssid=\"1\">We have presented a lexi...  Result_Citation  \n",
       "10  <S sid=\"85\" ssid=\"54\">As partition-function ca...  Method_Citation  \n",
       "11  <S sid=\"63\" ssid=\"32\">As we discuss in more de...  Method_Citation  \n",
       "12  <S sid=\"78\" ssid=\"47\">With some prior knowledg...  Method_Citation  \n",
       "13  <S sid=\"90\" ssid=\"1\">We created a parser based...  Method_Citation  \n",
       "14  <S sid=\"174\" ssid=\"1\">We have presented a lexi...  Result_Citation  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/A00-2018_aakansha.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b83bfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dadc3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P87-1015_sweta.csv',\n",
       " 'P05-1013_swastika.csv',\n",
       " 'J01-2004_sweta.csv',\n",
       " 'P08-1043_swastika.csv',\n",
       " 'P08-1043_sweta.csv',\n",
       " 'A00-2030_vardha.csv',\n",
       " 'P05-1013_aakansha.csv',\n",
       " 'W06-3114_sweta.csv',\n",
       " 'P11-1061_aakansha.csv',\n",
       " 'P11-1061_swastika.csv',\n",
       " 'P11-1061_sweta.csv',\n",
       " 'J01-2004_aakansha.csv',\n",
       " 'J01-2004_swastika.csv',\n",
       " 'W06-2932_swastika.csv',\n",
       " 'W11-2123_swastika.csv',\n",
       " 'A00-2030_aakansha.csv',\n",
       " 'E03-1005_sweta.csv',\n",
       " 'E03-1005_swastika.csv',\n",
       " 'A97-1014_sweta.csv',\n",
       " 'D10-1044_aakansha.csv',\n",
       " 'W99-0613_swastika.csv',\n",
       " 'A97-1014_vardha.csv',\n",
       " 'P08-1102_swastika.csv',\n",
       " 'W99-0613_vardha.csv',\n",
       " 'P08-1028_swastika.csv',\n",
       " 'W06-3114_aakansha.csv',\n",
       " 'P08-1028_sweta.csv',\n",
       " 'A00-2030_sweta.csv',\n",
       " 'W99-0623_sweta.csv',\n",
       " 'A00-2018_sweta.csv',\n",
       " 'D10-1044_sweta.csv',\n",
       " 'W06-3114_swastika.csv',\n",
       " 'P11-1060_sweta.csv',\n",
       " 'D10-1044_swastika.csv',\n",
       " 'A00-2018_aakansha.csv',\n",
       " 'P04-1036_swastika.csv',\n",
       " 'W11-2123_aakansha.csv',\n",
       " 'W06-2932_sweta.csv',\n",
       " 'W99-0613_sweta.csv',\n",
       " 'P05-1013_vardha.csv',\n",
       " 'W99-0623_swastika.csv',\n",
       " 'P11-1060_aakansha.csv',\n",
       " 'P08-1102_aakansha.csv',\n",
       " 'D09-1092_sweta.csv',\n",
       " 'E03-1005_aakansha.csv',\n",
       " 'P04-1036_vardha.csv',\n",
       " 'P08-1028_aakansha.csv',\n",
       " 'W99-0613_aakansha.csv',\n",
       " 'D09-1092_swastika.csv',\n",
       " 'P04-1036_aakansha.csv',\n",
       " 'W06-2932_vardha.csv',\n",
       " 'P87-1015_swastika.csv',\n",
       " 'P11-1060_swastika.csv',\n",
       " 'A97-1014_swastika.csv',\n",
       " 'P08-1043_aakansha.csv',\n",
       " 'P87-1015_vardha.csv',\n",
       " 'W99-0623_vardha.csv',\n",
       " 'P04-1036_sweta.csv',\n",
       " 'W11-2123_vardha.csv',\n",
       " 'D09-1092_vardha.csv',\n",
       " 'P08-1102_sweta.csv',\n",
       " 'A00-2018_vardha.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5081d69d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/P11-1060_aakanksha.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2036/2501086680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/P11-1060_aakanksha.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/P11-1060_aakanksha.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/ssd_scratch/cvit/dhawals1939/scisumm-2018/Test-Gold/Task1/P11-1060_aakanksha.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45663a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs['A00-2018']['ref_off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f61e0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']= '0'\n",
    "def evaluate(model, docs, thresh=0.6,topk=3,cs_based=True):\n",
    "    rouge1_lis = []\n",
    "    rouge2_lis = []\n",
    "    rouge_su4_lis = []\n",
    "    tp_big = 0\n",
    "    tot_relevent = 0\n",
    "    tot_retrieved = 0\n",
    "\n",
    "    for k in docs:\n",
    "        tp, rele,ret, rouge1, rouge2, rouge_su4 = get_matching_sentences(model,docs[k]['corpus'], docs[k]['cite_text'], docs[k]['ref_off'],thresh,topk,cs_based)\n",
    "        print(\"doc_id\",k,tp, rele,ret, rouge1, rouge2, rouge_su4)\n",
    "        \n",
    "        tp_big += tp\n",
    "        tot_relevent += rele\n",
    "        tot_retrieved += ret\n",
    "        rouge1_lis.append(rouge1)\n",
    "        rouge2_lis.append(rouge2)\n",
    "        rouge_su4_lis.append(rouge_su4)\n",
    "    recall = tp_big/tot_relevent\n",
    "    precision = tp_big/tot_retrieved\n",
    "    f1 = 2*recall*precision/(recall+precision+1e-10)\n",
    "    return recall,precision,f1, np.mean(rouge1_lis), np.mean(rouge2_lis), np.mean(rouge_su4_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52208c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': array(['word one language translated zero one several words languages',\n",
       "        'using word fertility features shown useful building word alignment models statistical machine translation',\n",
       "        'built fertility hidden markov model adding fertility hidden markov model',\n",
       "        'model achieves lower alignment error rate hidden markov model also runs faster',\n",
       "        'similar ways ibm model much easier understand',\n",
       "        'use gibbs sampling parameter estimation principled neighborhood method used ibm model',\n",
       "        'ibm models hidden markov model hmm word alignment influential statistical word alignment models brown et vogel et och ney',\n",
       "        'three kinds important information word alignment models lexicality locality fertility',\n",
       "        'ibm model uses lexical information ibm model hidden markov model take advantage lexical locality information ibm models use three kinds information remain state art despite fact developed almost two decades ago',\n",
       "        'recent experiments large datasets shown performance hidden markov model close ibm model',\n",
       "        'nevertheless believe ibm model essentially better model exploits fertility words tar get language',\n",
       "        'however ibm model complex researches use software package och ney ibm model treated black box',\n",
       "        'complexity ibm model makes hard understand improve',\n",
       "        'goal build model includes lexicality locality fertility time make easy understand',\n",
       "        'also want accurate computationally efficient',\n",
       "        'many years research word alignment',\n",
       "        'work different others essential ways',\n",
       "        'researchers take either hmm alignments liang et ibm model alignments cherry lin input perform whereas model potential replacement hmm ibm model',\n",
       "        'directly modeling fertility makes model fundamentally different others',\n",
       "        'models limited ability model fertility', 'liang et al',\n",
       "        'learn alignment translation directions jointly essentially pushing fertility towards',\n",
       "        'itg models wu assume fertility either zero one',\n",
       "        'model phrases phrase contiguous',\n",
       "        'works try simulate fertility using hidden markov model toutanova et deng byrne prefer model fertility directly',\n",
       "        'model coherent generative model combines hmm ibm model',\n",
       "        'easier understand ibm model see section',\n",
       "        'model also removes several undesired properties ibm model',\n",
       "        'use gibbs sampling instead neighborhood method parameter proceedings conference empirical methods natural language processing pages mit massachusetts usa october',\n",
       "        'qc association computational linguistics estimation',\n",
       "        'distortion parameters similar ibm model hmm ibm model uses inverse distortion brown et',\n",
       "        'model assumes fertility follows poisson distribution ibm model assumes multinomial distribution learn much larger number parameters makes slower less reliable',\n",
       "        'model much faster ibm model',\n",
       "        'fact show also faster hmm lower alignment error rate hmm',\n",
       "        'parameter estimation word alignment models model fertility difficult models without fertility',\n",
       "        'brown et al',\n",
       "        'och ney first compute viterbi alignments simpler models consider neighbors viterbi alignments modeling fertility',\n",
       "        'optimal alignment neighbors method able find opti total empty words hmm',\n",
       "        'moore also suggested adding multiple empty words target sentence ibm model',\n",
       "        'add empty words target sentence alignment mapping source target word positions j aj j',\n",
       "        'j', '', 'words position target sentence empty words',\n",
       "        'allow source word align exactly one target word target word may align multiple source words',\n",
       "        'fertility φi word ei position defined number aligned source words j mal alignment',\n",
       "        'use markov chain monte carlo mcmc method training decoding φi δ aj nice probabilistic guarantees',\n",
       "        'denero et al',\n",
       "        'applied markov chain monte carlo method word alignment machine translation model word fertility',\n",
       "        'δ kronecker delta function x δ x otherwise particular fertility empty words alignment fertility',\n",
       "        'target sentence φi define φǫ φi bilingual sentence pair given source sentence f j',\n",
       "        'fj f j φi φǫ j target sentence ei',\n",
       "        'ei define inverted alignments position tar alignments two sentences subset cartesian product word positions',\n",
       "        'following brown et al',\n",
       "        'assume source word aligned exactly one target word',\n",
       "        'get sentence set bi element bi aligned alignments bi',\n",
       "        'inverted alignments explicitly used ibm models model denote aj',\n",
       "        'aj alignments one reason model easier understand',\n",
       "        'f j ei word fj aligned word e aj',\n",
       "        'convenience add empty word ǫ target sentence position ǫ',\n",
       "        'however see add one empty word hmm', 'ibm model hmm',\n",
       "        'ibm model hmm generative models start defining probability alignments source sentence given order compute jump probability target sentence p ajj data likeli hmm model need know position hood computed summing alignments aligned target word previous source word',\n",
       "        'previous source word aligns empty word fj',\n",
       "        'align empty word fj alignswe could use position empty word indi empty word want record position target word aligns',\n",
       "        'possibilities fj cate nearest previous source word align empty word',\n",
       "        'reason use first word source sentence fj target word',\n",
       "        'aligns one ofp f j j p aj f j',\n",
       "        'alignwhere first two equations imply proba ments aj hidden variables',\n",
       "        'expectation maximization algorithm used learn parameters data likelihood maximized',\n",
       "        'without loss generality p aj f j bility jumping empty word either third equation implies probability jumping nonempty word probability jumping corespondent empty decomposed length probabilities distortion probabilities also called alignment probabilities lexical probabilities also called translation probabilities p aj f j j word',\n",
       "        'absolute position hmm important distortion probability terms distance adjacent alignment points vogel et och ney p j n p aj fj c p c j p j n p aj c count jumps given distance',\n",
       "        'ibm model word order mat ter',\n",
       "        'hmm likely align source p fj aj l p j length probability word target word adjacent previous aligned target word suitable ibm model adjacent words tend form aj distortion prob phrases',\n",
       "        'ability p fj j probability',\n",
       "        'aj e lexical two models theory fertility target word large length ibm model assumes uniform distortion probability length probability depends length target sentence lexical probability depends aligned target word j source sentence',\n",
       "        'practice fertility target word ibm model big except rare target words become garbage collector align many source words brown et och ney moore',\n",
       "        'hmm p aj f j p j n p f less likely garbage collector problem j j aj cause alignment probability constraint',\n",
       "        'however fertility inherent cross language propertythe hidden markov model assumes length prob ability depends length target sentence distortion probability depends previous alignment length target sentence lexical probability depends aligned target word p aj f j j p j n p aj p fj order make hmm work correctly enforce following constraints och ney two models assign consistent fertility words',\n",
       "        'motivation adding fertility two models expect resulting models perform better baseline models',\n",
       "        'hmm performs much better ibm model expect fertility hidden markov model perform much better fertility ibm model',\n",
       "        'throughout paper model refers fertility hidden markov model',\n",
       "        'due space constraints unable provide details ibm models see brown et al',\n",
       "        'och ney',\n",
       "        'want point locality property modeled hmm missing ibm model modeled invertedly ibm model',\n",
       "        'ibm model removes deficiency brown et och ney ibm model computationally expensive due larger number parameters ibm model ibm model often provides improvement alignment accuracy',\n",
       "        'fertility ibm model fertility hmm generative models start defining probability fertilities nonempty target word empty words alignments source sentence given target sentence p φi φǫ aj f j away mean low probability',\n",
       "        'ibm models use multinomial distribution fertility much larger number parameters learn',\n",
       "        'model one parameter target word learned reliably',\n",
       "        'fertility ibm model assume distortion probability uniform lexical probability depends aligned target word p φi φǫ aj f j data likelihood computed φi λ ei summing fertilities alignments n λ ei p f j j p φi φǫ aj f j',\n",
       "        'φi φǫ fertility nonempty word ei random variable φi assume φi follows poisson distribution poisson φi λ ei',\n",
       "        'sum fer λ ǫ φǫ λ ǫ φǫ',\n",
       "        'j tilities empty words φǫ grows length target sentence',\n",
       "        'therefore assume φǫ follows poisson distribution parameter λ ǫ',\n",
       "        'p φi φǫ aj f j decomposed j n p fj eaj following way p φi φǫ aj f j fertility hmm assume distor tion probability depends previous alignment length target sentence p φi p j lexical probability depends aligned target word n p aj fj φi φǫ p φi φǫ aj f j n λ ei ei φ λ e φi n λ ei λ ǫ φǫ λ ǫ φǫ',\n",
       "        'φ λ ǫ φǫ λ ǫ j n p aj φǫ',\n",
       "        'j φǫ n p aj p fj eaj p fj aj φi φǫ l superficially try model length accurately',\n",
       "        'however also en compute p f j sum force fertility target word across corpus consistent',\n",
       "        'expected fertility nonempty word ei λ ei expected fertil fertilities agree alignments ity empty words λ ǫ',\n",
       "        'fertility value p f j p aj f j nonzero probability fertility values j p aj f j auxiliar functio n l p f p λ e e p φi φǫ aj f j aj e f j log p aj f j φǫ j p φi φǫ aj f j e p f j e f n δ δ aj p j p aj f j exponential δ δ aj family get closed form parameters expected counts last two lines equation φǫ p f c f f e φi free variables determined f c f f e alignments',\n",
       "        'sum fer tilities consistent alignments p c f e j p f j model de c f e ficient similar ibm models brown et',\n",
       "        'remove deficiency fertility ibm model assuming different distortion λ e c e f e c f e probability distortion probability fertility number bilingual sentences andis consistent alignments uniform oth c f f j j j erwise',\n",
       "        'total number consistent fertility p j alignments j',\n",
       "        'replacing φǫ j φǫ j',\n",
       "        'j δ fj f δ ei e j c f j j aj j p φi φǫ aj f j j n λ ei φi ei λ ǫ φǫ λ ǫ c δ aj δ j j j j j n p fj j φ δ e e j j c f j k ei δ ei e experiments find noticeable change terms alignment accuracy removing deficiency',\n",
       "        'estimate parameters maximizing p f j using expectation maximization equations fertility hidden markov model',\n",
       "        'fertility ibm model need estimate distortion probability',\n",
       "        'although estimate parameters using em algorithm dempster et',\n",
       "        'em algorithm order compute expected counts sum possible unfortunately exponential',\n",
       "        'devel algorithm one iteration draw samples aj sentence pairoped gibbs sampling algorithm geman ge f j corpus man compute expected counts',\n",
       "        'j target sentence source sentence f j initialize alignment aj source word fj using viterbi alignments ibm model',\n",
       "        'training stage try possible alignments aj fix choose alignment aj probabil j corpus initialize aj ibm model j aj compute p aj f j ity p aj aj computed following way end p aj aj f j j j j draw sample aj using equation update counts p end j j aj p alignment variable aj choose samples',\n",
       "        'scan corpus many times satisfied parameters learned using equations',\n",
       "        'gibbs sampling method updates parameters constantly online learning algorithm',\n",
       "        'however sampling method needs large amount communication machines order keep parameters date compute expected counts parallel',\n",
       "        'instead batch learning fix parameters scan entire corpus compute expected counts parallel combine counts together update parameters step',\n",
       "        'analogous ibm models end end also consider initializing alignments using hmm viterbi algorithm',\n",
       "        'case fertility hidden markov model faster hmm',\n",
       "        'fortunately initializing using ibm model viterbi decrease accuracy noticeable way reduces complexity gibbs sampling algorithm',\n",
       "        'testing stage sampling algorithm except keep alignments maximize p',\n",
       "        'need hmm em algorithms',\n",
       "        'algorithm aj j j one machine machines independent algorithm',\n",
       "        'fertility hidden markov model updating p aj f j whenever change alignment aj done constant time complexity choosing samples aj j',\n",
       "        'j ti j', 'complexity hmm lower complexity constant',\n",
       "        'surprisingly achieve better results hmm computing sample alignment fertility hidden markov model much faster hmm',\n",
       "        'even choosing model times faster hmm achieve better results',\n",
       "        'fertility ibm model need compute', 'values identical empty words',\n",
       "        'samples testing stage unlikely get optimal alignments sampling times alignment',\n",
       "        'contrary training stage although samples accurate enough represent distribution defined equation alignment aj accurate enough computing expected counts defined corpus level',\n",
       "        'interestingly found throwing away fertility using hmm viterbi decoding achieves results sampling approach ignore difference tiny faster',\n",
       "        'therefore use gibbs sampling learning hmm viterbi decoder testing',\n",
       "        'gibbs sampling fertility ibm model similar simpler',\n",
       "        'omit details',\n",
       "        'al ig n en e l p r e r e n c n b b f h h f h f h mf b', '', '',\n",
       "        '', '', '', '', 'c n e n b b f h h f h f h mf b', '', '', '', '',\n",
       "        '', '', 'table aer results',\n",
       "        'refers fertility hmmf refers fertility hmm',\n",
       "        'choose fertility hmm',\n",
       "        'b b f h h f h f h f b figure aer comparison b b f h h f h f h f b figure aer comparison cn b b f h h f h f h f b figure training time comparison',\n",
       "        'training time model calculated scratch',\n",
       "        'example training time ibm model includes training time ibm model hmm ibm model',\n",
       "        'evaluated model computing word alignment machine translation quality',\n",
       "        'use alignment error rate aer word alignment evaluation criterion',\n",
       "        'let alignments output word alignment system p set possible alignments set sure alignments labeled human beings',\n",
       "        'subset p precision recall aer defined follows recall precision p aer p p aer extension',\n",
       "        'lower aer better',\n",
       "        'evaluate fertility models chineseenglish corpus',\n",
       "        'chineseenglish data taken fbis newswire data sentence pairs use first sentence pairs training data',\n",
       "        'used data reference', 'chineseenglish data sentence pairs',\n",
       "        'initialize ibm model fertility ibm model uniform distribution',\n",
       "        'smooth parameters λ e p f adding small value never become small',\n",
       "        'run models iterations',\n",
       "        'aer results computed using ibm model viterbi alignments viterbi alignments obtained gibbs sampling algorithm',\n",
       "        'initialize hmm fertility hmm parameters learned iteration ibm model',\n",
       "        'smooth parameters λ e p p f adding small value',\n",
       "        'run models iterations',\n",
       "        'aer results computed using traditional hmm viterbi decoding models',\n",
       "        'always difficult determine many samples enough sampling algorithms',\n",
       "        'however fertility models achieve better results baseline models using small amount samples',\n",
       "        'fertility ibm model sample times aj restart times training stage sample times restart times testing stage',\n",
       "        'fertility hmm sample times aj restarting training stage sampling testing stage use traditional hmm viterbi decoding testing',\n",
       "        'samples give improvement',\n",
       "        'initially fertility ibm model fertility hmm perform well',\n",
       "        'target word e appeared times training corpus model reliably estimate parameter λ e',\n",
       "        'hence smoothing needed',\n",
       "        'one may try solve forcing words share parameter λ einfrequent',\n",
       "        'unfortunately solve problem infrequent words tend larger fertility',\n",
       "        'solve problem following way estimate parameter λ enon empty nonempty words infrequent words share parameter',\n",
       "        'consider words appear less times infrequent words',\n",
       "        'table figure figure shows aer results different models',\n",
       "        'see fertility ibm model consistently outperforms ibm model fertility hmm consistently outperforms hmm',\n",
       "        'fertility hmm lower aer hmm also runs faster hmm',\n",
       "        'figure show training time different models',\n",
       "        'fact sample alignment model archives lower aer hmm runs times faster hmm',\n",
       "        'possible use sampling instead dynamic programming hmm reduce training time decrease aer often increase',\n",
       "        'conclude fertility hmm better aer results also runs faster hidden markov model',\n",
       "        'also evaluate model computing machine translation bleu score papineni et using moses system koehn et',\n",
       "        'training data word alignment evaluation bitexts alignments model symmetrized using heuristic',\n",
       "        'test sentences length four references',\n",
       "        'results shown table see better word alignment results lead better translations',\n",
       "        'model bleu hmm table bleu results',\n",
       "        'developed fertility hidden markov model runs faster lower aer hmm',\n",
       "        'model thus much faster ibm model',\n",
       "        'model also easier understand ibm model',\n",
       "        'markov chain monte carlo method used model principled neighborhood method ibm model',\n",
       "        'better word alignment results necessarily correspond better translation quality translation results comparable translation quality hmm ibm model',\n",
       "        'acknowledgments would like thank tagyoung chung matt post anonymous reviewers helpful comments',\n",
       "        'work supported nsf grants'], dtype=object),\n",
       " 'cite_text': ['zhao gildea explored model word order fertility model described based work em algorithm using gibbs sampling approximating expectations',\n",
       "  'recent work described extension hmm fertility model using mcmc techniques parameter estimation',\n",
       "  'fj word alignment vectors estimate posterior distribution using markov chain monte carlo methods gibbs sampling',\n",
       "  'following prior work augment standard hmm fertility distribution',\n",
       "  'prior work addressed using single parameter poisson distribution forcing infrequent words share global parameter estimated fertility words corpus',\n",
       "  'prior work compared viterbi form local search finding little difference two',\n",
       "  'zhao gildea use sampling proposed fertility extensions ibm model hmm place prior parameters',\n",
       "  'zhao proposes brief fertility based hmm also decreases complexity model fully bayesian inference word alignment table',\n",
       "  'models fertility computing expectations instead becomes intractable previous authors solved using approximative approximation consists ignoring dependence two draws word order jump distribution stling tiedemann ecient word alignment mcmc greedy optimization techniques local gibbs sampling',\n",
       "  'zhao gildea instead chose use gibbs sampling approximate expectations allowed perform efficient inference em hmm model fertility',\n",
       "  'another interesting extension hmm alignment presented zhao gildea added fertility distribution hmm',\n",
       "  'recent work described extension hmm fertility model using mcmc techniques parameter es timation',\n",
       "  'following prior work augment standard hmm fertility dis tribution',\n",
       "  'pr itp estimate posterior distribution using markov chain monte carlo methods gibbs sam pling',\n",
       "  'prior work addressed using single parameter pois son distribution forcing infrequent words share global parameter estimated fertility words corpus',\n",
       "  'prior work compared viterbi form local search finding little difference two',\n",
       "  'model easier implement recent experiments shown appropriately modified model produce comparable performance models',\n",
       "  'gibbs sampler similar mcmc algorithm zhao gildea assume dirichlet priors sampling model parameters take different sampling approach based source side dependency tree'],\n",
       " 'ref_off': [['3', '4', '6'],\n",
       "  ['18', '26', '33', '34'],\n",
       "  ['6', '46'],\n",
       "  ['86'],\n",
       "  ['88', '90'],\n",
       "  ['110'],\n",
       "  ['113', '114', '115'],\n",
       "  ['14', '26', '27'],\n",
       "  ['29', '30'],\n",
       "  ['29', '30', '107', '108'],\n",
       "  ['3'],\n",
       "  ['3', '46'],\n",
       "  ['86'],\n",
       "  ['6', '46'],\n",
       "  ['88', '90'],\n",
       "  ['110'],\n",
       "  ['3', '4', '5', '6'],\n",
       "  ['46']]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs['D10-1058']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20076e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': array(['present new parser parsing penn style parse trees achieves average sentences less length less trained tested previously established quot standard quot sections wall street journal treebank',\n",
       "        'represents decrease error rate best results corpus',\n",
       "        'major technical innovation use quot quot model conditioning smoothing let us successfully test combine many different conditioning events',\n",
       "        'also present partial results showing effects different conditioning information including surprising improvement due guessing lexical head guessing lexical head',\n",
       "        'present new parser parsing penn style parse trees achieves average sentences length sentences length trained tested previously established quot standard quot sections wall street journal',\n",
       "        'represents decrease error rate best results corpus',\n",
       "        'following parser based upon probabilistic generative model',\n",
       "        'sentences parses parser assigns probability p p r equality holding restrict consideration whose yield research supported part nsf grant lis sbr',\n",
       "        'author would like thank mark johnson rest brown laboratory linguistic information processing parser returns parse ir maximizes probability',\n",
       "        'parser implements function arg maxrp arg maxirp arg maxrp w',\n",
       "        'fundamentally distinguishes probabilistic generative parsers compute p r topic turn next',\n",
       "        'model assigns probability parse process considering constituent c ir c first guessing c c quot tag quot lexical head c h c expansion c constituents e c',\n",
       "        'thus probability parse given equation c label c whether noun phrase np etc h c relevant history c information outside c probability model deems important determining probability question',\n",
       "        'much interesting work determining goes h c',\n",
       "        'whenever clear constituent referring omit c h c',\n",
       "        'notation equation takes following form next describe assign probability expansion e constituent',\n",
       "        'section present results possible expansions constituent fixed advanced extracting grammar training corpus',\n",
       "        'method gives best results however uses markov grammar method assigning probabilities possible expansion using statistics gathered training corpus',\n",
       "        'method use follows',\n",
       "        'scheme traditional probabilistic grammar pcfg rule thought consisting side label c drawn symbols grammar side sequence one symbols',\n",
       "        'assume terminal symbols generated rules form quot preterm word quot treat special case',\n",
       "        'us symbols augmented symbols aux auxg assigned deterministically certain auxiliary verbs quot quot quot quot',\n",
       "        'expansion distinguish one side labels quot middle quot quot head quot symbol c',\n",
       "        'c constituent head lexical item h obtained according deterministic rules pick head constituent among heads children',\n",
       "        'left sequence one left labels li c including special termination symbol indicates symbols left similarly labels right ri c',\n",
       "        'thus expansion e c looks like expansion generated guessing first order l similarly ri pure markov pcfg given side label probabilistically generate side conditioning information possibly previously generated pieces side',\n",
       "        'simplest models zeroorder markov grammar label righthand side generated conditioned according distributions p li p p ri',\n",
       "        'generally one condition previously generated labels thereby obtaining markov grammar',\n",
       "        'example markov pcfg would conditioned complete model course probability label expansions also conditioned material specified equation p e h h',\n",
       "        'thus would use p h h',\n",
       "        'note ends expansion expression conditioned like label expansion',\n",
       "        'major problem confronting author generative parser information use condition probabilities required model smooth empirically obtained probabilities take sting sparse data problems inevitable even modest conditioning',\n",
       "        'example markov grammar conditioned label according distribution p h h',\n",
       "        'also remember h pla ceholder information beyond constituent c may useful assigning c probability',\n",
       "        'past years maximum entropy approach recommended probabilistic model builders flexibility novel approach smoothing',\n",
       "        'complete review models beyond scope paper',\n",
       "        'rather concentrate aspects models directly influenced model presented',\n",
       "        'compute probability model one first defines set quot features quot functions space configurations one trying compute probabilities integers denote number times pattern occurs input',\n",
       "        'work assume feature occur features pattern occur',\n",
       "        'parser assume features chosen certain feature schemata every feature boolean conjunction',\n",
       "        'example computing probability head might want feature schema f returns observed c label c zero otherwise',\n",
       "        'feature obviously composed two one recognizing',\n",
       "        'return feature returns',\n",
       "        'consider computing conditional probability p h set features h connect history model probability function takes following form ai weights negative positive infinity indicate relative importance feature relevant feature value probability higher absolute value associated',\n",
       "        'function z h called partition function normalizing constant fixed h probabilities sum one',\n",
       "        'purposes useful rewrite sequence multiplicative functions gi h j go h h gi h eai h fi',\n",
       "        'intuitive idea factor gi larger one feature question makes probability likely one feature effect smaller one makes probability less likely',\n",
       "        'models two benefits parser builder',\n",
       "        'first already implicit discussion factoring probability computation sequence values corresponding various quot features quot suggests probability model easily changeable change set features used',\n",
       "        'point emphasized ratnaparkhi discussing parser',\n",
       "        'second point yet mentioned features used models need particular independence one another',\n",
       "        'useful one using loglinear model smoothing',\n",
       "        'suppose want compute conditional probability p b c sure enough examples conditioning event b c training corpus ensure empirically obtained probability p b c accurate',\n",
       "        'traditional way handle also compute p b perhaps p c well take combination values one best estimate p b c',\n",
       "        'method known quot deleted interpolation quot smoothing',\n",
       "        'models one simply include features three events b c b c combine model according equation equivalently equation',\n",
       "        'fact features far independent concern',\n",
       "        'let us note get equation exactly form equation following fashion note first term equation gives probability based upon little conditioning information subsequent term number zero positive infinity greater smaller one new information considered makes probability greater smaller previous estimate',\n",
       "        'stands last equation pretty much',\n",
       "        'let us look works particular case parsing scheme',\n",
       "        'consider probability distribution choosing head constituent',\n",
       "        'equation wrote p h',\n",
       "        'discuss detail section several different features context surrounding c useful include h label head head parent c denoted lp tp hp label c left sibling lb quot quot label grandparent c la',\n",
       "        'wish compute p lp tp lb lg hp',\n",
       "        'rewrite form equation follows sequentially conditioned steadily increasing portions c history',\n",
       "        'many cases clearly warranted',\n",
       "        'example seem make much sense condition say hp without first conditioning ti',\n",
       "        'cases however seem conditioning apples oranges speak',\n",
       "        'example one well imagine one might want condition parent lexical head without conditioning left sibling grandparent label',\n",
       "        'one way modify simple version shown equation allow note changes last three terms equation',\n",
       "        'rather conditioning term previous ones conditioned aspects history seem relevant',\n",
       "        'hope less difficulty splitting conditioning events thus somewhat less difficulty sparse data',\n",
       "        'make one point connection equation maximum entropy formulation',\n",
       "        'suppose fact going compute true maximum entropy model based upon features used equation ii lp',\n",
       "        '',\n",
       "        'requires finding appropriate ais equation accomplished using algorithm iterative scaling ii values ai initially quot guessed quot modified converge stable values',\n",
       "        'prior knowledge values ai one traditionally starts ai neutral assumption feature neither positive negative impact probability question',\n",
       "        'prior knowledge values greatly speed process fewer iterations required convergence',\n",
       "        'comment example substantially speed process choosing values picked equation expressed form equation gi initial values values corresponding terms equation',\n",
       "        'experience rather requiring iterations three suffice',\n",
       "        'observe use approach run iterative scaling zero times would fact equation',\n",
       "        'major advantage using equation one generally get away without computing partition function z h',\n",
       "        'simple form equation clear z h',\n",
       "        'interesting version equation true general one would expect differ much one assume long publishing raw probabilities would example publishing perplexity results difference one unimportant',\n",
       "        'calculation typically major computational problem models simplifies model significantly',\n",
       "        'naturally distributions required equation used without smoothing',\n",
       "        'pure model done feature selection ratnaparkhi parser',\n",
       "        'could smoothed fashion choose instead use standard deleted interpolation',\n",
       "        'actually use minor variant described',\n",
       "        'created parser based upon model last section smoothed using standard deleted interpolation',\n",
       "        'generative model use standard probabilistic chart parser use chart parser first pass generate candidate possible parses evaluated second pass probabilistic model',\n",
       "        'runs generative model based upon markov grammar statistics first pass uses statistics conditioned standard pcfg information',\n",
       "        'allows second pass see expansions present training corpus',\n",
       "        'use gathered statistics observed words even low counts though obviously deleted interpolation smoothing gives less emphasis observed probabilities rare words',\n",
       "        'guess preterminals words observed training data using statistics capitalization hyphenation word endings last two letters probability given realized using previously unobserved word',\n",
       "        'noted probability model uses five smoothed probability distributions one li ri equation unsmoothed conditional probability distribution given equation',\n",
       "        'four equations found longer version paper available author website',\n",
       "        'l r conditioned three previous labels using markov grammar',\n",
       "        'also label parent constituent lp conditioned upon even obviously related conditioning events',\n",
       "        'due importance factor parsing noted',\n",
       "        'keeping standard methodology used penn wall street journal sections training section testing section development debugging tuning',\n",
       "        'performance test corpus measured using standard measures',\n",
       "        'particular measure labeled precision lp recall lr average number crossbrackets per sentence cb percentage sentences zero cross brackets ocb percentage sentences cross brackets',\n",
       "        'standard take separate measurements sentences length sentences length',\n",
       "        'note definitions labeled precision recall given used previous work',\n",
       "        'noted definitions typically give results higher obvious ones',\n",
       "        'results new parser well previous individual parsers corpus given figure',\n",
       "        'typical standard measures tell pretty much story new parser outperforming three parsers',\n",
       "        'looking particular precision recall figures new parser give us error reduction best previous work',\n",
       "        'previous sections concentrated relation parser maximumentropy approach aspect parser novel',\n",
       "        'however think aspect sole even important reason comparative success',\n",
       "        'list believe significant contributions give experimental results well program behaves without',\n",
       "        'take starting point parser labled figure program current parser derives',\n",
       "        'parser stated figure achieves average',\n",
       "        'noted system based upon quot grammar quot grammar read directly training corpus',\n",
       "        'opposed quot markovgrammar quot approach used current parser',\n",
       "        'also earlier parser uses two techniques employed current parser',\n",
       "        'first uses clustering scheme words give system quot soft quot clustering heads',\n",
       "        'quot soft quot clustering word belong one cluster different weights weights express probability producing word given one going produce word cluster',\n",
       "        'second uses unsupervised learning original system run thirty million words unparsed text output taken quot correct quot statistics collected resulting parses',\n",
       "        'without enhancements performs level sentences length',\n",
       "        'section evaluate effects various changes made running various versions current program',\n",
       "        'avoid repeated evaluations based upon testing corpus evaluation based upon sentences length development corpus',\n",
       "        'note corpus somewhat difficult quot official quot test corpus',\n",
       "        'example final version system achieves average test corpus average development corpus',\n",
       "        'indicated figure model labeled quot best quot precision recall average lower results official test corpus',\n",
       "        'accord experience developmentcorpus results lower obtained test corpus',\n",
       "        'model labeled quot old quot attempts recreate system using current program',\n",
       "        'makes use special features though presence made much easier perform experiments guess guessing lexical head uses grammar rather markov grammar',\n",
       "        'parser achieves average',\n",
       "        'consistent average mentioned latter test corpus former development corpus',\n",
       "        'old model best model figure gives measurements several different versions parser',\n",
       "        'one first without doubt significant change made current parser move two stages probabilistic decisions node three',\n",
       "        'already noted first guesses lexical head constituent given head guesses pcfg rule used expand constituent question',\n",
       "        'contrast current parser first guesses head head expansion',\n",
       "        'turns usefulness process already discovered collins turn notes personal communication previously used eisner',\n",
       "        'however collins stress decision guess head first might lost casual reader',\n",
       "        'indeed lost present author went back fact found',\n",
       "        'figure show one factor improves performance nearly',\n",
       "        'may obvious make great difference since words effectively unambiguous',\n",
       "        'example tagging using probable preterminal word accurate',\n",
       "        'believe two factors contribute performance gain',\n",
       "        'first simply first guess go guess head first thing condition upon compute p h',\n",
       "        'quantity relatively intuitive one example quantity used pcfg relate words seems particularly good condition upon since use effect unsmoothed probability upon smoothing p h based',\n",
       "        'one quot fix quot makes slightly percent difference results',\n",
       "        'second major reason first guessing makes much difference used backing lexical head computing probability rule expansion',\n",
       "        'example first guess lexical head move computing p r h p r h',\n",
       "        'even word quot conflating quot appear training corpus quot ng quot ending allows program guess relative security word vbg thus probability various rule expansions considerable sharpened',\n",
       "        'example pcfg probability rule quot vp vbg np quot whereas condition fact lexical head vbg get probability',\n",
       "        'second modification explicit marking noun coordination',\n",
       "        'already noted importance conditioning parent label',\n",
       "        'example information np conditioned parent vp pp etc',\n",
       "        'note np part np coordinate structure parent np similarly vp',\n",
       "        'nps vps occur np vp parents structures well',\n",
       "        'example penn treebank vp main auxiliary verbs structure shown figure',\n",
       "        'note subordinate vp vp parent',\n",
       "        'thus np vp parents constituents marked indicate parents coordinate structure',\n",
       "        'vp coordinate structure defined constituent two vp children one constituents comma cc conjp conjunctive phrase nothing else coordinate np phrases defined similarly',\n",
       "        'something much like done',\n",
       "        'shown figure conditioning information gives improvement',\n",
       "        'believe mostly due improvements guessing head',\n",
       "        'given already level accuracy judge improvement much worth',\n",
       "        'next add less obvious conditioning events noted previous discussion final model grandparent label lg left sibling label',\n",
       "        'using conditioning get another improvement average indicated figure line labeled quot',\n",
       "        'note also tried including information using standard model',\n",
       "        'results shown line quot standard interpolation quot',\n",
       "        'including information within standard model causes decrease results using less conventional model',\n",
       "        'indeed resulting performance worse using information',\n",
       "        'point models considered section grammar models',\n",
       "        'pcfg grammar rules read directly training corpus',\n",
       "        'already noted best model uses approach',\n",
       "        'one see figure firstorder markov grammar aforementioned improvements performs slightly worse equivalent parser',\n",
       "        'however grammar slightly better grammar significantly better parser',\n",
       "        'presented lexicalized markov grammar parsing model achieves using standard sections penn treebank average sentences length sentences length',\n",
       "        'corresponds error reduction best previously published single parser results test set collins',\n",
       "        'previous three best parsers test perform within percentage point despite quite different basic mechanisms led researchers wonder might maximum level parsing performance could obtained using treebank training conjecture perhaps',\n",
       "        'results reported disprove conjecture',\n",
       "        'results achieved combining aforementioned parsers also suggest limit trained parsers much higher previously thought',\n",
       "        'indeed may adding new parser mix may yield still higher results',\n",
       "        'perspective perhaps two important numbers come research overall error reduction results intermediateresult improvement nearly labeled due simple idea guessing head guessing head',\n",
       "        'neither results anticipated start research',\n",
       "        'noted main methodological innovation presented quot quot model conditioning smoothing',\n",
       "        'two aspects model deserve comment',\n",
       "        'first slight important improvement achieved using model conventional deleted interpolation indicated figure',\n",
       "        'expect experiment semantic conditioning information importance aspect model increase',\n",
       "        'important eyes though flexibility model',\n",
       "        'though respects quite flexible true maximum entropy much simpler estimation benefits comes smoothing',\n",
       "        'ultimately flexibility let us try various conditioning events move markov grammar approach try several markov grammars different orders without significant programming',\n",
       "        'indeed initiated line work attempt create parser would flexible enough allow modifications parsing semantic levels detail',\n",
       "        'project future parsing work devoted'], dtype=object),\n",
       " 'cite_text': [],\n",
       " 'ref_off': []}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs['A00-2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3069ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id A00-2018 0 0 0 nan nan nan\n",
      "doc_id P05-1013 0 0 0 nan nan nan\n",
      "doc_id D10-1044 0 0 0 nan nan nan\n",
      "doc_id P87-1015 0 0 0 nan nan nan\n",
      "doc_id P08-1102 0 0 0 nan nan nan\n",
      "doc_id W06-3114 0 0 0 nan nan nan\n",
      "doc_id E03-1005 0 0 0 nan nan nan\n",
      "doc_id W99-0623 0 0 0 nan nan nan\n",
      "doc_id P08-1028 0 0 0 nan nan nan\n",
      "doc_id W99-0613 0 0 0 nan nan nan\n",
      "doc_id A97-1014 0 0 0 nan nan nan\n",
      "doc_id P11-1060 0 0 0 nan nan nan\n",
      "doc_id P11-1061 0 0 0 nan nan nan\n",
      "doc_id W06-2932 0 0 0 nan nan nan\n",
      "doc_id A00-2030 0 0 0 nan nan nan\n",
      "doc_id P08-1043 0 0 0 nan nan nan\n",
      "doc_id D09-1092 0 0 0 nan nan nan\n",
      "doc_id J01-2004 0 0 0 nan nan nan\n",
      "doc_id W11-2123 0 0 0 nan nan nan\n",
      "doc_id P04-1036 0 0 0 nan nan nan\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_377/1691230943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_su4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcs_based\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metrics obtained in train: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_su4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_377/2012807472.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, docs, thresh, topk, cs_based)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrouge2_lis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrouge_su4_lis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_su4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_big\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtot_relevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_big\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtot_retrieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,cs_based=False,topk=10)   \n",
    "print(\"metrics obtained in train: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.65)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs['C08-1098']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.6)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb10c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,thresh=0.55)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall,precision,f1, rouge1, rouge2, rouge_su4 = evaluate(model,test_docs,cs_based=False)   \n",
    "print(\"metrics obtained in test: recall {}, precision {}, f1-score {}, rouge1 {}, rouge2 {}, rouge_su4 {}\".format(recall,precision,f1, rouge1, rouge2, rouge_su4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
