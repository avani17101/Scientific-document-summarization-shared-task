{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c2f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 15:43:18.167643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:/usr/local/apps/cuDNN/7.6.5-cuda-10.2/lib64\n",
      "2021-12-07 15:43:18.167688: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import torch.nn as nn\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import ast\n",
    "# from argparse import ArgumentParser\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument(\"-multi_lb_classi\", default=1, type=int, help=\"whether train a multi-lable classifier or individual classifiers for each class\")\n",
    "# opt = parser.parse_args()\n",
    "multi_lb_classi = 0\n",
    "docs = os.listdir(\"./scisumm-2018/Training\")\n",
    "train_e = int(0.8*len(docs))\n",
    "train_docs = docs[:train_e]\n",
    "test_docs = docs[train_e:]\n",
    "# test_docs  = os.listdir(\"./scisumm-2018/Test\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(example_sent):\n",
    "    global stop_words\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bea1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files, folder):\n",
    "    data_lis = []\n",
    "    labels = []\n",
    "    for z,f in enumerate(files):\n",
    "        data = None\n",
    "        ann = None\n",
    "        a_folder = folder+f+\"/annotation/\"\n",
    "        file = os.listdir(a_folder)[0]\n",
    "\n",
    "        ann = a_folder+file\n",
    "        with open(ann,\"r\") as f:\n",
    "            data = f.read()\n",
    "#         print(f)\n",
    "            facet = re.findall(\"Discourse Facet:\\s+([^|]*)\", data)\n",
    "            for f in facet:\n",
    "                if '[' in f:\n",
    "                    f = ast.literal_eval(f)\n",
    "                labels.append(f)\n",
    "\n",
    "            \n",
    "            ref_text = re.findall(\"Reference Text:\\s+([^|]*)\", data)\n",
    "            pattern = r'\\<.*?\\>'\n",
    "            pattern2 = r'\\(.*?\\)'\n",
    "            for ref in ref_text:\n",
    "                ref = re.sub(pattern2,'',re.sub(pattern, '', ref))\n",
    "                ref = preprocess(ref)\n",
    "                data_lis.append(ref)\n",
    "\n",
    "            \n",
    "    return data_lis, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2885f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_dataset(test_docs):\n",
    "#     '''\n",
    "#     gives citance and labels\n",
    "#     '''\n",
    "#     data_lis = []\n",
    "#     labels = []\n",
    "#     for z,f in enumerate(files):\n",
    "#         df = folder+f+\"/annotation/\"+f+\".csv\"\n",
    "#         data_lis.append()\n",
    "\n",
    "# test_data,test_labels = get_dataset(test_docs[:2],folder='scisumm-2018/Test/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1be2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = get_dataset(train_docs,\"scisumm-2018/Training/\")\n",
    "test_data, test_labels = get_dataset(test_docs,\"scisumm-2018/Training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d5c7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper propose probabilistic admixture model capture latent topics underlying context pairs',\n",
       " 'formalism parallel within assumed constitute mixture hidden topics follows bilingual translation paper propose probabilistic admixture model capture latent topics underlying context pairs',\n",
       " 'propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment translation lexicon p f',\n",
       " 'translation lexicons learned',\n",
       " 'two retrieval schemes designed bitams alignment alignment takes intersection two directions generates alignments',\n",
       " 'translation lexicons bf e k potential size v assuming vocabulary sizes languages v data sparsity poses serious problem estimating bf e k monolingual case instance reduce data sparsity problem introduce two remedies laplace smoothing',\n",
       " 'formalism parallel within assumed constitute mixture hidden topics follows bilingual translation bitam models proposed capture topic sharing different levels linguistic granularity propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment smt',\n",
       " 'preliminary experiments show proposed models improve word alignment accuracy lead better translation level topical information may help disambiguate translation candidates choices',\n",
       " 'beyond level topical information may help disambiguate translation candidates latent dirichlet allocation viewed special case target sentence n p f n n',\n",
       " 'beyond level topical information may help disambiguate translation candidates example word shot nice translated differently depending context sentence goal context sports photo within context sightseeing',\n",
       " 'propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment smt',\n",
       " 'previous works topical translation models concern mainly explicit logical representations semantics machine propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment smt',\n",
       " 'propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment smt',\n",
       " 'propose new statistical formalism bilingual topic admixture model bitam facilitate word alignment start baseline model alignment models embedded similarly within proposed coupling bitam likely improve coherency translations treating document whole entitynotably bitam allows test alignments two directions chinese shown table baseline gives best performance ce direc tion uda alignments give respectively significantly better',\n",
       " 'reduce data sparsity problem introduce two remedies interpolation employ linear interpolation avoid overfitting',\n",
       " 'foma compiler programming language regular library designed multipurpose use explicit support automata theoretic research constructing lexical analyzers programming languages building analyzers well spellchecking applications',\n",
       " 'compiler allows users specify automata transducers incrementally similar fashion amp fsm lextools state toolkit sfst toolkit',\n",
       " 'foma licensed gnu general public license keeping traditions free software distribution includes source code comes user manual library examples',\n",
       " 'though main concern foma efficiency compatibility extendibility usefulness perspective important avoid bottlenecks underlying algorithms cause compilation times skyrocket especially constructing combining large lexical transducers',\n",
       " 'foma licensed gnu general public license keeping traditions free software distribution includes source code comes user manual library examples',\n",
       " 'foma largely compatible toolkit',\n",
       " 'foma largely compatible toolkit',\n",
       " 'makes straightforward build morphological transducers simply extracting range transduction matching words approximately',\n",
       " 'thefigure graph accuracy full model best hyperparameter setting iteration',\n",
       " 'way restrict parameterization language original case english danish dutch german spanish swedish portuguese table upper bound tagging accuracy assuming word type assigned majority pos tag',\n",
       " 'empirical results demonstrate tagger rivals taggers employ sophisticated learning mechanisms exploit similar constraints',\n",
       " 'consider unsupervised pos induction problem without use tagging dictionary',\n",
       " 'languages make use tagging dictionary',\n",
       " 'experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts',\n",
       " 'simply assigning word frequent associated tag corpus achieves accuracy wsj portion penn treebank',\n",
       " 'consider unsupervised pos induction problem without use tagging dictionary',\n",
       " 'consider unsupervised pos induction problem without use tagging dictionary',\n",
       " 'hypothesize modeling morphological information greatly constrain set possible tags thereby refining representation tag lexicon',\n",
       " 'hypothesize modeling morphological information greatly constrain set possible tags thereby refining representation tag lexicon',\n",
       " 'first directly encodes linguistic intuitions pos tag assignments model structure reflects property level tag prior captures skew tag assignments tag prior next assume exists single prior distribution Ïˆ tag assignments drawn training treat observed language word types w well corpus utilize gibbs sampling approximate collapsed model posterior',\n",
       " 'table multilingual results report accuracy variety languages several experimental settings',\n",
       " 'presented method unsupervised tagging considers word type allowed pos tags primary element model',\n",
       " 'model starts generating tag assignment word type vocabulary assuming one tag per level hmm emission parameters drawn conditioned assignments word allowed probability mass single assigned tag',\n",
       " 'presented method unsupervised tagging considers word type allowed pos tags primary element model',\n",
       " 'present commentary consists commentaries allgemeine zeitung german regional daily',\n",
       " 'corpus german newspaper commentaries assembled potsdam university annotated different linguistic information different degrees',\n",
       " 'corpus annotated six different types information characterized following layers produced texts possible illustration look noun phrase marked topic bridging relation noun phrase',\n",
       " 'portion pcc found average coherence relations rst annotations explicitly signalled lexical adding fact connectives often ambiguous one conclude prospects automatic analysis rhetorical structure using shallow methods bright see sections',\n",
       " 'corpus german newspaper commentaries assembled potsdam university annotated different linguistic information different degrees',\n",
       " 'follow guidelines developed tiger project syntactic annotation german newspaper text using tool interactive construction tree structures',\n",
       " 'use discourse parser pcc texts emulating instance adds information annotations',\n",
       " 'paper however provides comprehensive overview data collection effort current present commentary consists commentaries allgemeine zeitung german regional choice genre commentary resulted fact investigation rhetorical structure interaction aspects discourse structure prospects automatic derivation key motivations building corpus',\n",
       " 'corpus german newspaper commentaries assembled potsdam university annotated different linguistic information different degrees',\n",
       " 'corpus german newspaper commentaries assembled potsdam university annotated different linguistic information different aspects corpus presented previous papers underspecified rhetorical structure perspective summarization',\n",
       " 'paper explains design decisions taken annotations describes number applications using corpus present commentary consists commentaries allgemeine zeitung german regional daily',\n",
       " 'present commentary consists commentaries allgemeine zeitung german regional corpus annotated six different types information characterized following layers produced texts portion pcc found average coherence relations rst annotations explicitly signalled lexical adding fact connectives often ambiguous one conclude prospects automatic analysis rhetorical structure using shallow methods bright see sections',\n",
       " 'commentaries annotated rhetorical structure using definitions discourse relations provided rhetorical structure theory',\n",
       " 'commentaries argue favor specific point view toward political issue often dicussing yet dismissing points view therefore typically offer interesting rhetorical structure say narrative text portions newspapers',\n",
       " 'corpus german newspaper commentaries assembled annotated different information syntax rhetorical structure connectives information structure',\n",
       " 'show cases estimated probability tight',\n",
       " 'ea h lt estimator natural quot relative frequency quot estimator',\n",
       " 'show cases estimated probability tight',\n",
       " 'corpus unparsed iterative approach estimation question arises get actual probabilities estimated pcfg apos assign mass infinite trees show cases estimated probability tight',\n",
       " 'joint segmentation parsing',\n",
       " 'joint segmentation parsing',\n",
       " 'gold segmentation available application settings segmenter parser arranged pipeline',\n",
       " 'joint segmentation parsing',\n",
       " 'finally show application settings absence gold segmentation lowers parsing performance',\n",
       " 'finally provide realistic eval uation segmentation performed pipeline jointly parsing',\n",
       " 'finally provide realistic eval uation segmentation performed pipeline jointly parsing head modif er dir gold label gold np np tag r adjp r sbar np np adj p r frag np np n p r vp np np sba r r np np p p r pp vp tag p p r np np np tag l advp vp tag sba r r whn p vp n p l major phrasal categories major pos categories ten lowest scoring dependencies occurring times table per category performance berkeley parser sentence lengths dev set results sentences length',\n",
       " 'investigate influence factors analyze modern standard arabic unusual opportunity presents comparison english parsing results',\n",
       " 'next show atb similar banks gross statistical terms annotation consistency remains low relative english use linguistic annotation insights develop manually annotated grammar arabic masdar lacks determiner constituent whole resem bles ubiquitous annexation construct f containsverb especially effective distinguishing root nodes equational sentences',\n",
       " 'propose limit words arabic parsing evaluations',\n",
       " 'grammar features realized annotations basic category start noun features since written arabic contains high proportion nps',\n",
       " 'use rules specified native speaker',\n",
       " 'establishing significantly higher parsing baselines shown arabic parsing performance poor previously thought remains much lower english',\n",
       " 'table shows mada produces high quality segmentation effect cascading segmentation errors parsing',\n",
       " 'investigate influence factors analyze modern standard arabic unusual opportunity presents comparison english parsing results',\n",
       " 'investigate influence factors analyze modern standard arabic unusual opportunity presents comparison english parsing results',\n",
       " 'investigate influence factors analyze modern standard arabic unusual opportunity presents comparison english parsing results',\n",
       " 'show coordination ambiguity arabic significant source parsing errors',\n",
       " 'next show atb similar banks gross statistical terms annotation consistency',\n",
       " 'show coordination ambiguity arabic significant source parsing errors',\n",
       " 'lattice parsing alternative pipeline prevents cascading errors placing segmentation options parse extend stanford parser accept lattices word represented finite state automaton',\n",
       " 'better arabic parsing baselines evaluations analysis',\n",
       " 'finally provide realistic eval uation segmentation performed pipeline jointly parsing',\n",
       " 'better arabic parsing baselines evaluations analysis',\n",
       " 'lattice parsing alternative pipeline prevents cascading errors placing segmentation options parse extend stanford parser accept lattices word represented finite state provide realistic eval uation segmentation performed pipeline jointly parsing',\n",
       " 'preprocessing raw trees improves parsing performance first discard trees dominated x indicates errors text',\n",
       " 'table shows mada produces high quality segmentation effect cascading segmentation errors parsing',\n",
       " 'table shows mada produces high quality segmentation effect cascading segmentation errors parsing',\n",
       " 'intuition role discourse marker usually de corpus split code http',\n",
       " 'phrasal level remove function tags traces',\n",
       " 'penn arabic treebank syntactic guidelines purposefully borrowed without major modification english',\n",
       " 'itowever problem method unitication result graph consists newly created unnecessary often input snbgraphs used part result graph without modification sharable parts one input graphs result sharable parts called redundant copying',\n",
       " 'furthermore structure sharing increases portion token identical substructures fss makes efficient keep unification results substructures fss reuse reduces repeated calculation substructures',\n",
       " 'called strategic incremental copy graph unification method uses early failure finding strategy first tries unify ubstructures tending fail unification method based stochastic data tim likelihood failure apos educes unnecessary computation',\n",
       " 'method theretbre failure tendency information acquired learning sing unification method applied analysis system uses failure tendency information acquired learning analysis learning process fs unification applied feature treatment orders randomized sake random extraction',\n",
       " 'paper proposes fs unification method allows structure sharing constant apos der node access method achieves structure sharing introducing lazy copying wroblewski apos incremental copy graph unification unification tl anti defined greatest lower bound meet',\n",
       " 'copying sharable parts called redundant structure sharing increases portion token identical substructures fss makes efficient keep unification results substructures fss reuse',\n",
       " 'furthermore structure sharing increases portion token identical substructures fss makes efficient keep unification results substructures fss reuse',\n",
       " 'example spoken analysis system based llpsg kogure uses elapsed time fs unification',\n",
       " 'ling unification method achieves structure sharing without data access overhead pereira apos unnecessary often input snbgraphs used part result graph without modification sharable parts one input graphs result sharable parts called redundant better method would nfinimize copying sharable varts',\n",
       " 'unnecessary often input snbgraphs used part result graph without modification sharable parts one input graphs result sharable parts called redundant better method would nfinimize copying sharable varts',\n",
       " 'copying sharable parts called redundant better method would nfinimize copying sharable varts',\n",
       " 'copying sharable parts called redundant disables structure sharing ttowever whole copying necessary lazy evaluation method method possible delay copying node either contents need change found arc node hat needs copied node x lt c gt fig',\n",
       " 'better method would nfinimize copying sharable varts',\n",
       " 'relation defined using metalanguage regular expressions suitable compiler regular expression source code compiled transducer shown figure implements relation computationally',\n",
       " 'insight kataja koskenniemi applied beesley morphological analyzer arabic first using implementation simulated intersection stems code runtime ran rather slowly later using xerox technology new implementation intersected stems compile time performed well runtime',\n",
       " 'significant experiments malay much larger application arabic shown value technique handling two classic examples morphotactics reduplication semitic stem interdigitation',\n",
       " 'replace algorithm reapplies expression compiler output compiling substrings intermediate network replacing result compilation',\n",
       " 'work directly related current solution kataja koskenniemi first demonstrated semitic roots could formalized regular languages interdigitation stems could elegantly formalized intersection regular languages',\n",
       " 'technique implemented algorithm called already proved useful handling malay stem reduplication arabic stem interdigitation described',\n",
       " 'regular expression calculus several operators involve concatenation',\n",
       " 'merge merge algorithm operation combines two regular languages template filler single one',\n",
       " 'technique described implemented algorithm allows compiler reapply modify output effectively freeing morphotactic description use operation',\n",
       " 'insight kataja koskenniemi applied beesley morphological analyzer arabic first using implementation simulated intersection stems code runtime ran rather slowly later using xerox technology new implementation intersected stems compile time performed well runtime',\n",
       " 'operations simulated apply routine runtime',\n",
       " 'algorithm merge operator introduced paper general techniques limited handling specific',\n",
       " 'next step examine investigate situations one tagger suggests estimate probability situation tag actually tx',\n",
       " 'order see whether combination component tuggers likely lead improvements tagging quality first examine results individual taggers applied accept measuring quality relation specific tagging',\n",
       " 'next step examine investigate situations one tagger suggests estimate probability situation tag actually used test pairwise voting strategy clearly outperforms voting strategies yet approach level tying majority votes handled correctly',\n",
       " 'democratic option give tagger one vote general quality tagger votes overall precision quality relation current situation tagger votes precision suggested tag next step examine combining taggers every tagger pair taken turn allowed vote possible tag ones suggested component taggers',\n",
       " 'democratic option give tagger one vote',\n",
       " 'experiment shows least task hand combination several different systems allows us raise performance ceiling data driven systems',\n",
       " 'however appears useful give weight taggers proved general quality tagger votes overall precision quality relation current situation tagger votes precision suggested tag',\n",
       " 'important observation every combination outperforms combination strict subset note improvement yielded best pairwise voting system using four individual taggers scores correct test reduction error rate best individual system viz',\n",
       " 'however appears useful give weight taggers proved general quality tagger votes overall precision quality relation current situation tagger votes precision suggested tag',\n",
       " 'first choice use based second level examine overtraining effects specific particular second level classifier also used system commercial version program induction decision trees training material',\n",
       " 'next step examine investigate situations one tagger suggests estimate probability situation tag actually tx',\n",
       " 'simple voting many ways results component taggers combined selecting single tag set proposed appears useful give weight taggers proved quality',\n",
       " 'second stage provided first level outputs additional information original input pattern',\n",
       " 'likely overtraining effect tune probably small collect case bases leverage stacking effect convincingly especially since second stage material shows disagreement featured important observation every combination outperforms combination strict subset note improvement yielded best pairwise voting system using four individual taggers scores correct test reduction error rate best individual system closer investigation feel results encouraging enough extend investigation combination starting additional component taggers selection strategies going shifts tagsets languages',\n",
       " 'second part tune consists data used select best tagger parameters applicable develop combination taggers correct majority correct correct present majority minority correct taggers wrong table tagger agreement likely overtraining effect tune probably small collect case bases leverage stacking effect convincingly especially since second stage material shows disagreement featured tags',\n",
       " 'four tagger generators trained corpus comparison outputs combined using several voting strategies second stage classifiers measurements far appears use detailed information leads better accuracy improvement',\n",
       " 'next step examine investigate situations one tagger suggests estimate probability situation tag actually practice feeding outputs number classifiers features next learner sit significantly better usually called stacking none based methods reaches quality tagpair',\n",
       " 'used test pairwise voting strategy clearly outperforms voting strategies yet approach level tying majority votes handled correctly none based methods reaches quality tagpair',\n",
       " 'data use experiment consists tagged lob corpus',\n",
       " 'four tagger generators trained corpus accuracy individual taggers combination methods',\n",
       " 'first choice use based second level none based methods reaches quality examine overtraining effects specific particular second level classifier also used system commercial version program induction decision trees training prunes decision tree overfitting training material less learning results test also worse',\n",
       " 'compare lexical phrase dependency syntax features well novel com date qg used word alignment adaptation projection parsing various monolingual recognition scoring tasks paper represents first application mt proceedings conference empirical methods natural language processing pages singapore august',\n",
       " 'compare lexical phrase dependency syntax features well novel com date qg used word alignment adaptation projection parsing various monolingual recognition scoring tasks paper represents first application mt proceedings conference empirical methods natural language processing pages singapore august',\n",
       " 'compare lexical phrase dependency syntax features well novel com date qg used word alignment adaptation projection parsing various monolingual recognition scoring tasks paper represents first application mt proceedings conference empirical methods natural language processing pages singapore august',\n",
       " 'compare lexical phrase dependency syntax features well novel com date qg used word alignment adaptation projection parsing various monolingual recognition scoring tasks paper represents first application mt proceedings conference empirical methods natural language processing pages singapore august',\n",
       " 'compare lexical phrase dependency syntax features well novel com date qg used word alignment adaptation projection parsing various monolingual recognition scoring tasks paper represents first application mt proceedings conference empirical methods natural language processing pages singapore august',\n",
       " 'novel decoder based efficient qg lattice parsing extended handle features using generic techniques also support efficient parameter estimation',\n",
       " 'use one feature configurations adding additional features score configura phrase syntactic features features att f val qg phr table feature set comparison',\n",
       " 'present machine translation framework incorporate arbitrary features input output sentences',\n",
       " 'figure decoding lattice parsing translation denoted black lattice arcs thicker blue arcs forming dependency tree',\n",
       " 'take first steps toward universal decoder making following contributions arbitrary feature model define sin gle direct translation model encodes popular mt features used encode features source target sentences dependency trees alignments',\n",
       " 'findings show phrase features dependency syntax produce complementary improvements translation quality tree configurations helpful translation substantial gains obtained permitting certain types isomorphism',\n",
       " 'decoding framework allows us perform many experiments feature representation inference algorithms including combining comparing features examining isomorphism constraints synchronous formalisms affect translation output',\n",
       " 'automatically detect lexical cohesion tics pairwise words three linguistic features considered word repetition collocation relation weights',\n",
       " 'word repetition combination collocation three features combination also achieved precision rate attained lower recall rate',\n",
       " 'automatically detect lexical cohesion tics pairwise words three linguistic features considered word repetition collocation relation weights',\n",
       " 'automatically detect lexical cohesion tics pairwise words three linguistic features considered word repetition collocation relation weights',\n",
       " 'another approach extracted semantic information roget apos thesaurus automatically detect lexical cohesion tics pairwise words three linguistic features considered word repetition collocation relation weights',\n",
       " 'reiteration subdivided four cohesive effects word repetition synonym includes hyponym superordinate general word repetition component lexical cohesion class reiteration collocation lexical cohesion class repetition word repetition ties lexical cohesion identified word matches matches inflections derived stem',\n",
       " 'combination features word repetition relation weights produced best precision recall rates used isolation performance feature inferior combined fact provides evidence different lexical relations detected linguistic feature considered',\n",
       " 'proposed segmentation algorithm compares adjacent windows sentences determines lexical similarity',\n",
       " 'proposed segmentation algorithm compares adjacent windows sentences determines lexical similarity',\n",
       " 'automatically detect lexical cohesion tics pairwise words three linguistic features considered word repetition collocation relation weights',\n",
       " 'wall street journal archives example consist series articles different subject data distinct topics useful information retrieval segments relevant user apos query retrieved',\n",
       " 'lexical cohesion expressed vocabulary used text semantic relations semantic relations text useful indicator conceptual structure',\n",
       " 'treat spelling correction task word try two ways combining components decision lists bayesian classifiers',\n",
       " 'try two ways combining components decision lists bayesian classifiers',\n",
       " 'table shows performance baseline method confusion sets',\n",
       " 'paper takes yarowsky apos method starting point hypothesizes improvements obtained taking account single strongest piece evidence available try two ways combining components decision lists bayesian classifiers',\n",
       " 'method presented based bayesian apply two component methods mentioned context words collocations',\n",
       " 'try two ways combining components decision lists bayesian classifiers',\n",
       " 'table shows performance baseline method confusion performance figures given based training method brown corpus francis testing corpus wall street journal text marcus et',\n",
       " 'method presented based bayesian work reported applied accent restoration related lexical biguation task spelling correction',\n",
       " 'method presented based bayesian ambiguity among words modelled confusion confusion set c w wn means word wi set ambiguous word c program sees occurrence either desert dessert target document takes ambiguous desert dessert tries infer context two',\n",
       " 'probability wi calculated using bayes apos rule stands likelihood term p difficult estimate training data would count situations entire context previously observed around word wi raises severe therefore assume presence one word context independent presence word',\n",
       " 'paper takes yarowsky apos method starting point hypothesizes improvements obtained taking account single strongest piece evidence available work reported applied accent restoration related lexical biguation task spelling method based bayesian classifiers applied task spelling correction found outperform component methods well decision lists',\n",
       " 'try two ways combining components decision lists bayesian classifiers',\n",
       " 'method presented based bayesian classifiers',\n",
       " 'method presented based bayesian apply two component methods mentioned context words collocations',\n",
       " 'table shows performance baseline method confusion sets',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling performance figures given based training method brown corpus kucera',\n",
       " 'performance figures given based training method brown corpus francis testing corpus wall street journal text marcus et',\n",
       " 'method presented based bayesian classifiers',\n",
       " 'thus c program sees occurrence either desert dessert target document takes ambiguous desert dessert tries infer context two',\n",
       " 'method presented based bayesian idea discriminate among words wi confusion set identifying collocations tend occur around w ambiguous target word classified finding collocations match context',\n",
       " 'method based bayesian classifiers applied task spelling correction found outperform component methods well decision lists',\n",
       " 'trigrams worst words confusion set part cases bayesian hybrid method clearly better',\n",
       " 'try two ways combining components decision lists bayesian classifiers',\n",
       " 'method presented based bayesian classifiers',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling correction',\n",
       " 'apply two component methods mentioned context words collocations',\n",
       " 'method presented based bayesian work reported applied accent restoration related lexical biguation task spelling correction',\n",
       " 'paper takes yarowsky apos method starting point hypothesizes improvements obtained taking account single strongest piece evidence available evidence',\n",
       " 'method presented based bayesian classifiers',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling try two ways combining components decision lists bayesian classifiers',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling performance figures given based training method brown corpus kucera',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling correction',\n",
       " 'yarowsky used following metric calculate strength feature f reliability',\n",
       " 'treat spelling correction task word disambiguation',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling try two ways combining components decision lists bayesian classifiers',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling try two ways combining components decision lists bayesian classifiers',\n",
       " 'work reported applied accent restoration related lexical biguation task spelling correction',\n",
       " 'detect different areas meaning local graphs use cluster algorithm graphs developed van dongen',\n",
       " 'following method build graph node represents noun two nodes edge lists given number times detect different areas meaning local graphs use cluster algorithm graphs developed van dongen',\n",
       " 'following method build graph node represents noun two nodes edge lists given number times',\n",
       " 'following method build graph node represents noun two nodes edge lists given number times',\n",
       " 'detect different areas meaning local graphs use cluster algorithm graphs developed van dongen',\n",
       " 'algorithm based graph model representing words relationships clusters iteratively computed clustering local graph similar words around ambiguous word',\n",
       " 'preliminary observations show different neighbours table used indicate great accuracy senses used',\n",
       " 'following method build graph node represents noun two nodes edge lists given number times',\n",
       " 'based intuition nouns list often semantically related extract contexts form noun noun noun quot genomic dna rat mouse dog quot',\n",
       " 'detect different areas meaning local graphs use cluster algorithm graphs developed van dongen',\n",
       " 'local graph step consists w ni neighbours w neighbours neighbours since iteration attempt find quot best quot cluster suffices build relatively small graph',\n",
       " 'gives rise automatic unsupervised word sense disambiguation algorithm trained data disambiguated',\n",
       " 'detect different areas meaning local graphs use cluster algorithm graphs developed van dongen algorithm based graph model representing words relationships clusters iteratively computed clustering local graph similar words around ambiguous word',\n",
       " 'focus work use contextual role knowledge coreference resolution',\n",
       " 'developed coreference resolver called babar uses contextual role knowledge make coreference employs information extraction techniques represent learn role evaluated babar two domains terrorism natural disasters',\n",
       " 'representation contextual roles based information extraction patterns converted simple caseframes',\n",
       " 'developed coreference resolver called babar uses contextual role knowledge make coreference employs information extraction techniques represent learn role pattern represents role noun phrase plays surrounding context',\n",
       " 'finally dempstershafer probabilistic model evaluates evidence provided knowledge sources candidate antecedents makes final resolution decision',\n",
       " 'using heuristic babar identifies existential definite nps training corpus using previous learning algorithm resolves occurrences existential np syntactic seeding babar also uses syntactic heuristics identify anaphors antecedents easily bush disclosed policy reading',\n",
       " 'babar uses information extraction patterns identify contextual roles creates four contextual role knowledge sources using unsupervised learning',\n",
       " 'present coreference resolver called babar uses contextual role knowledge evaluate possible antecedents anaphor focus work use contextual role knowledge coreference describe four contextual role knowledge sources created training examples applied autoslog system unannotated training texts generate set extraction patterns extraction pattern represents linguistic expression syntactic position indicating role filler knowledge source called wordsemcfsem analogous cflex checks whether anaphor candidate antecedent substitutable one another based semantic classes instead words',\n",
       " 'contextual role represents role noun phrase plays event relationship',\n",
       " 'representation contextual roles based information extraction patterns converted simple caseframe network first type contextual role knowledge babar learns caseframe network identifies caseframes resolutions',\n",
       " 'representation contextual roles based information extraction patterns converted simple caseframes',\n",
       " 'focus work use contextual role knowledge coreference resolution',\n",
       " 'finally dempstershafer probabilistic model evaluates evidence provided knowledge sources candidate antecedents makes final resolution decision',\n",
       " 'babar uses information extraction patterns identify contextual roles creates four contextual role knowledge sources using unsupervised learning',\n",
       " 'representation contextual roles based information extraction patterns converted simple caseframes',\n",
       " 'unsupervised learning contextual role knowledge coreference resolutionthese knowledge sources determine whether contexts surrounding anaphor antecedent evaluated babar two domains terrorism natural disasters',\n",
       " 'babar uses information extraction patterns identify contextual roles creates four contextual role knowledge sources using unsupervised learning',\n",
       " 'babar uses information extraction patterns identify contextual roles creates four contextual role knowledge sources using unsupervised knowledge sources determine whether contexts surrounding anaphor antecedent compatible',\n",
       " 'focus work use contextual role knowledge coreference resolution',\n",
       " 'candidate antecedent babar identifies caseframe would extract candidate pairs caseframe consults cf network see pair caseframes previous cf network reports anaphor candidate may coreferent',\n",
       " 'focus work use contextual role knowledge coreference resolution',\n",
       " 'table syntactic seeding heuristics reliable case resolution heuristics produced substantial set resolutions training data used learn contextual role confidence level used belief value knowledge babar performs reliable case resolution identify anaphora easily resolved using lexical syntactic heuristics described section measure score increased domains reflecting substantial increase recall small decrease contextual role knowledge greatest impact pronouns recall terrorism recall disasters precision gain terrorism small precision drop disasters',\n",
       " 'another line research tries squeeze many features possible relatively small dataset',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'another line research tries squeeze many features possible relatively small dataset',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'two features language models require intersecting synchronous cfg automata representing language models grammar parsed efficiently using cube pruning',\n",
       " 'hiero hierarchical string translation system',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'results add growing body evidence mira preferable mert across languages systems even tasks',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'results add growing body evidence mira preferable mert across languages systems even tasks',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'results add growing body evidence mira preferable mert across languages systems even tasks',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'results add growing body evidence mira preferable mert across languages systems even tasks',\n",
       " 'following chiang et al calculate sentence b scores context previous translations',\n",
       " 'another line research tries squeeze many features possible relatively small dataset',\n",
       " 'hiero hierarchical string translation system',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'following chiang et al calculate sentence b scores context previous translations',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'hiero hierarchical string translation system',\n",
       " 'incorporate new features linear model train using mira following previous work',\n",
       " 'hiero hierarchical string translation system',\n",
       " 'another line research tries squeeze many features possible relatively small dataset',\n",
       " 'two features language models require intersecting synchronous cfg automata representing language models grammar parsed efficiently using cube pruning',\n",
       " 'add features improve based mt systemalready single system nist chineseenglish trackby also add features hiero obtain b improvement',\n",
       " 'another line research tries squeeze many features possible relatively small dataset',\n",
       " 'two features language models require intersecting synchronous cfg automata representing language models grammar parsed efficiently using cube pruning',\n",
       " 'work described also makes use hidden markov model',\n",
       " 'model containing refinements described tested using magazine article containing sentences word dictionary used supplemented inflectional analysis words found directly dictionary',\n",
       " 'alternative uniformly increasing order conditioning extend order context modeled introducing explicit state arrangement basic network remains permitting possible category sequences modeling dependency',\n",
       " 'paper describes refinements currently investigated model assignment words unrestricted text',\n",
       " 'mixed order context modeled introducing explicit state basic network augmented extra state sequences model certain category sequences detail',\n",
       " 'regard word equivalence classes used assumed distribution use word depends set categories assume words partitioned accordingly',\n",
       " 'ranked list words corpus frequent words account approximately total tokens corpus thus data available estimate frequent words corpus assigned individually model thereby enabling different distributions categories',\n",
       " 'model advantage training corpus required',\n",
       " 'stochastic method assigning categories unrestricted english text described',\n",
       " 'regard word equivalence classes used assumed distribution use word depends set categories assume words partitioned accordingly',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'hypothesis resulting lowest average ter score aligned hypotheses chosen skeleton follows number systems',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'confusion networks connected single start node null arcs contain prior probability system used skeleton network confusion network connected common end node null final arcs probability one',\n",
       " 'confusion network decoding mt pick one hypothesis skeleton determines word order hypotheses aligned votes form confidences assigned word network',\n",
       " 'test set first set weights used generate list expanded list higher order second set weights used find final list',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'also confusion networks generated using hypothesis systems skeleton used prior probabilities derived average ter guarantees best path found network generated system zero weight',\n",
       " 'translation edit rate proposed intuitive evaluation met since based rate edits required transform hypothesis reference',\n",
       " 'generic weight tuning algorithm may used optimize various automatic evaluation metrics including ter bleu algorithm explores better weights iteratively starting set initial dimension optimized using line minimization new direction based changes objective function estimated speed search',\n",
       " 'using lists system words may assigned different score based rank hypothesis',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'test set first set weights used generate list expanded list higher order second set weights used find final list',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'work confusion networks generated using output system skeleton prior probabilities network estimated average ter scores skeleton hypotheses',\n",
       " 'new method evaluated arabic english chinese english nist baseline new method improves bleu scores significantly combination weights tuned optimize three automatic evaluation metrics ter bleu ter tuning seems yield good results arabic bleu tuning seems better also seems like meteor used tuning due high insertion rate low precision',\n",
       " 'confusion network may represented word lattice standard tools may used generate hypothesis lists including word confidence scores language model scores features',\n",
       " 'translation edit rate proposed intuitive evaluation met since based rate edits required transform hypothesis reference',\n",
       " 'modified levenshtein alignment used ter natural simple edit distance word error rate since machine translation hypotheses may different word orders meaning',\n",
       " 'prevent system low zero weight selected skeleton confusion networks generated system average ter score equation used estimate prior probability corresponding confusion networks connected single start node null arcs contain prior probability system used skeleton network confusion network connected common end node null arcs',\n",
       " 'work probabilities estimated confusion network arc instead using votes simple word confidences',\n",
       " 'due computational burden ter alignment hypotheses considered possible skeletons hypotheses per system approach estimate word posteriors adopted work',\n",
       " 'confusion network may represented word lattice standard tools may used generate hypothesis lists including word confidence scores language model scores features',\n",
       " 'work consists investigating impact multiword expressions applications focusing compound nouns information retrieval systems whether adequate treatment expressions bring possible improvements indexing expressions',\n",
       " 'selection appropriate indexing terms key factor quality ir systems',\n",
       " 'extensive use multiword expressions natural language texts prompts detailed studies aim adequate treatment expressions',\n",
       " 'task aimed explore contribution disambiguation words bilingual monolingual ir',\n",
       " 'one motivations work investigate identification appropriate treatment multiword expressions application contributes improve results ultimately lead precise interaction',\n",
       " 'example query pop star meaning celebrity terms indexed individually relevant documents may retrieved system would proceedings workshop multiword expressions parsing generation real world pages portland oregon usa june',\n",
       " 'paper perform evaluation inclusion mwe treatment information retrieval system',\n",
       " 'although language processing vital modern ir systems may convenient scenario nlp techniques may contribute selection mwes indexing single units ir system',\n",
       " 'automatic discovery specific types mwes attracted attention many researchers nlp past years',\n",
       " 'used zettair generate ranked list documents retrieved response used cosine metric calculate scores rank documents',\n",
       " 'built fertility hidden markov model adding fertility hidden markov model achieves lower alignment error rate hidden markov model also runs use gibbs sampling parameter estimation principled neighborhood method used ibm model',\n",
       " 'researchers take either hmm alignments ibm model alignments input perform whereas model potential replacement hmm ibm model model coherent generative model combines hmm ibm model model much faster ibm model fact show also faster hmm lower alignment error rate hmm',\n",
       " 'use gibbs sampling parameter estimation principled neighborhood method used ibm model use markov chain monte carlo method training decoding nice probabilistic guarantees',\n",
       " 'fertility ibm model fertility hmm generative models start defining probability fertilities alignments source sentence given target sentence p away mean low probability',\n",
       " 'model one parameter target word learned fertility nonempty word ei random variable assume follows poisson distribution poisson',\n",
       " 'j target sentence source sentence f j initialize alignment aj source word fj using viterbi alignments ibm model',\n",
       " 'gibbs sampling method updates parameters constantly online learning sampling method needs large amount communication machines order keep parameters date compute expected counts batch learning fix parameters scan entire corpus compute expected counts parallel combine counts together update parameters',\n",
       " 'goal build model includes lexicality locality fertility time make easy model coherent generative model combines hmm ibm model easier understand ibm model',\n",
       " 'use gibbs sampling instead neighborhood method parameter proceedings conference empirical methods natural language processing pages mit massachusetts usa october association computational linguistics estimation',\n",
       " 'use gibbs sampling instead neighborhood method parameter proceedings conference empirical methods natural language processing pages mit massachusetts usa october association computational linguistics estimate parameters using algorithm em algorithm order compute expected counts sum possible unfortunately exponential',\n",
       " 'built fertility hidden markov model adding fertility hidden markov model',\n",
       " 'built fertility hidden markov model adding fertility hidden markov use markov chain monte carlo method training decoding nice probabilistic guarantees',\n",
       " 'fertility ibm model fertility hmm generative models start defining probability fertilities alignments source sentence given target sentence p away mean low probability',\n",
       " 'use gibbs sampling parameter estimation principled neighborhood method used ibm model use markov chain monte carlo method training decoding nice probabilistic guarantees',\n",
       " 'model one parameter target word learned fertility nonempty word ei random variable assume follows poisson distribution poisson',\n",
       " 'j target sentence source sentence f j initialize alignment aj source word fj using viterbi alignments ibm model',\n",
       " 'built fertility hidden markov model adding fertility hidden markov model achieves lower alignment error rate hidden markov model also runs similar ways ibm model much easier use gibbs sampling parameter estimation principled neighborhood method used ibm model',\n",
       " 'use markov chain monte carlo method training decoding nice probabilistic guarantees',\n",
       " 'global features extracted occurrences token whole document',\n",
       " 'paper show maximum entropy framework able make use global information directly achieves performance comparable best previous machine ners test data',\n",
       " 'paper show maximum entropy framework able make use global information directly achieves performance comparable best previous machine ners test data',\n",
       " 'shown maximum entropy framework able use global information directly',\n",
       " 'case zone similarly initcaps feature set etc token information group consists features based string listed table',\n",
       " 'named entity recognizer useful many nlp applications information extraction question answering etc ner also provide users looking person organization names quick information',\n",
       " 'differs previous machine ners uses information whole document classify word one work involves gathering information whole document often uses secondary classifier corrects mistakes primary based classifier',\n",
       " 'global features extracted occurrences token whole document',\n",
       " 'global features extracted occurrences token whole document',\n",
       " 'paper show maximum entropy framework able make use global information directly achieves performance comparable best previous machine ners test data',\n",
       " 'lists except locations lists processed list tokens list processed list unigrams bigrams locations tokens matched unigrams sequences two consecutive tokens matched bigrams',\n",
       " 'features used divided classes local features features based neighboring tokens well token features extracted occurrences token whole document',\n",
       " 'case zone similarly initcaps feature set etc token information group consists features based string listed table',\n",
       " 'features used divided classes local features features based neighboring tokens well token features extracted occurrences token whole document',\n",
       " 'global feature groups initcaps occurrences features group checking whether first occurrence word unambiguous position document initcaps word whose initcaps might due position rather meaning case information occurrences might accurate',\n",
       " 'refer system menergi',\n",
       " 'global features extracted occurrences token whole document',\n",
       " 'paper show maximum entropy framework able make use global information directly achieves performance comparable best previous machine ners test data',\n",
       " 'focus extending applicability unsupervised methods lexical semantic classification verbs',\n",
       " 'development minimally supervised methods particular importance automatically classify verbs languages english substantial amounts labelled data available training classifiers',\n",
       " 'previously shown broad set noisy features performs well supervised verb classification',\n",
       " 'used hierarchical clustering command matlab implements agglomerative clustering unsupervised experiments',\n",
       " 'like others assumed lexical semantic classes verbs defined levin served gold standard computational linguistics research',\n",
       " 'experiments reported run final set verbs per class',\n",
       " 'like others assumed lexical semantic classes verbs defined levin served gold standard computational linguistics researchwe started list verbs given classes levin removing verb occur least times corpus',\n",
       " 'experiments however report results since found principled way automatically determining good cutoff',\n",
       " 'paper report results several feature selection approaches problem manual selection unsupervised selection supervised approach',\n",
       " 'although motivation verb class discovery perform experiments english accepted classification serve gold standard',\n",
       " 'table verb classes levin class numbers number experimental verbs',\n",
       " 'accuracy standard equivalent weighted mean precision clusters weighted according cluster size',\n",
       " 'started list verbs given classes levin removing verb occur least times corpus feature selection method highly successful outperforming full feature set tasks performing close seed set features outperforms manually selected set half tasks',\n",
       " 'accuracy standard equivalent weighted mean precision clusters weighted according cluster mean silhouette gives average individual goodness clusters measure overall goodness respect gold standard classes',\n",
       " 'classes mean multiway table experimental results',\n",
       " 'focus extending applicability unsupervised methods lexical semantic classification verbs',\n",
       " 'unsupervised scenario verb class discovery maintain benefit needing noisy features without generality feature space leading curse dimensionality',\n",
       " 'explored manual unsupervised supervised methods feature selection clustering approach verb class discovery',\n",
       " 'levin classes form hierarchy verb groupings shared meaning feature set essentially generalization scaling feature space useful across english verb classes general necessarily face dimensionality problem arise research',\n",
       " 'accuracy standard equivalent weighted mean precision clusters weighted according cluster size',\n",
       " 'previously shown broad set noisy features performs well supervised verb classification',\n",
       " 'addition plans put evaluations line public access starting ne evaluation intended make ne task familiar new sites give convenient way try hand following standardized test procedure',\n",
       " 'named entity coreference tasks entailed standard generalized markup language annotation texts conducted first middle spectrum definite descriptions pronouns whose choice referent constrained factors structural relations discourse focus',\n",
       " 'middle effort preparing test data formal evaluation interannotator variability test large number factors contributed disagreement including overlooking coreferential nps using different interpretations vague portions guidelines making different subjective decisions text article ambiguous sloppy etc human errors pertained definite descriptions bare nominals names pronouns',\n",
       " 'coreference insert sgml tags text link strings represent coreferring noun phrases',\n",
       " 'highest score person object recall precision close highest score ne subcategorization person recall precision',\n",
       " 'corpus testing conducted using wall street journal texts provided linguistic data articles used evaluation drawn corpus approximately articles spanning period january june training set test set consisted articles drawn corpus using text retrieval system called managing gigabytes whose retrieval engine based model producing ranked list hits according degree match keyword search texts test set relevant management succession scenario including six marginally relevant',\n",
       " 'article management succession scenario used basis management succession template consists four object types linked together via pointers form hierarchical structure',\n",
       " 'outputs scored quot quot mode though one annotator apos output represented quot key quot quot response quot humans achieved overall corresponding error per response fill score ne scores primary metrics top systems tested order decreasing key scores bbn baseline configuration bbn experimental configuration umanitoba umass mitre nmsu crl baseline configuration nyu usheffield sra baseline configuration sra quot fast quot configuration sra quot fastest quot configuration sra quot nonames quot configuration sri sterling software',\n",
       " 'common organization names first names people location names handled recourse list lookup although drawbacks names may one list lists complete may match name realized text etc',\n",
       " 'analysis done relative difficulty st task compared previous extraction evaluation limitation development preparation would difficult factor computation even without additional factor problem coming reasonable objective way measuring relative task difficulty adequately addressed',\n",
       " 'empirical study utilizes training test data senseval evaluation word sense disambiguation feature sets selected training data based top ranked bigrams according power divergence statistic dice accuracy approach good previously published results learned models complex diÃ¦cult interpret e ect acting accurate black boxes',\n",
       " 'approach paper relies upon feature set made bigrams two word sequences occur context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately words left right word developed bigram statistics package produce ranked lists bigrams using range tests',\n",
       " 'paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation',\n",
       " 'paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation',\n",
       " 'however suggest cases pearson apos statistic reliable likelihood ratio one test always preferred usually clear test appropriate particular sample data',\n",
       " 'approach paper relies upon feature set made bigrams two word sequences occur sparse skewed nature data statistical methods used select interesting bigrams must carefully number well known statistics belong family including likelihood ratio statisticg pearson apos sx statistic',\n",
       " 'compare two methods aligning headlines construct aligned corpus paraphrases one based clustering pairwise matching',\n",
       " 'news article headlines abundant web already grouped news aggregators google news',\n",
       " 'compare two methods aligning headlines construct aligned corpus paraphrases one based clustering pairwise matching',\n",
       " 'therefore paraphrase acquisition considered important technology producing resources generation',\n",
       " 'first approach use clustering algorithm cluster similar headlines',\n",
       " 'algorithm algorithm assigns k centers represent clustering n points vector space',\n",
       " 'original cluster clustering performed using k found cluster stopping function',\n",
       " 'clear clustering performs well unclustered headlines artificially ignored',\n",
       " 'mind adopt two thresholds cosine similarity function calculate similarity two sentences cos v v v v v v vectors two sentences compared',\n",
       " 'use f favour precision recall',\n",
       " 'developing text rewriting algorithm paraphrasing essential monolingual corpus aligned paraphrased sentences',\n",
       " 'use annotated clusters development test data developing method automatically obtain paraphrase pairs headline clusters',\n",
       " 'augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'present refinement levin classes tive sets sification coherent sets tactic frames associated semantic augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'present refinement levin classes tive sets sification coherent sets tactic frames associated semantic nents',\n",
       " 'however would useful wordnet senses access detailed syntactic information levin classes contain would equally useful guidance membership levin class fact indicate shared tic components',\n",
       " 'ambiguities levin augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'however would useful wordnet senses access detailed syntactic information levin classes contain would equally useful guidance membership levin class fact indicate shared tic components',\n",
       " 'large levin classes comprise verbs exhibit wide range possible semantic components could divided smaller subclasses',\n",
       " 'junction apart adverb adds change state semantic component respect object present carry verbs intersective class quot pure quot examples carry class always imply achievement causation regular sense extensions based intersective levin classes',\n",
       " 'fundamental assumption syntactic frames direct reflection derlying semantics',\n",
       " 'augmented existing database levin semantic classes set intersective classes created grouping gether subsets existing classes lapping members',\n",
       " 'first since ment based translation english portuguese',\n",
       " 'intersective levin sets partition classes according herent subsets features effect highlighting lattice semantic features determine sense verb',\n",
       " 'large levin classes comprise verbs exhibit wide range possible semantic components could divided smaller subclasses',\n",
       " 'present refinement levin classes tive sets sification coherent sets tactic frames associated semantic nents',\n",
       " 'fundamental assumption syntactic frames direct reflection derlying semantics',\n",
       " 'large levin classes comprise verbs exhibit wide range possible semantic components could divided smaller subclasses',\n",
       " 'current approaches english tion levin classes wordnet tions applicability impede utility general classification schemes',\n",
       " 'large levin classes comprise verbs exhibit wide range possible semantic components could divided smaller subclasses',\n",
       " 'two current approaches english verb fications wordnet levin classes',\n",
       " 'instead exploring full parse tree information directly previous related work incorporate base phrase chunking information performance improvement syntactic aspect incorporation parse tree dependence tree information slightly improves performance',\n",
       " 'moreover apply simple linear kernel although kernels peform reason choose svms purpose svms represent machine learning research community good implementations algorithm available',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines shows incorporation diverse features enables system achieve best reported performance',\n",
       " 'second well known full parsing always prone parsing errors although collins parser used system achieves full parsing still needs enhanced provide accurate enough information especially pp attachment',\n",
       " 'moreover apply simple linear kernel although kernels peform reason choose svms purpose svms represent machine learning research community good implementations algorithm available',\n",
       " 'kambhatla employed maximum entropy models relation extraction features derived word entity type mention level overlap dependency tree parse tree',\n",
       " 'culotta et al extended work estimate kernel functions augmented dependency trees achieved relation detection relation detection classification ace relation types',\n",
       " 'pair compute various lexical syntactic semantic paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'ace corpus suffers small amount annotated data subtypes subtype founder type role also shows ace rdc task defines difficult types subtypes located residence type difficult even human experts differentiate',\n",
       " 'kambhatla employed maximum entropy models relation extraction features derived word entity type mention level overlap dependency tree parse tree',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines shows incorporation diverse features enables system achieve best reported performance',\n",
       " 'tree approaches proposed zelenko et al culotta et al able explore implicit feature space without much feature paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'instead exploring full parse tree information directly previous related work incorporate base phrase chunking information performance improvement syntactic aspect incorporation parse tree dependence tree information slightly improves performance',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'kambhatla employed maximum entropy models relation extraction features derived word entity type mention level overlap dependency tree parse paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'instead exploring full parse tree information directly previous related work incorporate base phrase chunking information performance improvement syntactic aspect incorporation parse tree dependence tree information slightly improves performance',\n",
       " 'kambhatla employed maximum entropy models relation extraction features derived word entity type mention level overlap dependency tree parse paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper model explicit relations poor agreement annotation implicit relations limited number',\n",
       " 'kambhatla employed maximum entropy models relation extraction features derived word entity type mention level overlap dependency tree parse tree',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'done replacing pronominal mention recent antecedent determining word features include head word ace mention head annotation annotation',\n",
       " 'category features includes information words phrase labels words mentions dependent dependency tree derived syntactic full parse tree',\n",
       " 'evaluation shows incorporation diverse features enables system achieve best reported performance',\n",
       " 'moreover apply simple linear kernel although kernels peform better',\n",
       " 'paper use deleveloped joachims',\n",
       " 'ace corpus suffers small amount annotated data subtypes subtype founder type role',\n",
       " 'paper use deleveloped joachims corpus suffers small amount annotated data subtypes subtype founder type role',\n",
       " 'shows system achieves best performance combining diverse lexical syntactic semantic features',\n",
       " 'paper model explicit relations poor agreement annotation implicit relations limited semantic relation determined two addition distinguish argument order two mentions',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'study illustrates base phrase chunking information contributes performance inprovement syntactic aspect additional full parsing information contribute much largely due fact relations defined ace corpus within short largely due incorporation two semantic resources country name list personal relative trigger word list',\n",
       " 'study illustrates base phrase chunking information contributes performance inprovement syntactic aspect additional full parsing information contribute much largely due fact relations defined ace corpus within short distance',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines shows incorporation diverse features enables system achieve best reported performance',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic reason choose svms purpose svms represent machine learning research community good implementations algorithm available',\n",
       " 'paper uses ace corpus provided ldc train evaluate relation extraction system',\n",
       " 'addition distinguish argument order two mentions',\n",
       " 'paper measure performance relation extraction true mentions true chaining coreference ace corpus',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines',\n",
       " 'shows system achieves best performance combining diverse lexical syntactic semantic features',\n",
       " 'paper focuses ace rdc task employs diverse lexical syntactic semantic knowledge relation extraction using support vector machines way model relation extraction classification problem classes two relation subtype none class case two mentions related',\n",
       " 'paper use deleveloped joachims',\n",
       " 'suggests useful information full parse trees relation extraction shallow captured chunking',\n",
       " 'instead exploring full parse tree information directly previous related work incorporate base phrase chunking information performance improvement syntactic aspect incorporation parse tree dependence tree information slightly improves performance',\n",
       " 'relation extraction task formulated message understanding conference starting addressed within natural language processing machine learning communities',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'paper explore approach systematic study extensive incorporation diverse lexical syntactic semantic information',\n",
       " 'entities five types persons organizations locations facilities geopolitical entities',\n",
       " 'composed three parts word segmentation segmenting iv words based tagging crf recognizing oovs word segmentation used merging results iob table found iob tagging yielded better segmentation based approach',\n",
       " 'addition latter used balance rates rates',\n",
       " 'define confidence measure c tiob',\n",
       " 'comparing table found iob tagging yielded better segmentation based approach',\n",
       " 'confidence measure threshold defined making decision based value lower iob tag rejected segmentation used otherwise iob tagging segmentation used',\n",
       " 'confidence measure threshold defined making decision based value lower iob tag rejected segmentation used otherwise iob tagging segmentation table found iob tagging yielded better segmentation based rates getting worse return higher rates',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'composed three parts word segmentation segmenting iv words based tagging crf recognizing oovs word segmentation used merging results iob tagging',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'composed three parts word segmentation segmenting iv words based tagging crf recognizing oovs word segmentation used merging results iob tagging',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese also successfully employed confidence measure make word used data provided sighan bakeoff test approaches described previous sections',\n",
       " 'work proposed iob tagging method chinese word used data provided sighan bakeoff test approaches described previous sections',\n",
       " 'approach extracted word list training data gram lms generated using sri lm toolkit disambiguation',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'proposed two approaches improve chinese word segmentation tagging confidence measure found former achieved better performance existing tagging latter improved segmentation combining former segmentation',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'work propose iob tagging assigns tags predefined lexicon subset consisting frequent words addition single chinese characters',\n",
       " 'iob tagger one possibility multiple choices iob tagger',\n",
       " 'semantic representations produced boxer known discourse representation structures incorporate neodavidsonian representations events using verbnet inventory thematic numerical date expressions got correct representations',\n",
       " 'clearly defined choice logical form independent categorial framework underlying',\n",
       " 'clearly defined choice logical form independent categorial framework underlying',\n",
       " 'clearly defined choice logical form independent categorial framework underlying discourse representation theory boxer able construct discourse representation structures english sentences texts',\n",
       " 'based discourse representation theory boxer able construct discourse representation structures english sentences distributed c amp c tools natural language processing hosted site http',\n",
       " 'clearly defined choice logical form independent categorial framework underlying choose drt established documented formal theory meaning covering number semantic phenomena ranging pronouns abstract anaphora presupposition tense aspect propositional attitudes plurals',\n",
       " 'boxer tool computing reasoning semantic representations',\n",
       " 'takes input ccg derivation natural language expression produces formally interpretable semantic representations either form drss formulas logic',\n",
       " 'importance markov assumption discourse grammar view whole system discourse grammar local likelihoods hidden markov model conducted preliminary experiments assess neural networks compare decision trees type data studied combined utterance classification accuracies grammar accuracy prosody recognizer combined none unigram bigram table accuracy individual combined models two subtasks using uniform priors',\n",
       " 'speech recognition task framework provides mathematically principled way condition speech recognizer conversation context dialogue structure well nonlexical information correlated da present results obtained approach large widely available corpus spontaneous conversational speech',\n",
       " 'computation likelihoods p depends types evidence given acoustic features f capturing various aspects pitch duration energy speech signal associated likelihoods p',\n",
       " 'goal article twofold one hand aim present comprehensive framework modeling automatic classification das founded statistical speech recognition task framework provides mathematically principled way condition speech recognizer conversation context dialogue structure well nonlexical information correlated da identity',\n",
       " 'backchannel short utterance plays roles indicating speaker go usually referred conversation analysis literature quot continuers quot studied extensively',\n",
       " 'tag statement opinion appreciation nonverbal yes answers answers response acknowledgment hedge declarative quotation affirmative answers collaborative completion hold reject negative answers answers dispreferred answers offers options commits ownplayer declarative apology thanking example apos legal department',\n",
       " 'hardly consensus exactly discourse structure described agreement exists useful first level analysis involves identification dialogue acts das thought tag set classifies utterances according combination pragmatic semantic syntactic criteria',\n",
       " 'goal article twofold one hand aim present comprehensive framework modeling automatic classification das founded statistical framework generalizes earlier models giving us clean probabilistic approach performing da classification unreliable words nonlexical domain chose model switchboard corpus conversational telephone speech distributed linguistic data consortium',\n",
       " 'example model draws use da hidden markov models conversation present earlier work nagata morimoto woszczyna waibel importance markov assumption discourse grammar view whole system discourse grammar local likelihoods hidden markov model hmm states correspond das observations correspond utterances transition probabilities given discourse grammar observation probabilities given local likelihoods p',\n",
       " 'interactional dominance might measured accurately using da distributions simpler techniques could serve indicator type genre discourse cases da labels would enrich available input processing spoken words',\n",
       " 'described robust proach pronoun resolution operates texts shows success rate genre nical manuals least genre approach appears successful similar also adapted evaluated approach polish arabic',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents noun phrase highest aggregate score proposed antecedent rare event tie priority given candidate higher score mediate reference',\n",
       " 'makes use tagger plus simple noun phrase rules operates basis preferences antecedent indicators fied empirically related salience structural matches referential distance preference terms',\n",
       " 'immediate reference identified priority given candi date best collocation pattern collocation preference restricted patterns quot noun phrase verb quot quot verb noun phrase quot',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'referential distance complex sentences noun phrases previous best candidate antecedent anaphor subsequent clause followed noun phrases previous sentence nouns situated sentences back finally nouns sentences back',\n",
       " 'antecedent indicators play decisive role tracking antecedent set possible assigned score indicator candidate highest aggregate score proposed cedent',\n",
       " 'candidates assigned score indicator candidate highest aggregate score proposed antecedent indicators fied empirically related salience structural matches referential distance preference terms',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'approach works follows takes input output text processed tagger identifies noun phrases precede anaphor within distance sentences checks gender number agreement anaphor applies antecedent indicators maining candidates',\n",
       " 'makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'evaluation first evaluation exercise based random sample text technical manual english evaluation carried manual ensure added error erated reason hand ensure fair comparison breck baldwin apos method available us evaluation indicated success rate',\n",
       " 'makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'pronouns page technical manual pronouns exophoric',\n",
       " 'part anaphora resolution focused traditional linguistic methods',\n",
       " 'described robust proach pronoun resolution operates texts shows success rate genre nical manuals least genre approach appears successful similar methods',\n",
       " 'traditional approaches anaphora resolution rely heavily linguistic domain disadvantages developing based system however intensive described robust proach pronoun resolution operates texts shows success rate genre nical manuals least genre approach appears successful similar methods',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents shows success rate genre nical manuals least genre approach appears successful similar also adapted evaluated approach polish arabic',\n",
       " 'makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'paper ents robust approach resolving pronouns technical manuals operates texts reports success rate better cess rates approaches selected comparison tested data',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'paper ents robust approach resolving pronouns technical manuals operates texts assigned scores indicator candidate highest score returned antecedent',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'paper ents robust approach resolving pronouns technical manuals operates texts assigned scores indicator candidate highest score returned antecedent',\n",
       " 'antecedent indicators fied empirically related salience structural matches referential distance preference terms',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents anaphors',\n",
       " 'also example anaphors specific genre resolved quite successfully without sophisticated linguistic knowledge even without parsing',\n",
       " 'antecedent indicators fied empirically related salience structural matches referential distance preference terms',\n",
       " 'makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'antecedent indicators fied empirically related salience structural matches referential distance preference symptoms like quot lexical reiteration quot sign score quot quot whereas quot quot noun phrases given negative score quot quot point antecedent indicators preferences absolute factors',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'described robust proach pronoun resolution operates texts shows success rate genre nical manuals least genre approach appears successful similar methods',\n",
       " 'described robust proach pronoun resolution operates texts tagger',\n",
       " 'scores determined experimentally empirical basis constantly symptoms like quot lexical reiteration quot sign score quot quot whereas quot quot noun phrases given negative score quot quot point antecedent indicators preferences absolute factors',\n",
       " 'practical reasons approach presented incorporate syntactic semantic information istic expect performance good approach makes use syntactic semantic knowledge terms constraints would fair say even though results show superiority approach training data used generalised automatically genres unrestricted texts accurate picture extensive tests necessary',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'view avoiding complex syntactic tic discourse analysis developed robust approach pronoun resolution parse analyse input order identify antecedents makes use tagger plus simple noun phrase rules operates basis preferences',\n",
       " 'paper ents robust approach resolving pronouns technical manuals operates texts assigned scores indicator candidate highest score returned antecedent',\n",
       " 'antecedent indicators fied empirically related salience structural matches referential distance preference terms',\n",
       " 'order evaluate effectiveness proach explore far superior baseline models anaphora resolution also tested sample text baseline model checks agreement number gender one candidate remains picks dent recent subject matching gender number anaphor baseline model picks antecedent recent noun phrase matches gender number success rate quot baseline subject quot whereas success rate quot baseline recent np quot',\n",
       " 'work continuation latest trends search inexpensive fast reliable procedures ora also example anaphors specific genre resolved quite successfully without sophisticated linguistic knowledge even without parsing',\n",
       " 'proposed unsupervised method discover paraphrases large untagged corpus',\n",
       " 'example information retrieval match query expressions desired documents question answering find answer question even formulation answer document different obstacles completing idea believe automatic paraphrase discovery important component building fully automatic information extraction paper propose unsupervised method discover paraphrases large untagged corpus',\n",
       " 'paper propose unsupervised method discover paraphrases large untagged corpus',\n",
       " 'paper propose unsupervised method discover paraphrases large untagged ne tagger system ne categories sekine et al',\n",
       " 'ne tagger system ne categories sekine et al ne categories designed extending muc ne categories finer adding new types ne categories',\n",
       " 'set represented keyword number parentheses indicates number shared ne pair approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution lin pantel',\n",
       " 'pair also record context phrase two nes pair ne categories collect contexts find keywords topical ne category pair',\n",
       " 'extract ne instance pairs contexts first extract ne pair instances context corpus',\n",
       " 'extract ne instance pairs contexts first extract ne pair instances context corpus',\n",
       " 'ie researchers creating paraphrase knowledge hand specific order create ie system new domain one spend long time create knowledge',\n",
       " 'paper propose unsupervised method discover paraphrases large untagged focusing phrases two named entities types phrases important ie applications',\n",
       " 'rather believe several methods developed using different heuristics discover wider variety paraphrases',\n",
       " 'kinds efforts discover paraphrase automatically corpora',\n",
       " 'ie researchers creating paraphrase knowledge hand specific tasks',\n",
       " 'cluster phrases based links set phrases share step try link sets put single cluster',\n",
       " 'hereafter pair ne categories called domain company company domain call domain domain phrases contain keyword gathered build set phrases',\n",
       " 'paper propose unsupervised method discover paraphrases large untagged focusing phrases two named entities types phrases important ie applications',\n",
       " 'translation problem c viewed query c viewed use backoff linear interpolation probability estimation',\n",
       " 'investigate effect two individual sources information checked many translations could found using one source information chinese words translations english part comparable corpus',\n",
       " 'comparable corpora refer texts direct translation example various news agencies report major world events different languages news documents form readily available source comparable corpora',\n",
       " 'employ language modeling approach corresponding document translation c c retrieval problem',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'used list chineseenglish name pairs training data em algorithm',\n",
       " 'use context c likely retrieve context e use context c query c retrieve document c query try retrieve similar best matches query',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'finally english candidate word smallest average rank position appears within top positions ranked lists chosen english translation',\n",
       " 'paper propose new approach task mining new word translations comparable corpora combining context transliteration information',\n",
       " 'order machine translation system translate new words correctly bilingual lexicon needs constantly updated new word translations',\n",
       " 'investigate effect two individual sources information checked many translations could found using one source information chinese words translations english part comparable corpus']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc607fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_two_labels(data, labels):\n",
    "    '''\n",
    "    split two labels in 2 data-points\n",
    "    '''\n",
    "    labels_new = []\n",
    "    data_new = []\n",
    "    for i,lb in enumerate(labels):\n",
    "        if type(lb) == list: #needs to be split\n",
    "            for l in lb:\n",
    "                l = l.lower()\n",
    "                if '_' in l:\n",
    "                    l = l.split('_')[0]\n",
    "                else:\n",
    "                    l = l.split()[0]\n",
    "                data_new.append(data[i])\n",
    "        else:\n",
    "            l = lb.lower()\n",
    "            if '_' in l:\n",
    "                    l = l.split('_')[0]\n",
    "            else:\n",
    "                l = l.split()[0]\n",
    "            if l=='results':\n",
    "                l = 'result'\n",
    "            labels_new.append(l)\n",
    "            data_new.append(data[i])\n",
    "    return data_new, labels_new\n",
    "\n",
    "train_data, train_labels = split_two_labels(train_data, train_labels)\n",
    "test_data, test_labels  = split_two_labels(test_data, test_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6afdafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'result',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'result',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'aim',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'hypothesis',\n",
       " 'hypothesis',\n",
       " 'implication',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'implication',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'hypothesis',\n",
       " 'hypothesis',\n",
       " 'implication',\n",
       " 'hypothesis',\n",
       " 'method',\n",
       " 'result',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'result',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'hypothesis',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'result',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'aim',\n",
       " 'aim',\n",
       " 'method',\n",
       " 'result',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'implication',\n",
       " 'implication',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'method',\n",
       " 'implication',\n",
       " 'method']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b75fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "# val_labels = le.transform(val_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261bae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aim', 1: 'hypothesis', 2: 'implication', 3: 'method', 4: 'result'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_metrics(pred_lis, gt_lis):\n",
    "    acc = accuracy_score(pred_lis, gt_lis)\n",
    "    recall = recall_score(pred_lis, gt_lis,average='micro')\n",
    "    prec = precision_score(pred_lis, gt_lis,average='micro')\n",
    "    f1 = f1_score(pred_lis, gt_lis,average='micro')\n",
    "    print(\"metrics obtained in test: accuracy {} recall {}, precision {}, f1-score {}\".format(acc,recall,prec,f1))\n",
    "    \n",
    "def get_integer_mapping(le):\n",
    "    '''\n",
    "    Return a dict mapping labels to their integer values\n",
    "    from an SKlearn LabelEncoder\n",
    "    le = a fitted SKlearn LabelEncoder\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({le.transform([cl])[0]:cl})\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "mapping = get_integer_mapping(le)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd858f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data,val_labels = get_dataset(val_docs,folder='From-ScisummNet-2019')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "# val_encodings = tokenizer(val_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "test_encodings = tokenizer(test_data,return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels))\n",
    "# val_dataset = IMDbDataset(val_encodings, torch.from_numpy(val_labels))\n",
    "test_dataset = IMDbDataset(test_encodings, torch.from_numpy(test_labels))\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb91172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.75 recall 0.75, precision 0.75, f1-score 0.75\n",
      "for class:  aim\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n",
      "for class:  hypothesis\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n",
      "for class:  implication\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n",
      "for class:  method\n",
      "metrics obtained in test: accuracy 1.0 recall 1.0, precision 1.0, f1-score 1.0\n",
      "for class:  result\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "multi_lb_classi = True\n",
    "if multi_lb_classi:\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/results',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/logs',            # directory for storing logs\n",
    "        logging_steps=200000,\n",
    "        save_steps = 30000\n",
    "    )\n",
    "\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(mapping))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "    \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset          # training dataset\n",
    "        # eval_dataset=val_dataset             # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "    # trainer.evaluate()\n",
    "    \n",
    "    torch.save(model,\"task1b_bert.pth\")\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    pred_lis = []\n",
    "    gt_lis = []\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        pred_lis.extend(predictions.cpu().numpy())\n",
    "        gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "\n",
    "    calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "    gt_lis = np.array(gt_lis)\n",
    "    pred_lis = np.array(pred_lis)\n",
    "\n",
    "\n",
    "    #class-wise metrics\n",
    "    for cl in mapping:\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        ind = np.where(gt_lis==cl)\n",
    "        small_pred = pred_lis[ind]\n",
    "        calc_metrics(small_pred, gt_lis[ind])\n",
    "\n",
    "# ## Classifiers for each class\n",
    "else:\n",
    "    # train_data = np.array(train_data)\n",
    "    # test_data = np.array(test_data)\n",
    "\n",
    "    # train_encodings = tokenizer(list(train_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    # test_encodings = tokenizer(list(test_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    print(\"starting\")\n",
    "    for cl in mapping:\n",
    "#     cl = 2\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/avani.gupta/results'+str(cl),          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,              # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/avani.gupta/logs'+str(cl),            # directory for storing logs\n",
    "        logging_steps=20000,\n",
    "        save_steps = 30000\n",
    "        )\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        train_ind = np.where(train_labels==cl)[0].astype(int)\n",
    "        new_train_labels = np.zeros(train_labels.shape).astype(int)\n",
    "        new_train_labels[train_ind] = 1\n",
    "\n",
    "        # val_ind = np.where(val_labels==cl)[0].astype(int)\n",
    "        test_ind = np.where(test_labels==cl)[0].astype(int)\n",
    "        new_test_labels = np.zeros(test_labels.shape).astype(int)\n",
    "        new_test_labels[test_ind] = 1\n",
    "\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, torch.from_numpy(new_train_labels))\n",
    "        test_dataset = IMDbDataset(test_encodings, torch.from_numpy(new_test_labels))    \n",
    "\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset         # training dataset\n",
    "        )\n",
    "        trainer.train()\n",
    "        # eval\n",
    "        model.eval()\n",
    "        torch.save(model, \"bert_single_lb_classi\"+str(cl)+\".pth\")\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        pred_lis = []\n",
    "        gt_lis = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            pred_lis.extend(predictions.cpu().numpy())\n",
    "            gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9518b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "for class:  aim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9294871794871795 recall 0.9294871794871795, precision 0.9294871794871795, f1-score 0.9294871794871795\n",
      "for class:  hypothesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9935897435897436 recall 0.9935897435897436, precision 0.9935897435897436, f1-score 0.9935897435897436\n",
      "for class:  implication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9294871794871795 recall 0.9294871794871795, precision 0.9294871794871795, f1-score 0.9294871794871795\n",
      "for class:  method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.75 recall 0.75, precision 0.75, f1-score 0.75\n",
      "for class:  result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1623\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1623' max='1623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1623/1623 03:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.8974358974358975 recall 0.8974358974358975, precision 0.8974358974358975, f1-score 0.8974358974358975\n"
     ]
    }
   ],
   "source": [
    "multi_lb_classi = False\n",
    "if multi_lb_classi:\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/results',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/logs',            # directory for storing logs\n",
    "        logging_steps=200000,\n",
    "        save_steps = 30000\n",
    "    )\n",
    "\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(mapping))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "    \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset          # training dataset\n",
    "        # eval_dataset=val_dataset             # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "    # trainer.evaluate()\n",
    "    \n",
    "    torch.save(model,\"task1b_bert.pth\")\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    pred_lis = []\n",
    "    gt_lis = []\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        pred_lis.extend(predictions.cpu().numpy())\n",
    "        gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "\n",
    "    calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "    gt_lis = np.array(gt_lis)\n",
    "    pred_lis = np.array(pred_lis)\n",
    "\n",
    "\n",
    "    #class-wise metrics\n",
    "    for cl in mapping:\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        ind = np.where(gt_lis==cl)\n",
    "        small_pred = pred_lis[ind]\n",
    "        calc_metrics(small_pred, gt_lis[ind])\n",
    "\n",
    "# ## Classifiers for each class\n",
    "else:\n",
    "    # train_data = np.array(train_data)\n",
    "    # test_data = np.array(test_data)\n",
    "\n",
    "    # train_encodings = tokenizer(list(train_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    # test_encodings = tokenizer(list(test_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    print(\"starting\")\n",
    "    for cl in mapping:\n",
    "#     cl = 2\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/avani.gupta/results'+str(cl),          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,              # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/avani.gupta/logs'+str(cl),            # directory for storing logs\n",
    "        logging_steps=20000,\n",
    "        save_steps = 30000\n",
    "        )\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        train_ind = np.where(train_labels==cl)[0].astype(int)\n",
    "        new_train_labels = np.zeros(train_labels.shape).astype(int)\n",
    "        new_train_labels[train_ind] = 1\n",
    "\n",
    "        # val_ind = np.where(val_labels==cl)[0].astype(int)\n",
    "        test_ind = np.where(test_labels==cl)[0].astype(int)\n",
    "        new_test_labels = np.zeros(test_labels.shape).astype(int)\n",
    "        new_test_labels[test_ind] = 1\n",
    "\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, torch.from_numpy(new_train_labels))\n",
    "        test_dataset = IMDbDataset(test_encodings, torch.from_numpy(new_test_labels))    \n",
    "\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset         # training dataset\n",
    "        )\n",
    "        trainer.train()\n",
    "        # eval\n",
    "        model.eval()\n",
    "        torch.save(model, \"bert_single_lb_classi\"+str(cl)+\".pth\")\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        pred_lis = []\n",
    "        gt_lis = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            pred_lis.extend(predictions.cpu().numpy())\n",
    "            gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538b407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "for class:  aim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1360\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 05:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9294871794871795 recall 0.9294871794871795, precision 0.9294871794871795, f1-score 0.9294871794871795\n",
      "for class:  hypothesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1360\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 05:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9935897435897436 recall 0.9935897435897436, precision 0.9935897435897436, f1-score 0.9935897435897436\n",
      "for class:  implication\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1360\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 05:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.9166666666666666 recall 0.9166666666666666, precision 0.9166666666666666, f1-score 0.9166666666666666\n",
      "for class:  method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1360\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 05:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.75 recall 0.75, precision 0.75, f1-score 0.75\n",
      "for class:  result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1360\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 05:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics obtained in test: accuracy 0.8717948717948718 recall 0.8717948717948718, precision 0.8717948717948718, f1-score 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "multi_lb_classi = False\n",
    "if multi_lb_classi:\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/results',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=1,  # batch size per device during training\n",
    "        per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/logs',            # directory for storing logs\n",
    "        logging_steps=200000,\n",
    "        save_steps = 30000\n",
    "    )\n",
    "\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(mapping))\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "    \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset          # training dataset\n",
    "        # eval_dataset=val_dataset             # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "    # trainer.evaluate()\n",
    "    \n",
    "    torch.save(model,\"task1b_bert.pth\")\n",
    "    # val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    pred_lis = []\n",
    "    gt_lis = []\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        pred_lis.extend(predictions.cpu().numpy())\n",
    "        gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "        \n",
    "\n",
    "    calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "    gt_lis = np.array(gt_lis)\n",
    "    pred_lis = np.array(pred_lis)\n",
    "\n",
    "\n",
    "    #class-wise metrics\n",
    "    for cl in mapping:\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        ind = np.where(gt_lis==cl)\n",
    "        small_pred = pred_lis[ind]\n",
    "        calc_metrics(small_pred, gt_lis[ind])\n",
    "\n",
    "# ## Classifiers for each class\n",
    "else:\n",
    "    # train_data = np.array(train_data)\n",
    "    # test_data = np.array(test_data)\n",
    "\n",
    "    # train_encodings = tokenizer(list(train_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    # test_encodings = tokenizer(list(test_data),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    print(\"starting\")\n",
    "    for cl in mapping:\n",
    "#     cl = 2\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir='/ssd_scratch/cvit/avani.gupta/results'+str(cl),          # output directory\n",
    "        num_train_epochs=10,              # total number of training epochs\n",
    "        per_device_train_batch_size=4,  # batch size per device during training\n",
    "        per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "        warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,              # strength of weight decay\n",
    "        logging_dir='/ssd_scratch/cvit/avani.gupta/logs'+str(cl),            # directory for storing logs\n",
    "        logging_steps=20000,\n",
    "        save_steps = 30000\n",
    "        )\n",
    "        print(\"for class: \",mapping[cl])\n",
    "        train_ind = np.where(train_labels==cl)[0].astype(int)\n",
    "        new_train_labels = np.zeros(train_labels.shape).astype(int)\n",
    "        new_train_labels[train_ind] = 1\n",
    "\n",
    "        # val_ind = np.where(val_labels==cl)[0].astype(int)\n",
    "        test_ind = np.where(test_labels==cl)[0].astype(int)\n",
    "        new_test_labels = np.zeros(test_labels.shape).astype(int)\n",
    "        new_test_labels[test_ind] = 1\n",
    "\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, torch.from_numpy(new_train_labels))\n",
    "        test_dataset = IMDbDataset(test_encodings, torch.from_numpy(new_test_labels))    \n",
    "\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        #model = nn.DataParallel(model,device_ids=[0,1,2,3])\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset         # training dataset\n",
    "        )\n",
    "        trainer.train()\n",
    "        # eval\n",
    "        model.eval()\n",
    "        torch.save(model, \"bert_single_lb_classi\"+str(cl)+\".pth\")\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        pred_lis = []\n",
    "        gt_lis = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            pred_lis.extend(predictions.cpu().numpy())\n",
    "            gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        calc_metrics(pred_lis, gt_lis)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3640df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aim', 1: 'hypothesis', 2: 'implication', 3: 'method', 4: 'result'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
