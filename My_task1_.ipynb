{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ebf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1971d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e713843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "citants = pd.read_json('From-ScisummNet-2019/A00-1031/citing_sentences.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "citants = citants[['citance_No','clean_text']]\n",
    "citants\n",
    "queries = list(citants['clean_text'])\n",
    "# cit_no = list(citants['citance_No'])\n",
    "# query_sent = citants['raw_text']\n",
    "# query_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262db9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_no = list(citants.citance_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af5c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "tree = ET.parse(\"From-ScisummNet-2019/A00-1031\"+\"/Reference_XML/\"+\"A00-1031\"+\".xml\")\n",
    "root = tree.getroot()\n",
    "final1=[]\n",
    "final2=[]\n",
    "i = 0\n",
    "total = len(root)\n",
    "# print(total)\n",
    "for a in root:\n",
    "  #  print(a.attrib)\n",
    "   # print(i)\n",
    "    for b in a:\n",
    "\n",
    "        final1.append(b.text)\n",
    "        if i == 0:\n",
    "            final2.append(\"Abstract\")\n",
    "        if i == 1:\n",
    "            final2.append(\"Introduction\")\n",
    "        elif i < total-2:\n",
    "            final2.append(\"Experiment/Discussion\")\n",
    "        if i == total-2 or i == total-1:\n",
    "            final2.append(\"Results/Conclusion\")\n",
    "        if i == total:\n",
    "            final2.append(\"Acknowledgment\")\n",
    "    i = i+1\n",
    "\n",
    "\n",
    "d={'col1':final1,'col2':final2}\n",
    "rp = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a853b495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   col1                col2\n",
       "0    Trigrams'n'Tags (TnT) is an efficient statisti...        Introduction\n",
       "1    Contrary to claims found elsewhere in the lite...        Introduction\n",
       "2    A recent comparison has even shown that TnT pe...        Introduction\n",
       "3    We describe the basic model of TnT, the techni...        Introduction\n",
       "4    Furthermore, we present evaluations on two cor...        Introduction\n",
       "..                                                 ...                 ...\n",
       "173  Many thanks go to Hans Uszkoreit for his suppo...  Results/Conclusion\n",
       "174  Most of the work on TnT was carried out while ...  Results/Conclusion\n",
       "175  Large annotated corpora are the pre-requisite ...  Results/Conclusion\n",
       "176  Therefore, I would like to thank all the peopl...  Results/Conclusion\n",
       "177  And, last but not least, I would like to thank...  Results/Conclusion\n",
       "\n",
       "[178 rows x 2 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5daffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Trigrams'n'Tags (TnT) is an efficient statisti...\n",
       "1      Contrary to claims found elsewhere in the lite...\n",
       "2      A recent comparison has even shown that TnT pe...\n",
       "3      We describe the basic model of TnT, the techni...\n",
       "4      Furthermore, we present evaluations on two cor...\n",
       "                             ...                        \n",
       "173    Many thanks go to Hans Uszkoreit for his suppo...\n",
       "174    Most of the work on TnT was carried out while ...\n",
       "175    Large annotated corpora are the pre-requisite ...\n",
       "176    Therefore, I would like to thank all the peopl...\n",
       "177    And, last but not least, I would like to thank...\n",
       "Name: col1, Length: 178, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0fce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = \"From-ScisummNet-2019/A00-1031/annotation/A00-1031.ann.txt\"\n",
    "with open(ann,\"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f135ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = {'Citance Number':0,'Reference Article':1,'Citing Article':2,'Citation Marker Offset':3,'Citation Marker':4,'Citation Offset':5,'Reference Offset':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73438a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "lines = data.split('\\n')\n",
    "for line in lines:\n",
    "    lis = line.split('|')\n",
    "#     print(lis, len(lis))\n",
    "    if len(lis)==11:\n",
    "            cit_no = lis[0].split(':')[1]\n",
    "            ref_off = lis[7].split(':')[1].strip()\n",
    "            exec(\"ref_off=\"+ref_off)\n",
    "            ref_off = list(map(int, ref_off))\n",
    "            gt[int(cit_no)] = ref_off #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393e4df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0, 36],\n",
       " 2: [44, 178],\n",
       " 3: [15, 173],\n",
       " 4: [0, 1],\n",
       " 5: [27, 165],\n",
       " 6: [27, 141],\n",
       " 7: [115, 154],\n",
       " 8: [24, 165],\n",
       " 9: [36, 46],\n",
       " 10: [2, 75],\n",
       " 11: [0, 1],\n",
       " 12: [18, 40],\n",
       " 13: [36, 118],\n",
       " 14: [24, 173],\n",
       " 15: [79, 80],\n",
       " 16: [0, 113],\n",
       " 17: [12, 19],\n",
       " 18: [0, 1],\n",
       " 19: [28, 157],\n",
       " 20: [36, 164]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba3c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(line):\n",
    "#     line = line.split(\"|\")\n",
    "#     cit_2 , ref_2 = line[6].split(\"Citation Text:\")[1].strip() , line[8].split(\"Reference Text:\")[1].strip()\n",
    "#     cit_2 = cit_2.split(\" >\")[1].split(\"</S>\")[0]\n",
    "#     ref_2 = ref_2.split(\" >\")[1:]\n",
    "#     ref_2 = [ i.split(\"</S>\")[0] for i in ref_2]\n",
    "#     ans=\"\"\n",
    "#     for i in ref_2:\n",
    "#         ans=ans + cit_2 + \" $$$$$ \" + i +\"\\n\"\n",
    "#    # ref_2 = \" \".join(ref_2)\n",
    "    \n",
    "#     return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a452d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./From-ScisummNet-2019/A00-1031/annotation/A00-1031.ann.txt') as f:\n",
    "#     data = f.read()\n",
    "# data = [ clean(i) for i in data.split(\"\\n\") if i ]\n",
    "\n",
    "# # with open(\"cleaned.txt\",\"w+\") as f:\n",
    "# #     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ddd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rp.col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9fa591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3211e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d99f76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(example_sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w.isalpha()]\n",
    "    print(filtered_sentence)\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aa4d2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Trigrams'n'Tags (TnT) is an efficient statisti...\n",
       "1      Contrary to claims found elsewhere in the lite...\n",
       "2      A recent comparison has even shown that TnT pe...\n",
       "3      We describe the basic model of TnT, the techni...\n",
       "4      Furthermore, we present evaluations on two cor...\n",
       "                             ...                        \n",
       "173    Many thanks go to Hans Uszkoreit for his suppo...\n",
       "174    Most of the work on TnT was carried out while ...\n",
       "175    Large annotated corpora are the pre-requisite ...\n",
       "176    Therefore, I would like to thank all the peopl...\n",
       "177    And, last but not least, I would like to thank...\n",
       "Name: col1, Length: 178, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e45a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tnt', 'efficient', 'statistical', 'tagger']\n",
      "['contrary', 'claims', 'found', 'elsewhere', 'literature', 'argue', 'tagger', 'based', 'markov', 'models', 'performs', 'least', 'well', 'current', 'approaches', 'including', 'maximum', 'entropy', 'framework']\n",
      "['recent', 'comparison', 'even', 'shown', 'tnt', 'performs', 'significantly', 'better', 'tested', 'corpora']\n",
      "['describe', 'basic', 'model', 'tnt', 'techniques', 'used', 'smoothing', 'handling', 'unknown', 'words']\n",
      "['furthermore', 'present', 'evaluations', 'two', 'corpora']\n",
      "['large', 'number', 'current', 'language', 'processing', 'systems', 'use', 'tagger']\n",
      "['tagger', 'assigns', 'unique', 'ambiguous', 'tag', 'token', 'input', 'passes', 'output', 'next', 'processing', 'level', 'usually', 'parser']\n",
      "['furthermore', 'large', 'interest', 'tagging', 'corpus', 'annotation', 'projects', 'create', 'valuable', 'linguistic', 'resources', 'combination', 'automatic', 'processing', 'human', 'correction']\n",
      "['applications', 'tagger', 'highest', 'possible', 'accuracy', 'required']\n",
      "['debate', 'paradigm', 'solves', 'tagging', 'problem', 'best', 'finished']\n",
      "['recent', 'comparisons', 'approaches', 'trained', 'corpora', 'van', 'halteren', 'et', 'volk', 'schneider', 'shown', 'cases', 'statistical', 'aproaches', 'cutting', 'et', 'schmid', 'ratnaparkhi', 'yield', 'better', 'results', 'taggers', 'brill', 'daelemans', 'et']\n",
      "['surpassed', 'combinations', 'different', 'systems', 'forming', 'quot', 'voting', 'tagger', 'quot']\n",
      "['among', 'statistical', 'approaches', 'maximum', 'entropy', 'framework', 'strong', 'position']\n",
      "['nevertheless', 'recent', 'independent', 'comparison', 'taggers', 'zavrel', 'daelemans', 'shown', 'another', 'approach', 'even', 'works', 'better', 'markov', 'models', 'combined', 'good', 'smoothing', 'technique', 'handling', 'unknown', 'words']\n",
      "['tagger', 'tnt', 'yielded', 'highest', 'accuracy', 'also', 'fastest', 'training', 'tagging']\n",
      "['tagger', 'comparison', 'organized', 'quot', 'blackbox', 'test', 'quot', 'set', 'task', 'every', 'tagger', 'compare', 'outcomes']\n",
      "['paper', 'describes', 'models', 'techniques', 'used', 'tnt', 'together', 'implementation']\n",
      "['reader', 'surprised', 'simple', 'underlying', 'model']\n",
      "['result', 'tagger', 'comparison', 'seems', 'support', 'maxime', 'quot', 'simplest', 'best', 'quot']\n",
      "['however', 'paper', 'clarify', 'number', 'details', 'omitted', 'major', 'previous', 'publications', 'concerning', 'tagging', 'markov', 'models']\n",
      "['two', 'examples', 'rabiner', 'charniak', 'et', 'give', 'good', 'overviews', 'techniques', 'equations', 'used', 'markov', 'models', 'tagging', 'explicit', 'details', 'needed', 'application']\n",
      "['argue', 'choice', 'general', 'model', 'determines', 'result', 'tagger', 'also', 'various', 'quot', 'small', 'quot', 'decisions', 'alternatives']\n",
      "['aim', 'paper', 'give', 'detailed', 'account', 'techniques', 'used', 'tnt']\n",
      "['additionally', 'present', 'results', 'tagger', 'negra', 'corpus', 'brants', 'et', 'penn', 'treebank', 'marcus', 'et']\n",
      "['penn', 'treebank', 'results', 'reported', 'markov', 'model', 'approach', 'least', 'equivalent', 'reported', 'maximum', 'entropy', 'approach', 'ratnaparkhi']\n",
      "['comparison', 'taggers', 'reader', 'referred', 'zavrel', 'daelemans']\n",
      "['tnt', 'uses', 'second', 'order', 'markov', 'models', 'tagging']\n",
      "['states', 'model', 'represent', 'tags', 'outputs', 'represent', 'words']\n",
      "['transition', 'probabilities', 'depend', 'states', 'thus', 'pairs', 'tags']\n",
      "['output', 'probabilities', 'depend', 'recent', 'category']\n",
      "['explicit', 'calculate', 'given', 'sequence', 'words', 'length', 'tr', 'elements', 'tagset', 'additional', 'tags', 'markers']\n",
      "['using', 'additional', 'tags', 'even', 'stem', 'rudimentary', 'processing', 'punctuation', 'marks', 'slightly', 'improves', 'tagging', 'results']\n",
      "['different', 'formulas', 'presented', 'publications', 'stop', 'quot', 'loose', 'end', 'quot', 'last', 'word']\n",
      "['sentence', 'boundaries', 'marked', 'input', 'tnt', 'adds', 'tags', 'encounters', 'one']\n",
      "['token']\n",
      "['transition', 'output', 'probabilities', 'estimated', 'tagged', 'corpus']\n",
      "['first', 'step', 'use', 'maximum', 'likelihood', 'probabilities', 'p', 'derived', 'relative', 'frequencies', 'tagset', 'lexicon']\n",
      "['n', 'total', 'number', 'tokens', 'training', 'corpus']\n",
      "['define', 'maximum', 'likelihood', 'probability', 'zero', 'corresponding', 'nominators', 'denominators', 'zero']\n",
      "['second', 'step', 'contextual', 'frequencies', 'smoothed', 'lexical', 'frequences', 'completed', 'handling', 'words', 'lexicon', 'see']\n",
      "['trigram', 'probabilities', 'generated', 'corpus', 'usually', 'directly', 'used', 'sparsedata', 'problem']\n",
      "['means', 'enough', 'instances', 'trigram', 'reliably', 'estimate', 'probability']\n",
      "['furthermore', 'setting', 'probability', 'zero', 'corresponding', 'trigram', 'never', 'occured', 'corpus', 'undesired', 'effect']\n",
      "['causes', 'probability', 'complete', 'sequence', 'set', 'zero', 'use', 'necessary', 'new', 'text', 'sequence', 'thus', 'makes', 'impossible', 'rank', 'different', 'sequences', 'containing', 'zero', 'probability']\n",
      "['smoothing', 'paradigm', 'delivers', 'best', 'results', 'tnt', 'linear', 'interpolation', 'unigrams', 'bigrams', 'trigrams']\n",
      "['therefore', 'estimate', 'trigram', 'probability', 'follows', 'p', 'maximum', 'likelihood', 'estimates', 'probabilities', 'p', 'represent', 'probability', 'distributions']\n",
      "['use', 'variant', 'linear', 'interpolation', 'values', 'depend', 'particular', 'trigram']\n",
      "['contrary', 'intuition', 'yields', 'better', 'results', 'variant']\n",
      "['due', 'problems', 'one', 'estimate', 'different', 'set', 'trigram']\n",
      "['therefore', 'common', 'practice', 'group', 'trigrams', 'frequency', 'estimate', 'tied', 'sets']\n",
      "['however', 'aware', 'publication', 'investigated', 'frequency', 'groupings', 'linear', 'interpolation', 'tagging']\n",
      "['groupings', 'tested', 'yielded', 'equivalent', 'results', 'contextindependent', 'linear', 'interpolation']\n",
      "['groupings', 'even', 'yielded', 'worse', 'results']\n",
      "['tested', 'groupings', 'included', 'one', 'set', 'frequency', 'value', 'b', 'two', 'classes', 'low', 'high', 'frequency', 'two', 'ends', 'scale', 'well', 'several', 'groupings', 'several', 'settings', 'partitioning', 'classes']\n",
      "['values', 'estimated', 'deleted', 'interpolation']\n",
      "['technique', 'successively', 'removes', 'trigram', 'training', 'corpus', 'estimates', 'best', 'values', 'ngrams', 'corpus']\n",
      "['given', 'frequency', 'counts', 'trigrams', 'weights', 'efficiently', 'determined', 'processing', 'time', 'linear', 'number', 'different', 'trigrams']\n",
      "['algorithm', 'given', 'figure']\n",
      "['note', 'subtracting', 'means', 'taking', 'unseen', 'data', 'account']\n",
      "['without', 'subtraction', 'model', 'would', 'overfit', 'training', 'data', 'would', 'generally', 'yield', 'worse', 'results']\n",
      "['currently', 'method', 'handling', 'unknown', 'words', 'seems', 'work', 'best', 'inflected', 'languages', 'suffix', 'analysis', 'proposed', 'samuelsson']\n",
      "['tag', 'probabilities', 'set', 'according', 'word', 'ending']\n",
      "['suffix', 'strong', 'predictor', 'word', 'classes', 'words', 'wall', 'street', 'journal', 'part', 'penn', 'treebank', 'ending', 'able', 'adjectives', 'cases', 'fashionable', 'variable', 'rest', 'nouns', 'cable', 'variable']\n",
      "['probability', 'distribution', 'particular', 'suffix', 'generated', 'words', 'training', 'set', 'share', 'suffix', 'predefined', 'maximum', 'length']\n",
      "['term', 'suffix', 'used', 'means', 'quot', 'final', 'sequence', 'characters', 'word', 'quot', 'necessarily', 'linguistically', 'meaningful', 'suffix']\n",
      "['probabilities', 'smoothed', 'successive', 'abstraction']\n",
      "['calculates', 'probability', 'tag', 'given', 'last', 'letters', 'n', 'letter', 'word', 'p', 'ln']\n",
      "['sequence', 'increasingly', 'general', 'contexts', 'omits', 'characters', 'suffix', 'p', 'p', 'p', 'used', 'smoothing']\n",
      "['recursion', 'formula', 'set', 'foreach', 'trigram', 'f', 'ti', 'depending', 'maximum', 'following', 'three', 'values', 'using', 'maximum', 'likelihood', 'estimates', 'p', 'frequencies', 'lexicon', 'weights', 'oi', 'initialization', 'markov', 'model', 'need', 'inverse', 'conditional', 'probabilities', 'p', 'obtained', 'bayesian', 'inversion']\n",
      "['theoretical', 'motivated', 'argumentation', 'uses', 'standard', 'deviation', 'maximum', 'likelihood', 'probabilities', 'weights', 'samuelsson']\n",
      "['leaves', 'room', 'interpretation']\n",
      "['use', 'longest', 'suffix', 'find', 'training', 'set', 'frequency', 'greater', 'equal', 'characters']\n",
      "['empirically', 'determined', 'choice']\n",
      "['use', 'approach', 'contextual', 'weights']\n",
      "['turned', 'good', 'choice', 'set', 'standard', 'deviation', 'unconditioned', 'maximum', 'likelihood', 'probabilities', 'tags', 'training', 'corpus', 'set', 'using', 'tagset', 'tags', 'average', 'usually', 'yields', 'values', 'range']\n",
      "['use', 'different', 'estimates', 'uppercase', 'lowercase', 'words', 'maintain', 'two', 'different', 'suffix', 'tries', 'depending', 'capitalization', 'word']\n",
      "['information', 'improves', 'tagging', 'results']\n",
      "['another', 'freedom', 'concerns', 'choice', 'words', 'lexicon', 'used', 'suffix', 'handling']\n",
      "['use', 'words', 'better', 'suited', 'others']\n",
      "['accepting', 'unknown', 'words', 'probably', 'infrequent', 'one', 'argue', 'using', 'suffixes', 'infrequent', 'words', 'lexicon', 'better', 'approximation', 'unknown', 'words', 'using', 'suffixes', 'frequent', 'words']\n",
      "['therefore', 'restrict', 'procedure', 'suffix', 'handling', 'words', 'frequency', 'smaller', 'equal', 'threshold', 'value']\n",
      "['empirically', 'turned', 'good', 'choice', 'threshold']\n",
      "['additional', 'information', 'turned', 'useful', 'disambiguation', 'process', 'several', 'corpora', 'tagsets', 'capitalization', 'information']\n",
      "['tags', 'usually', 'informative', 'capitalization', 'probability', 'distributions', 'tags', 'around', 'capitalized', 'words', 'different', 'capitalized']\n",
      "['effect', 'larger', 'english', 'capitalizes', 'proper', 'names', 'smaller', 'german', 'capitalizes', 'nouns']\n",
      "['use', 'flags', 'ci', 'true', 'wi', 'capitalized', 'word', 'false', 'otherwise']\n",
      "['flags', 'added', 'contextual', 'probability', 'distributions']\n",
      "['instead', 'equations', 'updated', 'accordingly']\n",
      "['equivalent', 'doubling', 'size', 'tagset', 'using', 'different', 'tags', 'depending', 'capitalization']\n",
      "['processing', 'time', 'viterbi', 'algorithm', 'rabiner', 'reduced', 'introducing', 'beam', 'search']\n",
      "['state', 'receives', 'value', 'smaller', 'largest', 'divided', 'threshold', 'value', 'excluded', 'processing']\n",
      "['viterbi', 'algorithm', 'guaranteed', 'find', 'sequence', 'states', 'highest', 'probability', 'longer', 'true', 'beam', 'search', 'added']\n",
      "['nevertheless', 'practical', 'purposes', 'right', 'choice', 'virtually', 'difference', 'algorithm', 'without', 'beam']\n",
      "['empirically', 'value', 'turned', 'approximately', 'double', 'speed', 'tagger', 'without', 'affecting', 'accuracy']\n",
      "['tagger', 'currently', 'tags', 'tokens', 'per', 'second', 'including', 'file', 'pentium', 'running', 'linux']\n",
      "['speed', 'mainly', 'depends', 'percentage', 'unknown', 'words', 'average', 'ambiguity', 'rate']\n",
      "['evaluate', 'tagger', 'performance', 'several', 'aspects']\n",
      "['first', 'determine', 'tagging', 'accuracy', 'averaged', 'ten', 'iterations']\n",
      "['overall', 'accuracy', 'well', 'separate', 'accuracies', 'known', 'unknown', 'words', 'measured']\n",
      "['second', 'learning', 'curves', 'presented', 'indicate', 'performance', 'using', 'training', 'corpora', 'different', 'sizes', 'starting', 'tokens', 'ranging', 'size', 'entire', 'corpus', 'minus', 'test', 'set']\n",
      "['important', 'characteristic', 'statistical', 'taggers', 'assign', 'tags', 'words', 'also', 'probabilities', 'order', 'rank', 'different', 'assignments']\n",
      "['distinguish', 'reliable', 'unreliable', 'assignments', 'quotient', 'best', 'second', 'best', 'assignmentsl']\n",
      "['assignments', 'quotient', 'larger', 'threshold', 'regarded', 'reliable', 'others', 'unreliable']\n",
      "['see', 'accuracies', 'reliable', 'assignments', 'much', 'higher']\n",
      "['tests', 'performed', 'partitions', 'corpora', 'use', 'training', 'set', 'test', 'set', 'test', 'data', 'guaranteed', 'unseen', 'training']\n",
      "['result', 'obtained', 'repeating', 'experiment', 'times', 'different', 'partitions', 'averaging', 'single', 'outcomes']\n",
      "['experiments', 'contiguous', 'test', 'sets', 'used']\n",
      "['alternative', 'procedure', 'puts', 'every', 'sentence', 'test', 'set']\n",
      "['argue', 'contiguous', 'test', 'sets', 'yield', 'realistic', 'results', 'completely', 'unseen', 'articles', 'tagged']\n",
      "['using', 'procedure', 'parts', 'article', 'already', 'seen', 'significantly', 'reduces', 'percentage', 'unknown', 'words']\n",
      "['therefore', 'expect', 'even', 'definition', 'quotient', 'oo', 'one', 'possible', 'tag', 'given', 'word', 'higher', 'results', 'testing', 'every', 'sentence', 'instead', 'contiguous', 'set']\n",
      "['following', 'accuracy', 'denotes', 'number', 'correctly', 'assigned', 'tags', 'divided', 'number', 'tokens', 'corpus', 'processed']\n",
      "['tagger', 'allowed', 'assign', 'exactly', 'one', 'tag', 'token']\n",
      "['distinguish', 'overall', 'accuracy', 'taking', 'account', 'tokens', 'test', 'corpus', 'separate', 'accuracies', 'known', 'unknown', 'tokens']\n",
      "['latter', 'interesting', 'since', 'usually', 'unknown', 'tokens', 'much', 'difficult', 'process', 'known', 'tokens', 'list', 'valid', 'tags', 'found', 'lexicon']\n",
      "['german', 'negra', 'corpus', 'consists', 'sentences', 'tokens', 'newspaper', 'texts', 'frankfurter', 'rundschau', 'annotated', 'structures', 'skut', 'et']\n",
      "['developed', 'saarland', 'university']\n",
      "['part', 'tagged', 'ims', 'stuttgart']\n",
      "['evaluation', 'uses', 'annotation', 'ignores', 'structural', 'annotations']\n",
      "['tagging', 'accuracies', 'negra', 'corpus', 'shown', 'table']\n",
      "['figure', 'shows', 'learning', 'curve', 'tagger', 'accuracy', 'depending', 'amount', 'training', 'data']\n",
      "['training', 'length', 'number', 'tokens', 'used', 'training']\n",
      "['training', 'length', 'tested', 'ten', 'times', 'training', 'test', 'sets', 'randomly', 'chosen', 'disjoint', 'results', 'averaged']\n",
      "['training', 'length', 'given', 'logarithmic', 'scale']\n",
      "['remarkable', 'tagging', 'accuracy', 'known', 'words', 'high', 'even', 'small', 'training', 'corpora']\n",
      "['means', 'good', 'chance', 'getting', 'right', 'tag', 'word', 'seen', 'least', 'training']\n",
      "['average', 'percentages', 'unknown', 'tokens', 'shown', 'bottom', 'line', 'diagram']\n",
      "['exploit', 'fact', 'tagger', 'determines', 'tags', 'also', 'assigns', 'probabilities']\n",
      "['alternative', 'probability', 'quot', 'close', 'quot', 'best', 'assignment', 'alternative', 'viewed', 'almost', 'equally', 'well', 'suited']\n",
      "['notion', 'quot', 'close', 'quot', 'expressed', 'distance', 'probabilities', 'turn', 'expressed', 'quotient', 'probabilities']\n",
      "['distance', 'probabilities', 'best', 'tag', 'tbest', 'alternative', 'tag', 'tau', 'expressed', 'p', 'tbest', 'talt', 'value', 'greater', 'equal', 'since', 'best', 'tag', 'assignment', 'highest', 'probability']\n",
      "['figure', 'shows', 'accuracy', 'separating', 'assignments', 'quotients', 'larger', 'smaller', 'threshold', 'hence', 'reliable', 'unreliable', 'assignments']\n",
      "['expected', 'find', 'accuracies', 'percentage', 'known', 'unknown', 'overall', 'unknowns', 'acc', 'acc', 'acc', 'table', 'tagging', 'accuracy', 'penn', 'treebank']\n",
      "['table', 'shows', 'percentage', 'unknown', 'tokens', 'separate', 'accuracies', 'standard', 'deviations', 'known', 'unknown', 'tokens', 'well', 'overall', 'accuracy', 'percentage', 'known', 'unknown', 'overall', 'unknowns', 'acc', 'acc', 'acc', 'reliable', 'assignments', 'much', 'higher', 'unreliable', 'assignments']\n",
      "['distinction', 'useful', 'annotation', 'projects', 'cleaning', 'process', 'tagger', 'emit', 'multiple', 'tags', 'best', 'tag', 'classified', 'unreliable']\n",
      "['use', 'wall', 'street', 'journal', 'contained', 'penn', 'treebank', 'experiments']\n",
      "['annotation', 'consists', 'four', 'parts', 'structure', 'augmented', 'traces', 'mark', 'movement', 'discontinuous', 'constituents', 'phrasal', 'categories', 'annotated', 'node', 'labels', 'small', 'set', 'grammatical', 'functions', 'annotated', 'extensions', 'node', 'labels', 'tags', 'marcus', 'et']\n",
      "['evaluation', 'uses', 'annotation']\n",
      "['wall', 'street', 'journal', 'part', 'penn', 'treebank', 'consists', 'approx']\n",
      "['sentences', 'million', 'tokens']\n",
      "['tagging', 'accuracies', 'penn', 'treebank', 'shown', 'table']\n",
      "['figure', 'shows', 'learning', 'curve', 'tagger', 'accuracy', 'depending', 'amount', 'training', 'data']\n",
      "['training', 'length', 'number', 'tokens', 'used', 'training']\n",
      "['training', 'length', 'tested', 'ten', 'times']\n",
      "['training', 'test', 'sets', 'disjoint', 'results', 'averaged']\n",
      "['training', 'length', 'given', 'logarithmic', 'scale']\n",
      "['negra', 'corpus', 'tagging', 'accuracy', 'high', 'known', 'tokens', 'even', 'small', 'amounts', 'training', 'data']\n",
      "['exploit', 'fact', 'tagger', 'determines', 'tags', 'also', 'assigns', 'probabilities']\n",
      "['figure', 'shows', 'accuracy', 'separating', 'assignments', 'quotients', 'larger', 'smaller', 'threshold', 'hence', 'reliable', 'unreliable', 'assignments']\n",
      "['find', 'accuracies', 'reliable', 'assignments', 'much', 'higher', 'unreliable', 'assignments']\n",
      "['average', 'tagging', 'accuracy', 'depending', 'language', 'tagset', 'least', 'par', 'results', 'found', 'literature', 'possibly', 'better']\n",
      "['penn', 'treebank', 'ratnaparkhi', 'reports', 'accuracy', 'using', 'maximum', 'entropy', 'approach', 'much', 'simpler', 'therefore', 'faster', 'hmm', 'approach', 'delivers']\n",
      "['comparison', 'needs', 'since', 'use', 'crossvalidation', 'averaging', 'results', 'ratnaparkhi', 'makes', 'one', 'test', 'run']\n",
      "['accuracy', 'known', 'tokens', 'significantly', 'higher', 'unknown', 'tokens']\n",
      "['german', 'newspaper', 'data', 'results', 'better', 'word', 'seen', 'therefore', 'lexicon', 'seen']\n",
      "['accuracy', 'known', 'tokens', 'high', 'even', 'small', 'amounts', 'training', 'data']\n",
      "['tokens', 'sufficient', 'achieve', 'accuracy']\n",
      "['important', 'tagger', 'seen', 'word', 'least', 'training']\n",
      "['stochastic', 'taggers', 'assign', 'probabilities', 'tags']\n",
      "['exploit', 'probabilities', 'determine', 'reliability', 'assignments']\n",
      "['subset', 'determined', 'processing', 'tagger', 'achieve', 'accuracy', 'rates']\n",
      "['accuracy', 'complement', 'set', 'much', 'lower']\n",
      "['information', 'exploited', 'annotation', 'project', 'give', 'additional', 'treatment', 'unreliable', 'assignments', 'pass', 'selected', 'ambiguities', 'subsequent', 'processing', 'step']\n",
      "['shown', 'tagger', 'based', 'markov', 'models', 'yields', 'results', 'despite', 'contrary', 'claims', 'found', 'literature']\n",
      "['example', 'markov', 'model', 'tagger', 'used', 'comparison', 'van', 'halteren', 'et', 'yielded', 'worse', 'results', 'taggers']\n",
      "['opinion', 'reason', 'wrong', 'claim', 'basic', 'algorithms', 'leave', 'several', 'decisions', 'implementor']\n",
      "['rather', 'large', 'amount', 'freedom', 'handled', 'detail', 'previous', 'publications', 'handling', 'exact', 'smoothing', 'technique', 'determine', 'weights', 'context', 'probabilities', 'details', 'handling', 'unknown', 'words', 'determine', 'weights', 'unknown', 'words']\n",
      "['note', 'decisions', 'made', 'yield', 'good', 'results', 'german', 'english', 'corpus']\n",
      "['several', 'corpora', 'well']\n",
      "['architecture', 'remains', 'applicable', 'large', 'variety', 'languages']\n",
      "['according', 'current', 'tagger', 'comparisons', 'van', 'halteren', 'et', 'zavrel', 'daelemans', 'according', 'comparsion', 'results', 'presented', 'ratnaparkhi', 'maximum', 'entropy', 'framework', 'seems', 'approach', 'yielding', 'comparable', 'results', 'one', 'presented']\n",
      "['interesting', 'future', 'research', 'topic', 'determine', 'advantages', 'either', 'approaches', 'find', 'reason', 'high', 'accuracies', 'find', 'good', 'combination']\n",
      "['tnt', 'freely', 'available', 'universities', 'related', 'organizations', 'research', 'purposes', 'see', 'http']\n",
      "['many', 'thanks', 'go', 'hans', 'uszkoreit', 'support', 'development', 'tnt']\n",
      "['work', 'tnt', 'carried', 'author', 'received', 'grant', 'deutsche', 'forschungsgemeinschaft', 'graduiertenkolleg', 'kognitionswissenschaft', 'saarbriicken']\n",
      "['large', 'annotated', 'corpora', 'developing', 'testing', 'taggers', 'enable', 'generation', 'language', 'models']\n",
      "['therefore', 'would', 'like', 'thank', 'people', 'took', 'effort', 'annotate', 'penn', 'treebank', 'susanne', 'corpus', 'stuttgarter', 'referenzkorpus', 'negra', 'corpus', 'verbmobil', 'corpora', 'several', 'others']\n",
      "['last', 'least', 'would', 'like', 'thank', 'users', 'tnt', 'provided', 'bug', 'reports', 'valuable', 'suggestions', 'improvements']\n"
     ]
    }
   ],
   "source": [
    "new_corpus = corpus.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7786ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentences', 'dso', 'collection', 'tagged', 'parts', 'speech', 'using', 'tnt', 'brants', 'trained', 'brown', 'corpus']\n",
      "[]\n",
      "['english', 'carried', 'using', 'freely', 'available', 'tnt', 'tagger', 'brants']\n",
      "['proposition', 'quite', 'viable', 'statistical', 'pos', 'taggers', 'like', 'tnt', 'brants', 'available']\n",
      "['use', 'tnt', 'brants', 'second', 'order', 'markov', 'model', 'tagger']\n",
      "['pos', 'tagging', 'lemmatization', 'combine', 'genia', 'occasionally', 'deviant', 'kenizer', 'tnt', 'brants', 'operates', 'inputs', 'default', 'models', 'trained', 'financial', 'news', 'penn', 'tree', 'bank']\n",
      "['tag', 'tokens', 'pos', 'tags', 'using', 'tagger', 'brants']\n",
      "['example', 'petrov', 'et', 'al', 'build', 'supervised', 'pos', 'taggers', 'languages', 'using', 'tnt', 'tagger', 'brants', 'average', 'accuracy']\n",
      "['forun', 'aligned', 'words', 'simply', 'assign', 'random', 'pos', 'low', 'probability', 'substantially', 'affect', 'transition', 'probability', 'estimates', 'step', 'build', 'tagger', 'feeding', 'es', 'ti', 'mated', 'emission', 'transition', 'probabilities', 'tnt', 'tagger', 'brants', 'implementation', 'trigram', 'hmm', 'tagger']\n",
      "['based', 'various', 'complexity', 'grammar', 'model', 'using', 'tags', 'brants', 'achieved', 'automated', 'tagging', 'set', 'grammatical', 'function', 'tags', 'including', 'modifiers', 'trained', 'supervised', 'mode', 'tree', 'bank', 'german']\n",
      "['also', 'incorporated', 'speech', 'tagging', 'using', 'tnt', 'tagger', 'brants', 'retrained', 'genia', 'corpus', 'gold', 'standard', 'part', 'tagging']\n",
      "['pos', 'majority', 'lexical', 'type', 'noun', 'verb', 'adj', 'adv', 'table', 'pos', 'tags', 'lexical', 'types', 'mapping', 'comparison', 'built', 'another', 'simple', 'baseline', 'model', 'using', 'tnt', 'pos', 'tagger', 'brants']\n",
      "['texts', 'using', 'tnt']\n",
      "['pos', 'lexicon', 'sharoff', 'et', 'al', 'specifically', 'file', 'pos', 'tagger', 'tnt', 'brants', 'contains', 'full', 'words', 'unique', 'forms', 'frequency', 'information']\n",
      "['use', 'corpus', 'million', 'words', 'automatically', 'tagged', 'tnt', 'brants', 'freely', 'available', 'online', 'sharoff', 'et', 'al', 'want', 'make', 'corruptions', 'corrupt', 'words', 'information', 'identifying', 'words', 'corpus', 'found', 'lexicon', 'appropriate', 'pos', 'tag', 'also', 'select', 'words', 'inflectional', 'morphology', 'nouns', 'verbs', 'adjectives', 'pronouns', 'determining', 'word', 'properties', 'step', 'use', 'pos', 'tag', 'restrict', 'properties', 'word', 'regardless', 'exactly', 'corrupt']\n",
      "['pos', 'tag', 'use', 'hmm', 'tagger', 'tnt', 'brants', 'model', 'http']\n",
      "['finishing', 'corrections', 'experimented', 'training', 'testing', 'tnt', 'tagger', 'quot', 'old', 'quot', 'quot', 'corrected', 'quot', 'version', 'negra']\n",
      "['make', 'useful', 'necessary', 'preprocessing', 'steps', 'must', 'done', 'texts', 'first', 'automatically', 'segmented', 'tokenized', 'tagged', 'tnt', 'tagger', 'brants', 'trained', 'respective', 'wils', 'training', 'data']\n",
      "['pos', 'tags', 'represent', 'challenge', 'norm', 'lemma', 'pos', 'agreed', 'tokens', 'accuracy', 'table', 'agreement', 'agreement', 'two', 'annotators', 'cons', 'id', 'erably', 'lower', 'agreement', 'level', 'reported', 'annotating', 'corpus', 'modern', 'german', 'using', 'stts', 'brants']\n",
      "['plan', 'retrain', 'pos', 'taggers', 'treetagger', 'tnt', 'tagger', 'brants', 'data', 'finally', 'plan', 'investigate', 'linguistic', 'annotations', 'automatically', 'integrated', 'tei', 'annotated', 'version', 'corpus', 'produce', 'tei', 'con', 'formant', 'output']\n"
     ]
    }
   ],
   "source": [
    "q_lis = []\n",
    "for q in queries:\n",
    "    a = preprocess(q)\n",
    "    if len(a)>1:\n",
    "        q_lis.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8711d15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentences dso collection tagged parts speech using tnt brants trained brown corpus',\n",
       " 'english carried using freely available tnt tagger brants',\n",
       " 'proposition quite viable statistical pos taggers like tnt brants available',\n",
       " 'use tnt brants second order markov model tagger',\n",
       " 'pos tagging lemmatization combine genia occasionally deviant kenizer tnt brants operates inputs default models trained financial news penn tree bank',\n",
       " 'tag tokens pos tags using tagger brants',\n",
       " 'example petrov et al build supervised pos taggers languages using tnt tagger brants average accuracy',\n",
       " 'forun aligned words simply assign random pos low probability substantially affect transition probability estimates step build tagger feeding es ti mated emission transition probabilities tnt tagger brants implementation trigram hmm tagger',\n",
       " 'based various complexity grammar model using tags brants achieved automated tagging set grammatical function tags including modifiers trained supervised mode tree bank german',\n",
       " 'also incorporated speech tagging using tnt tagger brants retrained genia corpus gold standard part tagging',\n",
       " 'pos majority lexical type noun verb adj adv table pos tags lexical types mapping comparison built another simple baseline model using tnt pos tagger brants',\n",
       " 'texts using tnt',\n",
       " 'pos lexicon sharoff et al specifically file pos tagger tnt brants contains full words unique forms frequency information',\n",
       " 'use corpus million words automatically tagged tnt brants freely available online sharoff et al want make corruptions corrupt words information identifying words corpus found lexicon appropriate pos tag also select words inflectional morphology nouns verbs adjectives pronouns determining word properties step use pos tag restrict properties word regardless exactly corrupt',\n",
       " 'pos tag use hmm tagger tnt brants model http',\n",
       " 'finishing corrections experimented training testing tnt tagger quot old quot quot corrected quot version negra',\n",
       " 'make useful necessary preprocessing steps must done texts first automatically segmented tokenized tagged tnt tagger brants trained respective wils training data',\n",
       " 'pos tags represent challenge norm lemma pos agreed tokens accuracy table agreement agreement two annotators cons id erably lower agreement level reported annotating corpus modern german using stts brants',\n",
       " 'plan retrain pos taggers treetagger tnt tagger brants data finally plan investigate linguistic annotations automatically integrated tei annotated version corpus produce tei con formant output']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59dd9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_sentences(model):\n",
    "    # Get a vector for each headline (sentence) in the corpus\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "    corpus_embeddings = corpus_embeddings/np.linalg.norm(corpus_embeddings,axis=0).reshape(-1)\n",
    "    # Define search queries and embed them to vectors as well\n",
    "\n",
    "    query_embeddings = model.encode(queries)\n",
    "    # For each search term return 5 closest sentences\n",
    "    closest_n = 5\n",
    "\n",
    "    for i in range(len(queries)):\n",
    "        query, query_embedding  = queries[i], query_embeddings[i]\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "    #     results = dict(enumerate(distances,1))\n",
    "    #     results = dict(sorted(results.items(), key=lambda item: item[1]))\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query)\n",
    "        print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "        indexes = results[0:closest_n]\n",
    "        top_n = []\n",
    "        for l,k in indexes:\n",
    "            top_n.append(l)\n",
    "        print(np.intersect1d(top_n,gt[cite_no[i]]))\n",
    "        print(\"indexes{}, top_n{}, i{}, cite_no[i]{} ,gt[cite_no[i]]{}\".format(indexes, top_n,i,cite_no[i],gt[cite_no[i]]))\n",
    "\n",
    "    #     for idx, distance in results[0:closest_n]:\n",
    "    #         print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad8123fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-0.3931,  0.0389,  1.9874,  ..., -0.6094, -1.0946,  0.3265],\n",
      "        [ 0.0615,  0.3274,  1.8332,  ..., -0.1299,  0.4609,  0.2404]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d30925c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37231/2639810534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a vector for each headline (sentence) in the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/new/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# Get a vector for each headline (sentence) in the corpus\n",
    "corpus_embeddings = model.encode(new_corpus)\n",
    "print(len(corpus_embeddings[1]))\n",
    "corpus_embeddings = corpus_embeddings/np.linalg.norm(corpus_embeddings,axis=0).reshape(-1)\n",
    "print(corpus_embeddings[1][0])\n",
    "# Define search queries and embed them to vectors as well\n",
    "\n",
    "query_embeddings = model.encode(q_lis)\n",
    "query_embeddings  = query_embeddings/np.linalg.norm(query_embeddings ,axis=0).reshape(-1)\n",
    "\n",
    "# For each search term return 5 closest sentences\n",
    "closest_n = 10\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    query, query_embedding  = queries[i], query_embeddings[i]\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "#     results = dict(enumerate(distances,1))\n",
    "#     results = dict(sorted(results.items(), key=lambda item: item[1]))\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    indexes = results[0:closest_n]\n",
    "    top_n = []\n",
    "    for l,k in indexes:\n",
    "        top_n.append(l)\n",
    "    print(np.intersect1d(top_n,gt[cite_no[i]]))\n",
    "    print(\"indexes{}, top_n{}, i{}, cite_no[i]{} ,gt[cite_no[i]]{}\".format(indexes, top_n,i,cite_no[i],gt[cite_no[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# #Define your train examples. You need more than just two examples...\n",
    "\n",
    "# train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "#     InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "# #Define your train dataset, the dataloader and the train loss\n",
    "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# #Tune the model\n",
    "# model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(queries)):\n",
    "    for j in gt[cite_no[i]]:\n",
    "        if j < len(corpus):\n",
    "#             print(queries[i],gt[cite_no[i]],corpus[j])\n",
    "            data.append(InputExample(texts=[queries[i],corpus[j]],label=0.8))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf27f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10000, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e643bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matching_sentences(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef832cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-distilroberta-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-distilroberta-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ba9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6336f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e77038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
