{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64406aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P83-1020',\n",
       " 'W02-1039',\n",
       " 'P96-1021',\n",
       " 'N06-1020',\n",
       " 'C86-1016',\n",
       " 'C92-2070',\n",
       " 'D08-1082',\n",
       " 'J01-2002',\n",
       " 'C02-1114',\n",
       " 'P85-1018',\n",
       " 'P07-1032',\n",
       " 'W00-1427',\n",
       " 'C04-1111',\n",
       " 'J97-1002',\n",
       " 'W04-3237',\n",
       " 'E87-1002',\n",
       " 'P93-1041',\n",
       " 'P90-1034',\n",
       " 'P98-1035',\n",
       " 'W02-0505',\n",
       " 'P13-1045',\n",
       " 'A00-2026',\n",
       " 'J03-4004',\n",
       " 'N03-1003',\n",
       " 'C90-2067',\n",
       " 'P84-1075',\n",
       " 'W02-0908',\n",
       " 'M95-1005',\n",
       " 'P07-1028',\n",
       " 'C00-2136',\n",
       " 'P01-1019',\n",
       " 'N10-1056',\n",
       " 'J81-4003',\n",
       " 'P93-1024',\n",
       " 'C90-3030',\n",
       " 'W02-2024',\n",
       " 'P98-1013',\n",
       " 'P03-1019',\n",
       " 'P07-1065',\n",
       " 'I05-3017',\n",
       " 'P07-1056',\n",
       " 'W03-0419',\n",
       " 'W00-0726',\n",
       " 'A00-2031',\n",
       " 'D07-1090',\n",
       " 'P02-1040',\n",
       " 'P05-1011',\n",
       " 'P02-1043',\n",
       " 'A00-2009',\n",
       " 'H94-1046',\n",
       " 'C00-1044',\n",
       " 'P11-1138',\n",
       " 'C02-1011',\n",
       " 'C96-1055',\n",
       " 'N06-2015',\n",
       " 'N03-1024',\n",
       " 'P08-1101',\n",
       " 'P96-1025',\n",
       " 'P02-1019',\n",
       " 'P09-1088',\n",
       " 'C94-1042',\n",
       " 'P99-1068',\n",
       " 'N07-1018',\n",
       " 'D07-1043',\n",
       " 'J93-1007',\n",
       " 'C08-1022',\n",
       " 'P01-1030',\n",
       " 'J01-2001',\n",
       " 'N01-1008',\n",
       " 'P05-3026',\n",
       " 'P96-1024',\n",
       " 'A00-2019',\n",
       " 'E06-1042',\n",
       " 'N06-1014',\n",
       " 'P03-1011',\n",
       " 'P11-1016',\n",
       " 'C92-2082',\n",
       " 'P02-1038',\n",
       " 'J01-4004',\n",
       " 'N09-1046',\n",
       " 'D11-1125',\n",
       " 'P94-1020',\n",
       " 'W04-2609',\n",
       " 'P09-1027',\n",
       " 'H05-1073',\n",
       " 'E06-1051',\n",
       " 'P00-1065',\n",
       " 'P04-1085',\n",
       " 'P02-1017',\n",
       " 'J91-1002',\n",
       " 'J00-3004',\n",
       " 'D09-1159',\n",
       " 'P08-1004',\n",
       " 'P02-1014',\n",
       " 'W04-3250',\n",
       " 'W02-1018',\n",
       " 'J00-4003',\n",
       " 'J03-3002',\n",
       " 'P00-1071',\n",
       " 'D07-1101',\n",
       " 'P84-1018',\n",
       " 'N09-1009',\n",
       " 'P02-1060',\n",
       " 'A97-1052',\n",
       " 'C00-2137',\n",
       " 'P05-1057',\n",
       " 'D08-1022',\n",
       " 'C92-3150',\n",
       " 'J92-1004',\n",
       " 'P06-1067',\n",
       " 'E06-1031',\n",
       " 'J04-4004',\n",
       " 'N06-1039',\n",
       " 'W02-2018',\n",
       " 'P07-1121',\n",
       " 'W04-0807',\n",
       " 'P87-1033',\n",
       " 'N03-2002',\n",
       " 'W02-1001',\n",
       " 'J05-1003',\n",
       " 'D08-1031',\n",
       " 'J93-2005',\n",
       " 'N01-1023',\n",
       " 'W03-0407',\n",
       " 'C04-1051',\n",
       " 'N04-1015',\n",
       " 'D10-1001',\n",
       " 'A00-1031',\n",
       " 'P08-2007',\n",
       " 'P09-1010',\n",
       " 'I08-1059',\n",
       " 'J00-4005',\n",
       " 'E99-1023',\n",
       " 'M95-1012',\n",
       " 'C94-2178',\n",
       " 'P08-1086',\n",
       " 'W04-3208',\n",
       " 'W03-1508',\n",
       " 'N06-1058',\n",
       " 'P01-1064',\n",
       " 'P88-1020',\n",
       " 'J02-2003',\n",
       " 'E06-1040',\n",
       " 'P07-1031',\n",
       " 'J07-3004',\n",
       " 'P02-1006',\n",
       " 'P02-1062',\n",
       " 'C94-2174',\n",
       " 'N06-1006',\n",
       " 'N09-1028',\n",
       " 'P90-1005',\n",
       " 'C04-1010',\n",
       " 'P03-1071',\n",
       " 'N04-1021',\n",
       " 'A94-1009',\n",
       " 'N01-1020',\n",
       " 'P08-1024',\n",
       " 'P09-1068',\n",
       " 'N07-1030',\n",
       " 'J86-3001',\n",
       " 'J03-1005',\n",
       " 'N04-3012',\n",
       " 'D12-1133',\n",
       " 'N12-1052',\n",
       " 'H05-1066',\n",
       " 'P07-1094',\n",
       " 'P08-1108',\n",
       " 'P04-1075',\n",
       " 'P93-1001',\n",
       " 'N04-1014',\n",
       " 'P11-1020',\n",
       " 'W04-3219',\n",
       " 'J93-1003',\n",
       " 'S10-1011',\n",
       " 'P96-1006',\n",
       " 'J93-2006',\n",
       " 'P95-1007',\n",
       " 'J94-2003',\n",
       " 'D07-1002',\n",
       " 'P09-1042',\n",
       " 'E99-1010',\n",
       " 'W03-1730',\n",
       " 'D09-1001',\n",
       " 'J94-4003',\n",
       " 'N03-1021',\n",
       " 'A94-1016',\n",
       " 'P93-1020',\n",
       " 'J02-1002',\n",
       " 'H94-1048',\n",
       " 'W03-0301',\n",
       " 'W04-0811',\n",
       " 'J94-4002',\n",
       " 'P97-1009',\n",
       " 'N04-4038',\n",
       " 'P04-1061',\n",
       " 'J03-3005',\n",
       " 'J91-1003',\n",
       " 'J90-1003',\n",
       " 'P99-1059',\n",
       " 'P00-1037',\n",
       " 'W04-3103',\n",
       " 'D10-1048',\n",
       " 'P08-1084',\n",
       " 'J96-1002',\n",
       " 'W01-0501',\n",
       " 'H05-1010',\n",
       " 'W03-0405',\n",
       " 'H92-1026',\n",
       " 'P04-1018',\n",
       " 'J97-3002',\n",
       " 'P01-1008',\n",
       " 'J96-1001',\n",
       " 'N03-1017',\n",
       " 'C04-1080',\n",
       " 'P07-1004',\n",
       " 'P05-1036',\n",
       " 'P91-1027',\n",
       " 'P03-1056',\n",
       " 'P93-1008',\n",
       " 'W03-1028',\n",
       " 'P04-1041',\n",
       " 'P97-1005',\n",
       " 'P06-1124',\n",
       " 'J98-2004',\n",
       " 'J92-4007',\n",
       " 'P92-1005',\n",
       " 'C96-2183',\n",
       " 'P94-1012',\n",
       " 'W00-1308',\n",
       " 'J98-4004',\n",
       " 'W03-1812',\n",
       " 'P09-1077',\n",
       " 'P91-1034',\n",
       " 'J87-1004',\n",
       " 'N04-1013',\n",
       " 'P02-1039',\n",
       " 'W02-0902',\n",
       " 'N09-1036',\n",
       " 'W03-0501',\n",
       " 'D08-1092',\n",
       " 'P98-1012',\n",
       " 'P07-1005',\n",
       " 'H05-1004',\n",
       " 'J92-4003',\n",
       " 'D08-1035',\n",
       " 'D07-1072',\n",
       " 'D07-1104',\n",
       " 'P91-1023',\n",
       " 'N04-4015',\n",
       " 'P02-1033',\n",
       " 'P98-2182',\n",
       " 'P89-1010',\n",
       " 'P03-1023',\n",
       " 'P03-1054',\n",
       " 'J03-1003',\n",
       " 'W04-2705',\n",
       " 'N03-1016',\n",
       " 'P02-1035',\n",
       " 'P07-1049',\n",
       " 'P03-1012',\n",
       " 'N06-1025',\n",
       " 'I05-2038',\n",
       " 'J04-2003',\n",
       " 'P08-1066',\n",
       " 'P05-1034',\n",
       " 'W04-3206',\n",
       " 'P97-1035',\n",
       " 'P06-1043',\n",
       " 'P07-1037',\n",
       " 'N06-1011',\n",
       " 'S10-1010',\n",
       " 'N12-1047',\n",
       " 'P07-1019',\n",
       " 'P06-2005',\n",
       " 'P03-1035',\n",
       " 'N09-2004',\n",
       " 'J06-1003',\n",
       " 'W03-1728',\n",
       " 'P97-1063',\n",
       " 'P06-1114',\n",
       " 'P07-1059',\n",
       " 'P07-1034',\n",
       " 'H93-1051',\n",
       " 'N01-1021',\n",
       " 'N03-1014',\n",
       " 'P99-1041',\n",
       " 'W03-1017',\n",
       " 'P08-2012',\n",
       " 'P95-1037',\n",
       " 'C10-2028',\n",
       " 'P03-1003',\n",
       " 'J99-2004',\n",
       " 'P05-1001',\n",
       " 'N09-1037',\n",
       " 'P87-1022',\n",
       " 'P99-1032',\n",
       " 'P89-1009',\n",
       " 'D08-1016',\n",
       " 'H91-1026',\n",
       " 'P92-1008',\n",
       " 'C04-1197',\n",
       " 'H05-1012',\n",
       " 'W00-0403',\n",
       " 'D07-1109',\n",
       " 'P05-1074',\n",
       " 'J93-1001',\n",
       " 'N07-1023',\n",
       " 'D07-1007',\n",
       " 'P01-1005',\n",
       " 'P96-1038',\n",
       " 'P04-1021',\n",
       " 'P95-1021',\n",
       " 'P93-1005',\n",
       " 'W00-0717',\n",
       " 'C94-1027',\n",
       " 'J93-1002',\n",
       " 'H05-1043',\n",
       " 'P98-1034',\n",
       " 'N04-1016',\n",
       " 'N06-1033',\n",
       " 'P04-1013',\n",
       " 'P01-1025',\n",
       " 'C90-3063',\n",
       " 'N01-1024',\n",
       " 'W04-2319',\n",
       " 'P03-2026',\n",
       " 'P07-1073',\n",
       " 'E06-1032',\n",
       " 'P03-1013',\n",
       " 'E06-1015',\n",
       " 'W03-0428',\n",
       " 'J99-1003',\n",
       " 'P05-1072',\n",
       " 'C86-1045',\n",
       " 'P06-2014',\n",
       " 'J01-3003',\n",
       " 'W04-1013',\n",
       " 'J05-1004',\n",
       " 'W03-1810',\n",
       " 'P09-2004',\n",
       " 'P96-1027',\n",
       " 'P00-1016',\n",
       " 'W04-3239',\n",
       " 'H05-1091',\n",
       " 'I05-3027',\n",
       " 'C94-2195',\n",
       " 'S12-1053',\n",
       " 'P11-2031',\n",
       " 'P08-1030',\n",
       " 'J02-3001',\n",
       " 'C90-3044',\n",
       " 'P06-1005',\n",
       " 'P06-1055',\n",
       " 'D11-1014',\n",
       " 'P05-1010',\n",
       " 'E03-1008',\n",
       " 'W04-3236',\n",
       " 'P10-1001',\n",
       " 'P03-1009',\n",
       " 'P99-1069',\n",
       " 'P02-1034',\n",
       " 'P05-1018',\n",
       " 'J80-3003',\n",
       " 'P05-1033',\n",
       " 'P06-1097',\n",
       " 'W04-3213',\n",
       " 'P99-1071',\n",
       " 'P08-1036',\n",
       " 'P05-1045',\n",
       " 'P99-1067',\n",
       " 'N10-1061',\n",
       " 'P07-1107',\n",
       " 'D08-1065',\n",
       " 'P98-1010',\n",
       " 'P06-1103',\n",
       " 'P05-2008',\n",
       " 'P05-1066',\n",
       " 'P02-1046',\n",
       " 'J10-3003',\n",
       " 'W03-0425',\n",
       " 'W02-1503',\n",
       " 'W04-3201',\n",
       " 'P98-2204',\n",
       " 'D08-1083',\n",
       " 'W01-0521',\n",
       " 'P06-1091',\n",
       " 'P88-1015',\n",
       " 'D09-1101',\n",
       " 'J98-1001',\n",
       " 'J90-1004',\n",
       " 'J10-4006',\n",
       " 'P02-1001',\n",
       " 'P00-1058',\n",
       " 'H93-1061',\n",
       " 'H05-1044',\n",
       " 'P05-1022',\n",
       " 'P08-1076',\n",
       " 'P94-1002',\n",
       " 'D11-1006',\n",
       " 'A00-2034',\n",
       " 'J08-1002',\n",
       " 'P08-1115',\n",
       " 'P09-1104',\n",
       " 'C02-1139',\n",
       " 'P98-1112',\n",
       " 'N06-2013',\n",
       " 'D08-1059',\n",
       " 'J97-3003',\n",
       " 'P09-1039',\n",
       " 'J09-3003',\n",
       " 'P06-1066',\n",
       " 'J82-3004',\n",
       " 'N03-1022',\n",
       " 'P08-1012',\n",
       " 'J98-3005',\n",
       " 'W04-3207',\n",
       " 'E03-1076',\n",
       " 'W04-2401',\n",
       " 'P00-1010',\n",
       " 'C02-2025',\n",
       " 'P98-2177',\n",
       " 'A97-1039',\n",
       " 'D07-1074',\n",
       " 'D07-1111',\n",
       " 'N03-1026',\n",
       " 'P09-1057',\n",
       " 'J08-4003',\n",
       " 'P07-1007',\n",
       " 'D07-1076',\n",
       " 'P00-1027',\n",
       " 'P06-1009',\n",
       " 'P85-1011',\n",
       " 'C94-1032',\n",
       " 'D07-1103',\n",
       " 'D11-1141',\n",
       " 'D11-1142',\n",
       " 'P11-1019',\n",
       " 'I05-3025',\n",
       " 'P05-1067',\n",
       " 'P85-1008',\n",
       " 'P05-1059',\n",
       " 'A00-1043',\n",
       " 'D08-1027',\n",
       " 'D12-1050',\n",
       " 'J94-2001',\n",
       " 'P06-1014',\n",
       " 'C96-1021',\n",
       " 'J98-2001',\n",
       " 'W04-3247',\n",
       " 'W03-1719',\n",
       " 'C92-3126',\n",
       " 'W02-1006',\n",
       " 'J95-4004',\n",
       " 'A94-1006',\n",
       " 'W04-1221',\n",
       " 'C08-1107',\n",
       " 'J93-2003',\n",
       " 'W01-0511',\n",
       " 'P03-1022',\n",
       " 'P96-1008',\n",
       " 'D09-1058',\n",
       " 'P04-1005',\n",
       " 'P95-1034',\n",
       " 'J01-3001',\n",
       " 'P83-1021',\n",
       " 'P98-1069',\n",
       " 'J06-3003',\n",
       " 'D08-1014',\n",
       " 'W01-1605',\n",
       " 'H01-1035',\n",
       " 'J99-4004',\n",
       " 'J94-4001',\n",
       " 'H05-1059',\n",
       " 'A97-1030',\n",
       " 'J94-3001',\n",
       " 'J04-4002',\n",
       " 'E06-1038',\n",
       " 'P06-4020',\n",
       " 'P11-1055',\n",
       " 'P08-1088',\n",
       " 'E99-1001',\n",
       " 'P07-2045',\n",
       " 'P86-1004',\n",
       " 'H05-1079',\n",
       " 'C10-1152',\n",
       " 'J99-3001',\n",
       " 'J99-4005',\n",
       " 'P97-1041',\n",
       " 'N06-2033',\n",
       " 'P98-2173',\n",
       " 'P84-1008',\n",
       " 'P09-1116',\n",
       " 'P93-1023',\n",
       " 'P02-1053',\n",
       " 'W04-3230',\n",
       " 'W02-0603',\n",
       " 'P03-1021',\n",
       " 'N07-1011',\n",
       " 'J05-3002',\n",
       " 'D10-1119',\n",
       " 'E06-1011',\n",
       " 'J08-2005',\n",
       " 'D07-1061',\n",
       " 'P09-1040',\n",
       " 'C08-1109',\n",
       " 'N09-1003',\n",
       " 'W00-0730',\n",
       " 'P05-1071',\n",
       " 'H94-1020',\n",
       " 'C90-3045',\n",
       " 'P06-1115',\n",
       " 'P09-2012',\n",
       " 'P03-1044',\n",
       " 'J05-4003',\n",
       " 'C02-1054',\n",
       " 'C04-1100',\n",
       " 'C02-1145',\n",
       " 'C04-1024',\n",
       " 'P93-1035',\n",
       " 'P06-1085',\n",
       " 'P06-2006',\n",
       " 'N07-4013',\n",
       " 'P07-1091',\n",
       " 'P98-2127',\n",
       " 'P96-1011',\n",
       " 'P06-1004',\n",
       " 'N10-1119',\n",
       " 'P11-1038',\n",
       " 'P03-1001',\n",
       " 'E06-1027',\n",
       " 'D08-1024',\n",
       " 'J03-1002',\n",
       " 'C92-1038',\n",
       " 'N09-1012',\n",
       " 'N03-2021',\n",
       " 'J88-1003',\n",
       " 'W03-1014',\n",
       " 'C96-1079',\n",
       " 'P08-1023',\n",
       " 'J95-2003',\n",
       " 'P92-1017',\n",
       " 'W02-1021',\n",
       " 'E89-1037',\n",
       " 'P98-1106',\n",
       " 'J93-1006',\n",
       " 'N07-1029',\n",
       " 'P93-1016',\n",
       " 'P06-1077',\n",
       " 'W02-1502',\n",
       " 'W02-0301',\n",
       " 'P09-1058',\n",
       " 'P10-4002',\n",
       " 'H05-2018',\n",
       " 'P01-1067',\n",
       " 'P04-1043',\n",
       " 'W04-0308',\n",
       " 'N04-4026',\n",
       " 'N13-1090',\n",
       " 'P99-1048',\n",
       " 'N03-1033',\n",
       " 'P09-1074',\n",
       " 'P04-1053',\n",
       " 'W02-1011',\n",
       " 'D11-1033',\n",
       " 'P98-2180',\n",
       " 'P06-2094',\n",
       " 'J07-2003',\n",
       " 'J91-4003',\n",
       " 'W04-3212',\n",
       " 'D08-1076',\n",
       " 'J88-2003',\n",
       " 'P99-1065',\n",
       " 'P05-1077',\n",
       " 'P02-1047',\n",
       " 'A97-1011',\n",
       " 'D09-1030',\n",
       " 'W01-0514',\n",
       " 'D07-1096',\n",
       " 'P06-3002',\n",
       " 'W03-0404',\n",
       " 'P04-1066',\n",
       " 'C90-3052',\n",
       " 'D10-1125',\n",
       " 'P02-1042',\n",
       " 'P06-2101',\n",
       " 'N03-1028',\n",
       " 'C00-2163',\n",
       " 'J96-2004',\n",
       " 'P07-1036',\n",
       " 'P04-1077',\n",
       " 'C92-1025',\n",
       " 'P08-1067',\n",
       " 'P11-2033',\n",
       " 'J98-1006',\n",
       " 'P09-1094',\n",
       " 'P84-1085',\n",
       " 'N04-1043',\n",
       " 'P05-1052',\n",
       " 'P91-1022',\n",
       " 'P89-1002',\n",
       " 'C88-2147',\n",
       " 'P05-1020',\n",
       " 'P05-1047',\n",
       " 'W03-0430',\n",
       " 'P97-1003',\n",
       " 'P94-1013',\n",
       " 'J02-1003',\n",
       " 'P03-1069',\n",
       " 'P06-1010',\n",
       " 'J93-3003',\n",
       " 'P00-1041',\n",
       " 'C08-1018',\n",
       " 'P09-1113',\n",
       " 'P05-1065',\n",
       " 'P07-1106',\n",
       " 'P95-1026',\n",
       " 'C04-1072',\n",
       " 'J87-1005',\n",
       " 'C92-2066',\n",
       " 'C88-2128',\n",
       " 'J93-2004',\n",
       " 'P90-1032',\n",
       " 'N04-1035',\n",
       " 'N01-1026',\n",
       " 'D10-1115',\n",
       " 'D07-1077',\n",
       " 'P11-1098',\n",
       " 'P08-1085',\n",
       " 'P07-1055',\n",
       " 'N04-1042',\n",
       " 'J08-4004',\n",
       " 'C04-1059',\n",
       " 'N06-1056',\n",
       " 'P98-1029',\n",
       " 'C04-1046',\n",
       " 'W00-1201',\n",
       " 'P06-1032',\n",
       " 'J93-1004',\n",
       " 'W00-0712',\n",
       " 'P05-1044',\n",
       " 'D10-1124',\n",
       " 'P91-1030',\n",
       " 'P08-1064',\n",
       " 'P06-1038',\n",
       " 'J90-2002',\n",
       " 'P06-1084',\n",
       " 'W03-1006',\n",
       " 'P86-1031',\n",
       " 'C04-1180',\n",
       " 'P10-1044',\n",
       " 'W03-1809',\n",
       " 'C88-1016',\n",
       " 'C04-1200',\n",
       " 'E06-1005',\n",
       " 'E06-1025',\n",
       " 'J04-1005',\n",
       " 'P06-1101',\n",
       " 'P04-3022',\n",
       " 'C00-1007',\n",
       " 'P04-1083',\n",
       " 'P99-1016',\n",
       " 'D11-1062',\n",
       " 'P06-1123',\n",
       " 'D09-1086',\n",
       " 'H05-1021',\n",
       " 'P10-1040',\n",
       " 'N07-1071',\n",
       " 'N04-1025',\n",
       " 'C96-1005',\n",
       " 'P02-1051',\n",
       " 'A00-2024',\n",
       " 'H05-1053',\n",
       " 'P06-1121',\n",
       " 'P10-1052',\n",
       " 'P90-1010',\n",
       " 'P97-1017',\n",
       " 'P06-1109',\n",
       " 'D08-1021',\n",
       " 'N09-1041',\n",
       " 'J93-1005',\n",
       " 'P06-2066',\n",
       " 'P97-1013',\n",
       " 'J94-4004',\n",
       " 'P96-1041',\n",
       " 'A92-1006',\n",
       " 'W04-3111',\n",
       " 'D09-1127',\n",
       " 'W00-1303',\n",
       " 'N04-1041',\n",
       " 'N01-1016',\n",
       " 'J93-2002',\n",
       " 'N10-1019',\n",
       " 'C10-1011',\n",
       " 'P04-1056',\n",
       " 'C96-2141',\n",
       " 'J04-1002',\n",
       " 'P07-1030',\n",
       " 'J00-1004',\n",
       " 'J08-1001',\n",
       " 'P07-1003',\n",
       " 'D08-1036',\n",
       " 'P03-1058',\n",
       " 'N03-1030',\n",
       " 'P99-1004',\n",
       " 'P10-1142',\n",
       " 'C96-1058',\n",
       " 'W02-2016',\n",
       " 'P93-1003',\n",
       " 'W03-0424',\n",
       " 'J97-4005',\n",
       " 'E09-1013',\n",
       " 'C02-1150',\n",
       " 'P07-1092',\n",
       " 'E03-1071',\n",
       " 'C08-1114',\n",
       " 'C10-2005',\n",
       " 'P93-1002',\n",
       " 'W03-1008',\n",
       " 'N04-1023',\n",
       " 'P08-2026',\n",
       " 'P06-1134',\n",
       " 'J98-4003',\n",
       " 'P09-1011',\n",
       " 'P08-1068',\n",
       " 'P07-1098',\n",
       " 'C04-1081',\n",
       " 'N01-1006',\n",
       " 'P03-2041',\n",
       " 'W00-1401',\n",
       " 'W03-1011',\n",
       " 'P06-1104',\n",
       " 'N13-1039',\n",
       " 'N12-1067',\n",
       " 'P12-1092',\n",
       " 'C94-1079',\n",
       " 'N07-1038',\n",
       " 'N10-1020',\n",
       " 'J03-3001',\n",
       " 'J98-2002',\n",
       " 'P06-1095',\n",
       " 'D07-1114',\n",
       " 'D08-1068',\n",
       " 'W01-1313',\n",
       " 'N04-1022',\n",
       " 'D08-1089',\n",
       " 'P03-1029',\n",
       " 'P04-1015',\n",
       " 'P83-1007',\n",
       " 'P94-1019',\n",
       " 'P88-1012',\n",
       " 'P09-1019',\n",
       " 'W04-0803',\n",
       " 'P10-1110',\n",
       " 'P10-1146',\n",
       " 'P99-1008',\n",
       " 'N04-1030',\n",
       " 'A97-1004',\n",
       " 'P05-1017',\n",
       " 'P08-1119',\n",
       " 'J03-4003',\n",
       " 'D09-1005',\n",
       " 'P08-1109',\n",
       " 'N03-1020',\n",
       " 'E03-1009',\n",
       " 'P06-1015',\n",
       " 'N10-1115',\n",
       " 'J88-2006',\n",
       " 'P91-1017',\n",
       " 'C02-1144',\n",
       " 'N06-1041',\n",
       " 'N01-1025',\n",
       " 'P06-1011',\n",
       " 'A88-1019',\n",
       " 'W04-2406',\n",
       " 'N10-1063',\n",
       " 'L08-1093',\n",
       " 'D07-1091',\n",
       " 'D07-1003',\n",
       " 'P93-1022',\n",
       " 'D07-1097',\n",
       " 'P02-1018',\n",
       " 'P97-1023',\n",
       " 'W01-0513',\n",
       " 'N06-1003',\n",
       " 'D10-1120',\n",
       " 'D07-1080',\n",
       " 'P05-1015',\n",
       " 'N07-1051',\n",
       " 'P03-1051',\n",
       " 'P92-1032',\n",
       " 'W02-1028',\n",
       " 'P93-1032',\n",
       " 'H92-1045',\n",
       " 'E09-1005',\n",
       " 'D09-1026',\n",
       " 'P08-1090',\n",
       " 'E06-1043',\n",
       " 'P03-1010',\n",
       " 'H93-1052',\n",
       " 'J95-2002',\n",
       " 'W04-3205',\n",
       " 'D09-1120',\n",
       " 'E89-1009',\n",
       " 'P05-1012',\n",
       " 'N04-1001',\n",
       " 'D08-1020',\n",
       " 'A00-2004',\n",
       " 'P09-1026',\n",
       " 'P01-1017',\n",
       " 'P07-1125',\n",
       " 'P02-1031',\n",
       " 'W02-2026',\n",
       " 'N04-1033',\n",
       " 'P06-1072',\n",
       " 'D09-1098',\n",
       " 'J97-1005',\n",
       " 'P07-1123',\n",
       " 'C88-2121',\n",
       " 'J04-3002',\n",
       " 'H91-1060',\n",
       " 'P04-1035',\n",
       " 'N04-1019',\n",
       " 'P07-1096',\n",
       " 'D07-1031',\n",
       " 'H05-1011',\n",
       " 'D11-1129',\n",
       " 'P99-1042',\n",
       " 'C04-1073',\n",
       " 'C04-1146',\n",
       " 'P05-1073',\n",
       " 'P02-1050',\n",
       " 'P03-1002',\n",
       " 'W02-0109',\n",
       " 'N10-1013',\n",
       " 'P04-1014',\n",
       " 'C00-1072',\n",
       " 'P96-1042',\n",
       " 'P10-2041',\n",
       " 'P83-1019',\n",
       " 'P11-2008',\n",
       " 'J08-2002',\n",
       " 'A92-1018',\n",
       " 'C92-1019',\n",
       " 'H05-1045',\n",
       " 'P03-1004',\n",
       " 'N07-1047',\n",
       " 'J97-1003',\n",
       " 'J07-4004',\n",
       " 'P99-1014',\n",
       " 'P95-1050',\n",
       " 'A92-1021',\n",
       " 'W02-1210',\n",
       " 'W04-2407',\n",
       " 'A97-1029',\n",
       " 'C04-1041',\n",
       " 'J02-4002',\n",
       " 'J92-1001',\n",
       " 'P02-1022',\n",
       " 'P08-1114',\n",
       " 'P04-1054',\n",
       " 'E06-1002',\n",
       " 'D07-1071',\n",
       " 'J00-2004',\n",
       " 'W02-0817',\n",
       " 'P89-1031',\n",
       " 'J97-2003',\n",
       " 'D08-1011',\n",
       " 'P00-1056']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"./From-ScisummNet-2019\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0e9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = int(len(files)*0.7)\n",
    "val_end = int(len(files)*0.1)+train_end\n",
    "train_docs = files[0:train_end]\n",
    "val_docs = files[train_end:val_end]\n",
    "# test_docs = files[val_end:len(files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e713843c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f P83-1020\n",
      "f W02-1039\n",
      "f P96-1021\n",
      "f N06-1020\n",
      "f C86-1016\n",
      "f C92-2070\n",
      "f D08-1082\n",
      "f J01-2002\n",
      "f C02-1114\n",
      "f P85-1018\n",
      "f P07-1032\n",
      "f W00-1427\n",
      "f C04-1111\n",
      "f J97-1002\n",
      "f W04-3237\n",
      "f E87-1002\n",
      "All arrays must be of the same length\n",
      "f P93-1041\n",
      "f P90-1034\n",
      "f P98-1035\n",
      "f W02-0505\n",
      "f P13-1045\n",
      "f A00-2026\n",
      "f J03-4004\n",
      "f N03-1003\n",
      "f C90-2067\n",
      "f P84-1075\n",
      "f W02-0908\n",
      "f M95-1005\n",
      "f P07-1028\n",
      "f C00-2136\n",
      "f P01-1019\n",
      "f N10-1056\n",
      "f J81-4003\n",
      "f P93-1024\n",
      "f C90-3030\n",
      "f W02-2024\n",
      "f P98-1013\n",
      "f P03-1019\n",
      "f P07-1065\n",
      "f I05-3017\n",
      "f P07-1056\n",
      "f W03-0419\n",
      "f W00-0726\n",
      "f A00-2031\n",
      "f D07-1090\n",
      "f P02-1040\n",
      "f P05-1011\n",
      "f P02-1043\n",
      "f A00-2009\n",
      "f H94-1046\n",
      "f C00-1044\n",
      "f P11-1138\n",
      "f C02-1011\n",
      "f C96-1055\n",
      "f N06-2015\n",
      "f N03-1024\n",
      "f P08-1101\n",
      "f P96-1025\n",
      "f P02-1019\n",
      "f P09-1088\n",
      "f C94-1042\n",
      "f P99-1068\n",
      "f N07-1018\n",
      "f D07-1043\n",
      "f J93-1007\n",
      "f C08-1022\n",
      "f P01-1030\n",
      "f J01-2001\n",
      "f N01-1008\n",
      "f P05-3026\n",
      "f P96-1024\n",
      "f A00-2019\n",
      "f E06-1042\n",
      "f N06-1014\n",
      "f P03-1011\n",
      "f P11-1016\n",
      "f C92-2082\n",
      "f P02-1038\n",
      "f J01-4004\n",
      "f N09-1046\n",
      "f D11-1125\n",
      "f P94-1020\n",
      "f W04-2609\n",
      "f P09-1027\n",
      "f H05-1073\n",
      "f E06-1051\n",
      "f P00-1065\n",
      "f P04-1085\n",
      "f P02-1017\n",
      "f J91-1002\n",
      "f J00-3004\n",
      "f D09-1159\n",
      "f P08-1004\n",
      "f P02-1014\n",
      "f W04-3250\n",
      "f W02-1018\n",
      "f J00-4003\n",
      "f J03-3002\n",
      "f P00-1071\n",
      "f D07-1101\n",
      "f P84-1018\n",
      "f N09-1009\n",
      "f P02-1060\n",
      "f A97-1052\n",
      "f C00-2137\n",
      "f P05-1057\n",
      "f D08-1022\n",
      "f C92-3150\n",
      "f J92-1004\n",
      "f P06-1067\n",
      "f E06-1031\n",
      "f J04-4004\n",
      "f N06-1039\n",
      "f W02-2018\n",
      "f P07-1121\n",
      "f W04-0807\n",
      "f P87-1033\n",
      "f N03-2002\n",
      "f W02-1001\n",
      "f J05-1003\n",
      "f D08-1031\n",
      "f J93-2005\n",
      "f N01-1023\n",
      "f W03-0407\n",
      "f C04-1051\n",
      "f N04-1015\n",
      "All arrays must be of the same length\n",
      "f D10-1001\n",
      "f A00-1031\n",
      "f P08-2007\n",
      "f P09-1010\n",
      "f I08-1059\n",
      "f J00-4005\n",
      "f E99-1023\n",
      "f M95-1012\n",
      "f C94-2178\n",
      "f P08-1086\n",
      "f W04-3208\n",
      "f W03-1508\n",
      "f N06-1058\n",
      "f P01-1064\n",
      "f P88-1020\n",
      "f J02-2003\n",
      "f E06-1040\n",
      "f P07-1031\n",
      "f J07-3004\n",
      "f P02-1006\n",
      "f P02-1062\n",
      "f C94-2174\n",
      "f N06-1006\n",
      "f N09-1028\n",
      "f P90-1005\n",
      "f C04-1010\n",
      "f P03-1071\n",
      "f N04-1021\n",
      "f A94-1009\n",
      "f N01-1020\n",
      "f P08-1024\n",
      "f P09-1068\n",
      "f N07-1030\n",
      "f J86-3001\n",
      "f J03-1005\n",
      "f N04-3012\n",
      "f D12-1133\n",
      "f N12-1052\n",
      "f H05-1066\n",
      "f P07-1094\n",
      "f P08-1108\n",
      "f P04-1075\n",
      "f P93-1001\n",
      "f N04-1014\n",
      "f P11-1020\n",
      "f W04-3219\n",
      "f J93-1003\n",
      "f S10-1011\n",
      "f P96-1006\n",
      "f J93-2006\n",
      "f P95-1007\n",
      "f J94-2003\n",
      "f D07-1002\n",
      "f P09-1042\n",
      "f E99-1010\n",
      "f W03-1730\n",
      "f D09-1001\n",
      "f J94-4003\n",
      "f N03-1021\n",
      "f A94-1016\n",
      "f P93-1020\n",
      "f J02-1002\n",
      "f H94-1048\n",
      "f W03-0301\n",
      "f W04-0811\n",
      "f J94-4002\n",
      "f P97-1009\n",
      "f N04-4038\n",
      "f P04-1061\n",
      "f J03-3005\n",
      "f J91-1003\n",
      "f J90-1003\n",
      "f P99-1059\n",
      "f P00-1037\n",
      "f W04-3103\n",
      "f D10-1048\n",
      "f P08-1084\n",
      "f J96-1002\n",
      "f W01-0501\n",
      "f H05-1010\n",
      "f W03-0405\n",
      "f H92-1026\n",
      "f P04-1018\n",
      "f J97-3002\n",
      "f P01-1008\n",
      "f J96-1001\n",
      "f N03-1017\n",
      "f C04-1080\n",
      "f P07-1004\n",
      "f P05-1036\n",
      "f P91-1027\n",
      "f P03-1056\n",
      "f P93-1008\n",
      "f W03-1028\n",
      "f P04-1041\n",
      "f P97-1005\n",
      "f P06-1124\n",
      "f J98-2004\n",
      "f J92-4007\n",
      "f P92-1005\n",
      "f C96-2183\n",
      "f P94-1012\n",
      "f W00-1308\n",
      "f J98-4004\n",
      "f W03-1812\n",
      "f P09-1077\n",
      "f P91-1034\n",
      "f J87-1004\n",
      "f N04-1013\n",
      "f P02-1039\n",
      "f W02-0902\n",
      "f N09-1036\n",
      "f W03-0501\n",
      "f D08-1092\n",
      "f P98-1012\n",
      "f P07-1005\n",
      "f H05-1004\n",
      "All arrays must be of the same length\n",
      "f J92-4003\n",
      "f D08-1035\n",
      "f D07-1072\n",
      "f D07-1104\n",
      "f P91-1023\n",
      "f N04-4015\n",
      "f P02-1033\n",
      "f P98-2182\n",
      "f P89-1010\n",
      "f P03-1023\n",
      "f P03-1054\n",
      "f J03-1003\n",
      "f W04-2705\n",
      "f N03-1016\n",
      "f P02-1035\n",
      "f P07-1049\n",
      "f P03-1012\n",
      "f N06-1025\n",
      "f I05-2038\n",
      "f J04-2003\n",
      "f P08-1066\n",
      "f P05-1034\n",
      "f W04-3206\n",
      "f P97-1035\n",
      "f P06-1043\n",
      "f P07-1037\n",
      "f N06-1011\n",
      "f S10-1010\n",
      "f N12-1047\n",
      "f P07-1019\n",
      "f P06-2005\n",
      "f P03-1035\n",
      "f N09-2004\n",
      "f J06-1003\n",
      "f W03-1728\n",
      "f P97-1063\n",
      "f P06-1114\n",
      "f P07-1059\n",
      "f P07-1034\n",
      "f H93-1051\n",
      "f N01-1021\n",
      "f N03-1014\n",
      "f P99-1041\n",
      "f W03-1017\n",
      "f P08-2012\n",
      "f P95-1037\n",
      "f C10-2028\n",
      "f P03-1003\n",
      "f J99-2004\n",
      "f P05-1001\n",
      "f N09-1037\n",
      "f P87-1022\n",
      "f P99-1032\n",
      "f P89-1009\n",
      "f D08-1016\n",
      "f H91-1026\n",
      "f P92-1008\n",
      "f C04-1197\n",
      "f H05-1012\n",
      "f W00-0403\n",
      "f D07-1109\n",
      "f P05-1074\n",
      "f J93-1001\n",
      "f N07-1023\n",
      "f D07-1007\n",
      "f P01-1005\n",
      "f P96-1038\n",
      "f P04-1021\n",
      "f P95-1021\n",
      "f P93-1005\n",
      "f W00-0717\n",
      "f C94-1027\n",
      "f J93-1002\n",
      "f H05-1043\n",
      "f P98-1034\n",
      "f N04-1016\n",
      "f N06-1033\n",
      "f P04-1013\n",
      "f P01-1025\n",
      "f C90-3063\n",
      "f N01-1024\n",
      "f W04-2319\n",
      "f P03-2026\n",
      "f P07-1073\n",
      "f E06-1032\n",
      "f P03-1013\n",
      "f E06-1015\n",
      "f W03-0428\n",
      "f J99-1003\n",
      "f P05-1072\n",
      "f C86-1045\n",
      "f P06-2014\n",
      "f J01-3003\n",
      "f W04-1013\n",
      "f J05-1004\n",
      "f W03-1810\n",
      "f P09-2004\n",
      "f P96-1027\n",
      "f P00-1016\n",
      "f W04-3239\n",
      "f H05-1091\n",
      "f I05-3027\n",
      "f C94-2195\n",
      "f S12-1053\n",
      "f P11-2031\n",
      "f P08-1030\n",
      "f J02-3001\n",
      "f C90-3044\n",
      "f P06-1005\n",
      "f P06-1055\n",
      "f D11-1014\n",
      "f P05-1010\n",
      "f E03-1008\n",
      "f W04-3236\n",
      "f P10-1001\n",
      "f P03-1009\n",
      "f P99-1069\n",
      "f P02-1034\n",
      "f P05-1018\n",
      "f J80-3003\n",
      "f P05-1033\n",
      "f P06-1097\n",
      "f W04-3213\n",
      "f P99-1071\n",
      "f P08-1036\n",
      "f P05-1045\n",
      "f P99-1067\n",
      "f N10-1061\n",
      "f P07-1107\n",
      "f D08-1065\n",
      "f P98-1010\n",
      "f P06-1103\n",
      "f P05-2008\n",
      "f P05-1066\n",
      "f P02-1046\n",
      "f J10-3003\n",
      "f W03-0425\n",
      "f W02-1503\n",
      "f W04-3201\n",
      "f P98-2204\n",
      "f D08-1083\n",
      "f W01-0521\n",
      "f P06-1091\n",
      "f P88-1015\n",
      "f D09-1101\n",
      "f J98-1001\n",
      "f J90-1004\n",
      "f J10-4006\n",
      "f P02-1001\n",
      "f P00-1058\n",
      "f H93-1061\n",
      "f H05-1044\n",
      "f P05-1022\n",
      "f P08-1076\n",
      "f P94-1002\n",
      "f D11-1006\n",
      "f A00-2034\n",
      "f J08-1002\n",
      "f P08-1115\n",
      "f P09-1104\n",
      "f C02-1139\n",
      "f P98-1112\n",
      "f N06-2013\n",
      "f D08-1059\n",
      "f J97-3003\n",
      "f P09-1039\n",
      "f J09-3003\n",
      "f P06-1066\n",
      "f J82-3004\n",
      "f N03-1022\n",
      "f P08-1012\n",
      "f J98-3005\n",
      "f W04-3207\n",
      "f E03-1076\n",
      "f W04-2401\n",
      "f P00-1010\n",
      "f C02-2025\n",
      "f P98-2177\n",
      "f A97-1039\n",
      "f D07-1074\n",
      "f D07-1111\n",
      "f N03-1026\n",
      "f P09-1057\n",
      "f J08-4003\n",
      "f P07-1007\n",
      "f D07-1076\n",
      "f P00-1027\n",
      "f P06-1009\n",
      "f P85-1011\n",
      "f C94-1032\n",
      "f D07-1103\n",
      "f D11-1141\n",
      "f D11-1142\n",
      "f P11-1019\n",
      "f I05-3025\n",
      "f P05-1067\n",
      "f P85-1008\n",
      "f P05-1059\n",
      "f A00-1043\n",
      "f D08-1027\n",
      "f D12-1050\n",
      "f J94-2001\n",
      "f P06-1014\n",
      "f C96-1021\n",
      "f J98-2001\n",
      "f W04-3247\n",
      "f W03-1719\n",
      "f C92-3126\n",
      "f W02-1006\n",
      "f J95-4004\n",
      "f A94-1006\n",
      "f W04-1221\n",
      "f C08-1107\n",
      "f J93-2003\n",
      "f W01-0511\n",
      "f P03-1022\n",
      "f P96-1008\n",
      "f D09-1058\n",
      "f P04-1005\n",
      "f P95-1034\n",
      "f J01-3001\n",
      "f P83-1021\n",
      "f P98-1069\n",
      "f J06-3003\n",
      "f D08-1014\n",
      "f W01-1605\n",
      "f H01-1035\n",
      "f J99-4004\n",
      "f J94-4001\n",
      "f H05-1059\n",
      "f A97-1030\n",
      "f J94-3001\n",
      "f J04-4002\n",
      "f E06-1038\n",
      "f P06-4020\n",
      "f P11-1055\n",
      "f P08-1088\n",
      "f E99-1001\n",
      "f P07-2045\n",
      "f P86-1004\n",
      "f H05-1079\n",
      "f C10-1152\n",
      "f J99-3001\n",
      "f J99-4005\n",
      "f P97-1041\n",
      "f N06-2033\n",
      "f P98-2173\n",
      "f P84-1008\n",
      "f P09-1116\n",
      "f P93-1023\n",
      "f P02-1053\n",
      "f W04-3230\n",
      "f W02-0603\n",
      "f P03-1021\n",
      "f N07-1011\n",
      "f J05-3002\n",
      "f D10-1119\n",
      "f E06-1011\n",
      "f J08-2005\n",
      "f D07-1061\n",
      "f P09-1040\n",
      "f C08-1109\n",
      "f N09-1003\n",
      "f W00-0730\n",
      "f P05-1071\n",
      "f H94-1020\n",
      "f C90-3045\n",
      "f P06-1115\n",
      "f P09-2012\n",
      "f P03-1044\n",
      "f J05-4003\n",
      "f C02-1054\n",
      "f C04-1100\n",
      "f C02-1145\n",
      "All arrays must be of the same length\n",
      "f C04-1024\n",
      "f P93-1035\n",
      "f P06-1085\n",
      "f P06-2006\n",
      "f N07-4013\n",
      "f P07-1091\n",
      "f P98-2127\n",
      "f P96-1011\n",
      "f P06-1004\n",
      "f N10-1119\n",
      "f P11-1038\n",
      "f P03-1001\n",
      "f E06-1027\n",
      "f D08-1024\n",
      "f J03-1002\n",
      "f C92-1038\n",
      "f N09-1012\n",
      "f N03-2021\n",
      "f J88-1003\n",
      "f W03-1014\n",
      "f C96-1079\n",
      "f P08-1023\n",
      "f J95-2003\n",
      "f P92-1017\n",
      "f W02-1021\n",
      "f E89-1037\n",
      "f P98-1106\n",
      "f J93-1006\n",
      "f N07-1029\n",
      "f P93-1016\n",
      "f P06-1077\n",
      "f W02-1502\n",
      "f W02-0301\n",
      "f P09-1058\n",
      "f P10-4002\n",
      "f H05-2018\n",
      "f P01-1067\n",
      "f P04-1043\n",
      "f W04-0308\n",
      "f N04-4026\n",
      "f N13-1090\n",
      "f P99-1048\n",
      "f N03-1033\n",
      "f P09-1074\n",
      "f P04-1053\n",
      "f W02-1011\n",
      "f D11-1033\n",
      "f P98-2180\n",
      "f P06-2094\n",
      "f J07-2003\n",
      "f J91-4003\n",
      "f W04-3212\n",
      "f D08-1076\n",
      "f J88-2003\n",
      "f P99-1065\n",
      "f P05-1077\n",
      "f P02-1047\n",
      "f A97-1011\n",
      "f D09-1030\n",
      "f W01-0514\n",
      "f D07-1096\n",
      "f P06-3002\n",
      "f W03-0404\n",
      "f P04-1066\n",
      "f C90-3052\n",
      "f D10-1125\n",
      "f P02-1042\n",
      "f P06-2101\n",
      "f N03-1028\n",
      "f C00-2163\n",
      "f J96-2004\n",
      "f P07-1036\n",
      "f P04-1077\n",
      "f C92-1025\n",
      "f P08-1067\n",
      "f P11-2033\n",
      "f J98-1006\n",
      "f P09-1094\n",
      "f P84-1085\n",
      "All arrays must be of the same length\n",
      "f N04-1043\n",
      "f P05-1052\n",
      "f P91-1022\n",
      "f P89-1002\n",
      "f C88-2147\n",
      "f P05-1020\n",
      "f P05-1047\n",
      "f W03-0430\n",
      "f P97-1003\n",
      "f P94-1013\n",
      "f J02-1003\n",
      "f P03-1069\n",
      "f P06-1010\n",
      "f J93-3003\n",
      "f P00-1041\n",
      "f C08-1018\n",
      "f P09-1113\n",
      "f P05-1065\n",
      "f P07-1106\n",
      "f P95-1026\n",
      "f C04-1072\n",
      "f J87-1005\n",
      "f C92-2066\n",
      "All arrays must be of the same length\n",
      "f C88-2128\n",
      "f J93-2004\n",
      "f P90-1032\n",
      "f N04-1035\n",
      "f N01-1026\n",
      "f D10-1115\n",
      "f D07-1077\n",
      "f P11-1098\n",
      "f P08-1085\n",
      "f P07-1055\n",
      "f N04-1042\n",
      "f J08-4004\n",
      "f C04-1059\n",
      "f N06-1056\n",
      "f P98-1029\n",
      "f C04-1046\n",
      "f W00-1201\n",
      "f P06-1032\n",
      "f J93-1004\n",
      "f W00-0712\n",
      "f P05-1044\n",
      "f D10-1124\n",
      "f P91-1030\n",
      "f P08-1064\n",
      "f P06-1038\n",
      "f J90-2002\n",
      "f P06-1084\n",
      "f W03-1006\n",
      "f P86-1031\n",
      "f C04-1180\n",
      "All arrays must be of the same length\n",
      "f P10-1044\n",
      "f W03-1809\n",
      "f C88-1016\n",
      "f C04-1200\n",
      "f E06-1005\n",
      "f E06-1025\n",
      "f J04-1005\n",
      "f P06-1101\n",
      "f P04-3022\n",
      "f C00-1007\n",
      "f P04-1083\n",
      "f P99-1016\n",
      "f D11-1062\n",
      "f P06-1123\n",
      "f D09-1086\n",
      "f H05-1021\n",
      "f P10-1040\n",
      "f N07-1071\n",
      "f N04-1025\n",
      "f C96-1005\n",
      "f P02-1051\n",
      "f A00-2024\n",
      "f H05-1053\n",
      "f P06-1121\n",
      "f P10-1052\n",
      "f P90-1010\n",
      "f P97-1017\n",
      "f P06-1109\n",
      "f D08-1021\n",
      "f N09-1041\n",
      "f J93-1005\n",
      "f P06-2066\n",
      "f P97-1013\n",
      "f J94-4004\n",
      "f P96-1041\n",
      "f A92-1006\n",
      "f W04-3111\n",
      "f D09-1127\n",
      "f W00-1303\n",
      "f N04-1041\n",
      "f N01-1016\n",
      "f J93-2002\n",
      "f N10-1019\n",
      "f C10-1011\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example_sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w.isalpha()]\n",
    "#     print(filtered_sentence)\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n",
    "\n",
    "    \n",
    "def get_dataset(files):\n",
    "    data_lis = []\n",
    "    label_lis = []\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    new_corpus_prev = None\n",
    "    for z,f in enumerate(files):\n",
    "        print(\"f\",f)\n",
    "        try:\n",
    "            citants = pd.read_json(\"From-ScisummNet-2019/\"+str(f)+\"/citing_sentences.json\")\n",
    "            citants = citants[['citance_No','clean_text']]\n",
    "            queries = list(citants['clean_text'])\n",
    "            cite_no = list(citants.citance_No)\n",
    "            tree = ET.parse(\"From-ScisummNet-2019/\"+f+\"/Reference_XML/\"+f+\".xml\")\n",
    "            root = tree.getroot()\n",
    "            final1=[]\n",
    "            final2=[]\n",
    "            i = 0\n",
    "            total = len(root)\n",
    "            for a in root:\n",
    "                for b in a:\n",
    "                    final1.append(b.text)\n",
    "                    if i == 0:\n",
    "                        final2.append(\"Abstract\")\n",
    "                    if i == 1:\n",
    "                        final2.append(\"Introduction\")\n",
    "                    elif i < total-2:\n",
    "                        final2.append(\"Experiment/Discussion\")\n",
    "                    if i == total-2 or i == total-1:\n",
    "                        final2.append(\"Results/Conclusion\")\n",
    "                    if i == total:\n",
    "                        final2.append(\"Acknowledgment\")\n",
    "                i = i+1\n",
    "\n",
    "            d={'col1':final1,'col2':final2}\n",
    "\n",
    "            rp = pd.DataFrame(data=d)\n",
    "            data_lis.extend(list(rp.col1))\n",
    "            label_lis.extend(list(rp.col2))\n",
    "            \n",
    "        except Exception as e: print(e)\n",
    "    return data_lis, label_lis\n",
    "            \n",
    "    \n",
    "train_data, train_labels = get_dataset(train_docs)\n",
    "val_data,val_labels = get_dataset(val_docs)\n",
    "# test_data = get_dataset(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb89272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "val_labels = le.transform(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a542f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2269c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_data,return_tensors=\"pt\",max_length=512,padding='max_length')\n",
    "val_encodings = tokenizer(val_data,return_tensors=\"pt\",max_length=512,padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9145096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# train_encodings = tokenizer(train_data,return_tensors=\"pt\",padding=True)\n",
    "# val_encodings = tokenizer(val_data,return_tensors=\"pt\",padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9deddec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels))\n",
    "val_dataset = IMDbDataset(val_encodings, torch.from_numpy(val_labels))\n",
    "# test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72209ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_labels), max(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd400ea8",
   "metadata": {},
   "source": [
    "## Multi-class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b120d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmp9wvt3upx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7829bdab064e9cafdc26c16c9dc8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 794\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2382\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2382' max='2382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2382/2382 06:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2382, training_loss=1.2230342989904754, metrics={'train_runtime': 360.9461, 'train_samples_per_second': 6.599, 'train_steps_per_second': 6.599, 'total_flos': 626736161028096.0, 'train_loss': 1.2230342989904754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1500,\n",
    ")\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "537c58d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 1\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='832' max='832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [832/832 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4331277012825012,\n",
       " 'eval_runtime': 22.2682,\n",
       " 'eval_samples_per_second': 37.363,\n",
       " 'eval_steps_per_second': 37.363,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd4e2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7372f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"task1b_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "344a1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# acc = load_metric(\"accuracy\")\n",
    "\n",
    "# f1_score = load_metric(\"f1\")\n",
    "model.eval()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "pred_lis = []\n",
    "gt_lis = []\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    pred_lis.extend(predictions.cpu().numpy())\n",
    "    gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "    \n",
    "\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eaf1c40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14664,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3967001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4d8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(pred_lis, gt_lis):\n",
    "    acc = accuracy_score(pred_lis, gt_lis)\n",
    "    recall = recall_score(pred_lis, gt_lis,average='micro')\n",
    "    prec = precision_score(pred_lis, gt_lis,average='micro')\n",
    "    f1 = f1_score(pred_lis, gt_lis,average='micro')\n",
    "    print(\"metrics obtained in test: accuracy {} recall {}, precision {}, f1-score {}\".format(acc,recall,prec,f1))\n",
    "    \n",
    "def get_integer_mapping(le):\n",
    "    '''\n",
    "    Return a dict mapping labels to their integer values\n",
    "    from an SKlearn LabelEncoder\n",
    "    le = a fitted SKlearn LabelEncoder\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({le.transform([cl])[0]:cl})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ad6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(pred_lis, gt_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec16e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lis = np.array(gt_lis)\n",
    "pred_lis = np.array(pred_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17a921b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Experiment/Discussion', 1: 'Introduction', 2: 'Results/Conclusion'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mapping = get_integer_mapping(le)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1af839e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for class:  Experiment/Discussion\n",
      "metrics obtained in test: accuracy 1.0 recall 1.0, precision 1.0, f1-score 1.0\n",
      "for class:  Introduction\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n",
      "for class:  Results/Conclusion\n",
      "metrics obtained in test: accuracy 0.0 recall 0.0, precision 0.0, f1-score 0.0\n"
     ]
    }
   ],
   "source": [
    "#class-wise metrics\n",
    "for cl in mapping:\n",
    "    print(\"for class: \",mapping[cl])\n",
    "    ind = np.where(gt_lis==cl)\n",
    "    small_pred = pred_lis[ind]\n",
    "#     print(small_pred, gt_lis[ind])\n",
    "    calc_metrics(small_pred, gt_lis[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13aac74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a07f8",
   "metadata": {},
   "source": [
    "This is due to class-imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c0afd",
   "metadata": {},
   "source": [
    "## Classifiers Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e80da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_distribution(labels, typ = \"dataset\"):\n",
    "    lb = list(Counter(labels).keys()) # equals to list(set(words))\n",
    "    count = list(Counter(labels).values()) # counts the elements' frequency\n",
    "    lb_map = []\n",
    "    for l in lb:\n",
    "        lb_map.append(mapping[l])\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Creating plot\n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    plt.pie(count, labels = lb_map)\n",
    "\n",
    "    # show plot\n",
    "    plt.title(\"distribution in \"+str(typ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28455d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_distribution(np.concatenate((train_labels,val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085d0a5",
   "metadata": {},
   "source": [
    "## Classifiers for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a980cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bca6216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for class:  Experiment/Discussion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmpb9l234l3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07a715f2c374d8b8fe783fdc09b950b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmp87h3at1o\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d5a40c6b4f47658c6471f6fd71fcf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 91222\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 273666\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='682' max='273666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   682/273666 01:08 < 7:37:40, 9.94 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35913/3711926773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/new/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m                 ):\n\u001b[1;32m   1323\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='/ssd_scratch/cvit/logs',            # directory for storing logs\n",
    "    logging_steps=1500,\n",
    ")\n",
    "\n",
    "\n",
    "for cl in mapping:\n",
    "    print(\"for class: \",mapping[cl])\n",
    "    train_ind = np.where(train_labels==cl)[0].astype(int)\n",
    "    val_ind = np.where(val_labels==cl)[0].astype(int)\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    val_data = np.array(val_data)\n",
    "    \n",
    "    train_encodings = tokenizer(list(train_data[train_ind]),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    val_encodings = tokenizer(list(val_data[val_ind]),return_tensors=\"pt\",max_length=512,padding='max_length',truncation=True)\n",
    "    train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels[train_ind]))\n",
    "    val_dataset = IMDbDataset(val_encodings, torch.from_numpy(val_labels[val_ind]))    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    \n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset             # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    # eval\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    pred_lis = []\n",
    "    gt_lis = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        pred_lis.extend(predictions.cpu().numpy())\n",
    "        gt_lis.extend(batch[\"labels\"].cpu().numpy())\n",
    "    \n",
    "    calc_metrics(pred_lis, gt_lis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07123f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
