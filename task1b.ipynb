{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d64406aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P83-1020',\n",
       " 'W02-1039',\n",
       " 'P96-1021',\n",
       " 'N06-1020',\n",
       " 'C86-1016',\n",
       " 'C92-2070',\n",
       " 'D08-1082',\n",
       " 'J01-2002',\n",
       " 'C02-1114',\n",
       " 'P85-1018',\n",
       " 'P07-1032',\n",
       " 'W00-1427',\n",
       " 'C04-1111',\n",
       " 'J97-1002',\n",
       " 'W04-3237',\n",
       " 'E87-1002',\n",
       " 'P93-1041',\n",
       " 'P90-1034',\n",
       " 'P98-1035',\n",
       " 'W02-0505',\n",
       " 'P13-1045',\n",
       " 'A00-2026',\n",
       " 'J03-4004',\n",
       " 'N03-1003',\n",
       " 'C90-2067',\n",
       " 'P84-1075',\n",
       " 'W02-0908',\n",
       " 'M95-1005',\n",
       " 'P07-1028',\n",
       " 'C00-2136',\n",
       " 'P01-1019',\n",
       " 'N10-1056',\n",
       " 'J81-4003',\n",
       " 'P93-1024',\n",
       " 'C90-3030',\n",
       " 'W02-2024',\n",
       " 'P98-1013',\n",
       " 'P03-1019',\n",
       " 'P07-1065',\n",
       " 'I05-3017',\n",
       " 'P07-1056',\n",
       " 'W03-0419',\n",
       " 'W00-0726',\n",
       " 'A00-2031',\n",
       " 'D07-1090',\n",
       " 'P02-1040',\n",
       " 'P05-1011',\n",
       " 'P02-1043',\n",
       " 'A00-2009',\n",
       " 'H94-1046',\n",
       " 'C00-1044',\n",
       " 'P11-1138',\n",
       " 'C02-1011',\n",
       " 'C96-1055',\n",
       " 'N06-2015',\n",
       " 'N03-1024',\n",
       " 'P08-1101',\n",
       " 'P96-1025',\n",
       " 'P02-1019',\n",
       " 'P09-1088',\n",
       " 'C94-1042',\n",
       " 'P99-1068',\n",
       " 'N07-1018',\n",
       " 'D07-1043',\n",
       " 'J93-1007',\n",
       " 'C08-1022',\n",
       " 'P01-1030',\n",
       " 'J01-2001',\n",
       " 'N01-1008',\n",
       " 'P05-3026',\n",
       " 'P96-1024',\n",
       " 'A00-2019',\n",
       " 'E06-1042',\n",
       " 'N06-1014',\n",
       " 'P03-1011',\n",
       " 'P11-1016',\n",
       " 'C92-2082',\n",
       " 'P02-1038',\n",
       " 'J01-4004',\n",
       " 'N09-1046',\n",
       " 'D11-1125',\n",
       " 'P94-1020',\n",
       " 'W04-2609',\n",
       " 'P09-1027',\n",
       " 'H05-1073',\n",
       " 'E06-1051',\n",
       " 'P00-1065',\n",
       " 'P04-1085',\n",
       " 'P02-1017',\n",
       " 'J91-1002',\n",
       " 'J00-3004',\n",
       " 'D09-1159',\n",
       " 'P08-1004',\n",
       " 'P02-1014',\n",
       " 'W04-3250',\n",
       " 'W02-1018',\n",
       " 'J00-4003',\n",
       " 'J03-3002',\n",
       " 'P00-1071',\n",
       " 'D07-1101',\n",
       " 'P84-1018',\n",
       " 'N09-1009',\n",
       " 'P02-1060',\n",
       " 'A97-1052',\n",
       " 'C00-2137',\n",
       " 'P05-1057',\n",
       " 'D08-1022',\n",
       " 'C92-3150',\n",
       " 'J92-1004',\n",
       " 'P06-1067',\n",
       " 'E06-1031',\n",
       " 'J04-4004',\n",
       " 'N06-1039',\n",
       " 'W02-2018',\n",
       " 'P07-1121',\n",
       " 'W04-0807',\n",
       " 'P87-1033',\n",
       " 'N03-2002',\n",
       " 'W02-1001',\n",
       " 'J05-1003',\n",
       " 'D08-1031',\n",
       " 'J93-2005',\n",
       " 'N01-1023',\n",
       " 'W03-0407',\n",
       " 'C04-1051',\n",
       " 'N04-1015',\n",
       " 'D10-1001',\n",
       " 'A00-1031',\n",
       " 'P08-2007',\n",
       " 'P09-1010',\n",
       " 'I08-1059',\n",
       " 'J00-4005',\n",
       " 'E99-1023',\n",
       " 'M95-1012',\n",
       " 'C94-2178',\n",
       " 'P08-1086',\n",
       " 'W04-3208',\n",
       " 'W03-1508',\n",
       " 'N06-1058',\n",
       " 'P01-1064',\n",
       " 'P88-1020',\n",
       " 'J02-2003',\n",
       " 'E06-1040',\n",
       " 'P07-1031',\n",
       " 'J07-3004',\n",
       " 'P02-1006',\n",
       " 'P02-1062',\n",
       " 'C94-2174',\n",
       " 'N06-1006',\n",
       " 'N09-1028',\n",
       " 'P90-1005',\n",
       " 'C04-1010',\n",
       " 'P03-1071',\n",
       " 'N04-1021',\n",
       " 'A94-1009',\n",
       " 'N01-1020',\n",
       " 'P08-1024',\n",
       " 'P09-1068',\n",
       " 'N07-1030',\n",
       " 'J86-3001',\n",
       " 'J03-1005',\n",
       " 'N04-3012',\n",
       " 'D12-1133',\n",
       " 'N12-1052',\n",
       " 'H05-1066',\n",
       " 'P07-1094',\n",
       " 'P08-1108',\n",
       " 'P04-1075',\n",
       " 'P93-1001',\n",
       " 'N04-1014',\n",
       " 'P11-1020',\n",
       " 'W04-3219',\n",
       " 'J93-1003',\n",
       " 'S10-1011',\n",
       " 'P96-1006',\n",
       " 'J93-2006',\n",
       " 'P95-1007',\n",
       " 'J94-2003',\n",
       " 'D07-1002',\n",
       " 'P09-1042',\n",
       " 'E99-1010',\n",
       " 'W03-1730',\n",
       " 'D09-1001',\n",
       " 'J94-4003',\n",
       " 'N03-1021',\n",
       " 'A94-1016',\n",
       " 'P93-1020',\n",
       " 'J02-1002',\n",
       " 'H94-1048',\n",
       " 'W03-0301',\n",
       " 'W04-0811',\n",
       " 'J94-4002',\n",
       " 'P97-1009',\n",
       " 'N04-4038',\n",
       " 'P04-1061',\n",
       " 'J03-3005',\n",
       " 'J91-1003',\n",
       " 'J90-1003',\n",
       " 'P99-1059',\n",
       " 'P00-1037',\n",
       " 'W04-3103',\n",
       " 'D10-1048',\n",
       " 'P08-1084',\n",
       " 'J96-1002',\n",
       " 'W01-0501',\n",
       " 'H05-1010',\n",
       " 'W03-0405',\n",
       " 'H92-1026',\n",
       " 'P04-1018',\n",
       " 'J97-3002',\n",
       " 'P01-1008',\n",
       " 'J96-1001',\n",
       " 'N03-1017',\n",
       " 'C04-1080',\n",
       " 'P07-1004',\n",
       " 'P05-1036',\n",
       " 'P91-1027',\n",
       " 'P03-1056',\n",
       " 'P93-1008',\n",
       " 'W03-1028',\n",
       " 'P04-1041',\n",
       " 'P97-1005',\n",
       " 'P06-1124',\n",
       " 'J98-2004',\n",
       " 'J92-4007',\n",
       " 'P92-1005',\n",
       " 'C96-2183',\n",
       " 'P94-1012',\n",
       " 'W00-1308',\n",
       " 'J98-4004',\n",
       " 'W03-1812',\n",
       " 'P09-1077',\n",
       " 'P91-1034',\n",
       " 'J87-1004',\n",
       " 'N04-1013',\n",
       " 'P02-1039',\n",
       " 'W02-0902',\n",
       " 'N09-1036',\n",
       " 'W03-0501',\n",
       " 'D08-1092',\n",
       " 'P98-1012',\n",
       " 'P07-1005',\n",
       " 'H05-1004',\n",
       " 'J92-4003',\n",
       " 'D08-1035',\n",
       " 'D07-1072',\n",
       " 'D07-1104',\n",
       " 'P91-1023',\n",
       " 'N04-4015',\n",
       " 'P02-1033',\n",
       " 'P98-2182',\n",
       " 'P89-1010',\n",
       " 'P03-1023',\n",
       " 'P03-1054',\n",
       " 'J03-1003',\n",
       " 'W04-2705',\n",
       " 'N03-1016',\n",
       " 'P02-1035',\n",
       " 'P07-1049',\n",
       " 'P03-1012',\n",
       " 'N06-1025',\n",
       " 'I05-2038',\n",
       " 'J04-2003',\n",
       " 'P08-1066',\n",
       " 'P05-1034',\n",
       " 'W04-3206',\n",
       " 'P97-1035',\n",
       " 'P06-1043',\n",
       " 'P07-1037',\n",
       " 'N06-1011',\n",
       " 'S10-1010',\n",
       " 'N12-1047',\n",
       " 'P07-1019',\n",
       " 'P06-2005',\n",
       " 'P03-1035',\n",
       " 'N09-2004',\n",
       " 'J06-1003',\n",
       " 'W03-1728',\n",
       " 'P97-1063',\n",
       " 'P06-1114',\n",
       " 'P07-1059',\n",
       " 'P07-1034',\n",
       " 'H93-1051',\n",
       " 'N01-1021',\n",
       " 'N03-1014',\n",
       " 'P99-1041',\n",
       " 'W03-1017',\n",
       " 'P08-2012',\n",
       " 'P95-1037',\n",
       " 'C10-2028',\n",
       " 'P03-1003',\n",
       " 'J99-2004',\n",
       " 'P05-1001',\n",
       " 'N09-1037',\n",
       " 'P87-1022',\n",
       " 'P99-1032',\n",
       " 'P89-1009',\n",
       " 'D08-1016',\n",
       " 'H91-1026',\n",
       " 'P92-1008',\n",
       " 'C04-1197',\n",
       " 'H05-1012',\n",
       " 'W00-0403',\n",
       " 'D07-1109',\n",
       " 'P05-1074',\n",
       " 'J93-1001',\n",
       " 'N07-1023',\n",
       " 'D07-1007',\n",
       " 'P01-1005',\n",
       " 'P96-1038',\n",
       " 'P04-1021',\n",
       " 'P95-1021',\n",
       " 'P93-1005',\n",
       " 'W00-0717',\n",
       " 'C94-1027',\n",
       " 'J93-1002',\n",
       " 'H05-1043',\n",
       " 'P98-1034',\n",
       " 'N04-1016',\n",
       " 'N06-1033',\n",
       " 'P04-1013',\n",
       " 'P01-1025',\n",
       " 'C90-3063',\n",
       " 'N01-1024',\n",
       " 'W04-2319',\n",
       " 'P03-2026',\n",
       " 'P07-1073',\n",
       " 'E06-1032',\n",
       " 'P03-1013',\n",
       " 'E06-1015',\n",
       " 'W03-0428',\n",
       " 'J99-1003',\n",
       " 'P05-1072',\n",
       " 'C86-1045',\n",
       " 'P06-2014',\n",
       " 'J01-3003',\n",
       " 'W04-1013',\n",
       " 'J05-1004',\n",
       " 'W03-1810',\n",
       " 'P09-2004',\n",
       " 'P96-1027',\n",
       " 'P00-1016',\n",
       " 'W04-3239',\n",
       " 'H05-1091',\n",
       " 'I05-3027',\n",
       " 'C94-2195',\n",
       " 'S12-1053',\n",
       " 'P11-2031',\n",
       " 'P08-1030',\n",
       " 'J02-3001',\n",
       " 'C90-3044',\n",
       " 'P06-1005',\n",
       " 'P06-1055',\n",
       " 'D11-1014',\n",
       " 'P05-1010',\n",
       " 'E03-1008',\n",
       " 'W04-3236',\n",
       " 'P10-1001',\n",
       " 'P03-1009',\n",
       " 'P99-1069',\n",
       " 'P02-1034',\n",
       " 'P05-1018',\n",
       " 'J80-3003',\n",
       " 'P05-1033',\n",
       " 'P06-1097',\n",
       " 'W04-3213',\n",
       " 'P99-1071',\n",
       " 'P08-1036',\n",
       " 'P05-1045',\n",
       " 'P99-1067',\n",
       " 'N10-1061',\n",
       " 'P07-1107',\n",
       " 'D08-1065',\n",
       " 'P98-1010',\n",
       " 'P06-1103',\n",
       " 'P05-2008',\n",
       " 'P05-1066',\n",
       " 'P02-1046',\n",
       " 'J10-3003',\n",
       " 'W03-0425',\n",
       " 'W02-1503',\n",
       " 'W04-3201',\n",
       " 'P98-2204',\n",
       " 'D08-1083',\n",
       " 'W01-0521',\n",
       " 'P06-1091',\n",
       " 'P88-1015',\n",
       " 'D09-1101',\n",
       " 'J98-1001',\n",
       " 'J90-1004',\n",
       " 'J10-4006',\n",
       " 'P02-1001',\n",
       " 'P00-1058',\n",
       " 'H93-1061',\n",
       " 'H05-1044',\n",
       " 'P05-1022',\n",
       " 'P08-1076',\n",
       " 'P94-1002',\n",
       " 'D11-1006',\n",
       " 'A00-2034',\n",
       " 'J08-1002',\n",
       " 'P08-1115',\n",
       " 'P09-1104',\n",
       " 'C02-1139',\n",
       " 'P98-1112',\n",
       " 'N06-2013',\n",
       " 'D08-1059',\n",
       " 'J97-3003',\n",
       " 'P09-1039',\n",
       " 'J09-3003',\n",
       " 'P06-1066',\n",
       " 'J82-3004',\n",
       " 'N03-1022',\n",
       " 'P08-1012',\n",
       " 'J98-3005',\n",
       " 'W04-3207',\n",
       " 'E03-1076',\n",
       " 'W04-2401',\n",
       " 'P00-1010',\n",
       " 'C02-2025',\n",
       " 'P98-2177',\n",
       " 'A97-1039',\n",
       " 'D07-1074',\n",
       " 'D07-1111',\n",
       " 'N03-1026',\n",
       " 'P09-1057',\n",
       " 'J08-4003',\n",
       " 'P07-1007',\n",
       " 'D07-1076',\n",
       " 'P00-1027',\n",
       " 'P06-1009',\n",
       " 'P85-1011',\n",
       " 'C94-1032',\n",
       " 'D07-1103',\n",
       " 'D11-1141',\n",
       " 'D11-1142',\n",
       " 'P11-1019',\n",
       " 'I05-3025',\n",
       " 'P05-1067',\n",
       " 'P85-1008',\n",
       " 'P05-1059',\n",
       " 'A00-1043',\n",
       " 'D08-1027',\n",
       " 'D12-1050',\n",
       " 'J94-2001',\n",
       " 'P06-1014',\n",
       " 'C96-1021',\n",
       " 'J98-2001',\n",
       " 'W04-3247',\n",
       " 'W03-1719',\n",
       " 'C92-3126',\n",
       " 'W02-1006',\n",
       " 'J95-4004',\n",
       " 'A94-1006',\n",
       " 'W04-1221',\n",
       " 'C08-1107',\n",
       " 'J93-2003',\n",
       " 'W01-0511',\n",
       " 'P03-1022',\n",
       " 'P96-1008',\n",
       " 'D09-1058',\n",
       " 'P04-1005',\n",
       " 'P95-1034',\n",
       " 'J01-3001',\n",
       " 'P83-1021',\n",
       " 'P98-1069',\n",
       " 'J06-3003',\n",
       " 'D08-1014',\n",
       " 'W01-1605',\n",
       " 'H01-1035',\n",
       " 'J99-4004',\n",
       " 'J94-4001',\n",
       " 'H05-1059',\n",
       " 'A97-1030',\n",
       " 'J94-3001',\n",
       " 'J04-4002',\n",
       " 'E06-1038',\n",
       " 'P06-4020',\n",
       " 'P11-1055',\n",
       " 'P08-1088',\n",
       " 'E99-1001',\n",
       " 'P07-2045',\n",
       " 'P86-1004',\n",
       " 'H05-1079',\n",
       " 'C10-1152',\n",
       " 'J99-3001',\n",
       " 'J99-4005',\n",
       " 'P97-1041',\n",
       " 'N06-2033',\n",
       " 'P98-2173',\n",
       " 'P84-1008',\n",
       " 'P09-1116',\n",
       " 'P93-1023',\n",
       " 'P02-1053',\n",
       " 'W04-3230',\n",
       " 'W02-0603',\n",
       " 'P03-1021',\n",
       " 'N07-1011',\n",
       " 'J05-3002',\n",
       " 'D10-1119',\n",
       " 'E06-1011',\n",
       " 'J08-2005',\n",
       " 'D07-1061',\n",
       " 'P09-1040',\n",
       " 'C08-1109',\n",
       " 'N09-1003',\n",
       " 'W00-0730',\n",
       " 'P05-1071',\n",
       " 'H94-1020',\n",
       " 'C90-3045',\n",
       " 'P06-1115',\n",
       " 'P09-2012',\n",
       " 'P03-1044',\n",
       " 'J05-4003',\n",
       " 'C02-1054',\n",
       " 'C04-1100',\n",
       " 'C02-1145',\n",
       " 'C04-1024',\n",
       " 'P93-1035',\n",
       " 'P06-1085',\n",
       " 'P06-2006',\n",
       " 'N07-4013',\n",
       " 'P07-1091',\n",
       " 'P98-2127',\n",
       " 'P96-1011',\n",
       " 'P06-1004',\n",
       " 'N10-1119',\n",
       " 'P11-1038',\n",
       " 'P03-1001',\n",
       " 'E06-1027',\n",
       " 'D08-1024',\n",
       " 'J03-1002',\n",
       " 'C92-1038',\n",
       " 'N09-1012',\n",
       " 'N03-2021',\n",
       " 'J88-1003',\n",
       " 'W03-1014',\n",
       " 'C96-1079',\n",
       " 'P08-1023',\n",
       " 'J95-2003',\n",
       " 'P92-1017',\n",
       " 'W02-1021',\n",
       " 'E89-1037',\n",
       " 'P98-1106',\n",
       " 'J93-1006',\n",
       " 'N07-1029',\n",
       " 'P93-1016',\n",
       " 'P06-1077',\n",
       " 'W02-1502',\n",
       " 'W02-0301',\n",
       " 'P09-1058',\n",
       " 'P10-4002',\n",
       " 'H05-2018',\n",
       " 'P01-1067',\n",
       " 'P04-1043',\n",
       " 'W04-0308',\n",
       " 'N04-4026',\n",
       " 'N13-1090',\n",
       " 'P99-1048',\n",
       " 'N03-1033',\n",
       " 'P09-1074',\n",
       " 'P04-1053',\n",
       " 'W02-1011',\n",
       " 'D11-1033',\n",
       " 'P98-2180',\n",
       " 'P06-2094',\n",
       " 'J07-2003',\n",
       " 'J91-4003',\n",
       " 'W04-3212',\n",
       " 'D08-1076',\n",
       " 'J88-2003',\n",
       " 'P99-1065',\n",
       " 'P05-1077',\n",
       " 'P02-1047',\n",
       " 'A97-1011',\n",
       " 'D09-1030',\n",
       " 'W01-0514',\n",
       " 'D07-1096',\n",
       " 'P06-3002',\n",
       " 'W03-0404',\n",
       " 'P04-1066',\n",
       " 'C90-3052',\n",
       " 'D10-1125',\n",
       " 'P02-1042',\n",
       " 'P06-2101',\n",
       " 'N03-1028',\n",
       " 'C00-2163',\n",
       " 'J96-2004',\n",
       " 'P07-1036',\n",
       " 'P04-1077',\n",
       " 'C92-1025',\n",
       " 'P08-1067',\n",
       " 'P11-2033',\n",
       " 'J98-1006',\n",
       " 'P09-1094',\n",
       " 'P84-1085',\n",
       " 'N04-1043',\n",
       " 'P05-1052',\n",
       " 'P91-1022',\n",
       " 'P89-1002',\n",
       " 'C88-2147',\n",
       " 'P05-1020',\n",
       " 'P05-1047',\n",
       " 'W03-0430',\n",
       " 'P97-1003',\n",
       " 'P94-1013',\n",
       " 'J02-1003',\n",
       " 'P03-1069',\n",
       " 'P06-1010',\n",
       " 'J93-3003',\n",
       " 'P00-1041',\n",
       " 'C08-1018',\n",
       " 'P09-1113',\n",
       " 'P05-1065',\n",
       " 'P07-1106',\n",
       " 'P95-1026',\n",
       " 'C04-1072',\n",
       " 'J87-1005',\n",
       " 'C92-2066',\n",
       " 'C88-2128',\n",
       " 'J93-2004',\n",
       " 'P90-1032',\n",
       " 'N04-1035',\n",
       " 'N01-1026',\n",
       " 'D10-1115',\n",
       " 'D07-1077',\n",
       " 'P11-1098',\n",
       " 'P08-1085',\n",
       " 'P07-1055',\n",
       " 'N04-1042',\n",
       " 'J08-4004',\n",
       " 'C04-1059',\n",
       " 'N06-1056',\n",
       " 'P98-1029',\n",
       " 'C04-1046',\n",
       " 'W00-1201',\n",
       " 'P06-1032',\n",
       " 'J93-1004',\n",
       " 'W00-0712',\n",
       " 'P05-1044',\n",
       " 'D10-1124',\n",
       " 'P91-1030',\n",
       " 'P08-1064',\n",
       " 'P06-1038',\n",
       " 'J90-2002',\n",
       " 'P06-1084',\n",
       " 'W03-1006',\n",
       " 'P86-1031',\n",
       " 'C04-1180',\n",
       " 'P10-1044',\n",
       " 'W03-1809',\n",
       " 'C88-1016',\n",
       " 'C04-1200',\n",
       " 'E06-1005',\n",
       " 'E06-1025',\n",
       " 'J04-1005',\n",
       " 'P06-1101',\n",
       " 'P04-3022',\n",
       " 'C00-1007',\n",
       " 'P04-1083',\n",
       " 'P99-1016',\n",
       " 'D11-1062',\n",
       " 'P06-1123',\n",
       " 'D09-1086',\n",
       " 'H05-1021',\n",
       " 'P10-1040',\n",
       " 'N07-1071',\n",
       " 'N04-1025',\n",
       " 'C96-1005',\n",
       " 'P02-1051',\n",
       " 'A00-2024',\n",
       " 'H05-1053',\n",
       " 'P06-1121',\n",
       " 'P10-1052',\n",
       " 'P90-1010',\n",
       " 'P97-1017',\n",
       " 'P06-1109',\n",
       " 'D08-1021',\n",
       " 'N09-1041',\n",
       " 'J93-1005',\n",
       " 'P06-2066',\n",
       " 'P97-1013',\n",
       " 'J94-4004',\n",
       " 'P96-1041',\n",
       " 'A92-1006',\n",
       " 'W04-3111',\n",
       " 'D09-1127',\n",
       " 'W00-1303',\n",
       " 'N04-1041',\n",
       " 'N01-1016',\n",
       " 'J93-2002',\n",
       " 'N10-1019',\n",
       " 'C10-1011',\n",
       " 'P04-1056',\n",
       " 'C96-2141',\n",
       " 'J04-1002',\n",
       " 'P07-1030',\n",
       " 'J00-1004',\n",
       " 'J08-1001',\n",
       " 'P07-1003',\n",
       " 'D08-1036',\n",
       " 'P03-1058',\n",
       " 'N03-1030',\n",
       " 'P99-1004',\n",
       " 'P10-1142',\n",
       " 'C96-1058',\n",
       " 'W02-2016',\n",
       " 'P93-1003',\n",
       " 'W03-0424',\n",
       " 'J97-4005',\n",
       " 'E09-1013',\n",
       " 'C02-1150',\n",
       " 'P07-1092',\n",
       " 'E03-1071',\n",
       " 'C08-1114',\n",
       " 'C10-2005',\n",
       " 'P93-1002',\n",
       " 'W03-1008',\n",
       " 'N04-1023',\n",
       " 'P08-2026',\n",
       " 'P06-1134',\n",
       " 'J98-4003',\n",
       " 'P09-1011',\n",
       " 'P08-1068',\n",
       " 'P07-1098',\n",
       " 'C04-1081',\n",
       " 'N01-1006',\n",
       " 'P03-2041',\n",
       " 'W00-1401',\n",
       " 'W03-1011',\n",
       " 'P06-1104',\n",
       " 'N13-1039',\n",
       " 'N12-1067',\n",
       " 'P12-1092',\n",
       " 'C94-1079',\n",
       " 'N07-1038',\n",
       " 'N10-1020',\n",
       " 'J03-3001',\n",
       " 'J98-2002',\n",
       " 'P06-1095',\n",
       " 'D07-1114',\n",
       " 'D08-1068',\n",
       " 'W01-1313',\n",
       " 'N04-1022',\n",
       " 'D08-1089',\n",
       " 'P03-1029',\n",
       " 'P04-1015',\n",
       " 'P83-1007',\n",
       " 'P94-1019',\n",
       " 'P88-1012',\n",
       " 'P09-1019',\n",
       " 'W04-0803',\n",
       " 'P10-1110',\n",
       " 'P10-1146',\n",
       " 'P99-1008',\n",
       " 'N04-1030',\n",
       " 'A97-1004',\n",
       " 'P05-1017',\n",
       " 'P08-1119',\n",
       " 'J03-4003',\n",
       " 'D09-1005',\n",
       " 'P08-1109',\n",
       " 'N03-1020',\n",
       " 'E03-1009',\n",
       " 'P06-1015',\n",
       " 'N10-1115',\n",
       " 'J88-2006',\n",
       " 'P91-1017',\n",
       " 'C02-1144',\n",
       " 'N06-1041',\n",
       " 'N01-1025',\n",
       " 'P06-1011',\n",
       " 'A88-1019',\n",
       " 'W04-2406',\n",
       " 'N10-1063',\n",
       " 'L08-1093',\n",
       " 'D07-1091',\n",
       " 'D07-1003',\n",
       " 'P93-1022',\n",
       " 'D07-1097',\n",
       " 'P02-1018',\n",
       " 'P97-1023',\n",
       " 'W01-0513',\n",
       " 'N06-1003',\n",
       " 'D10-1120',\n",
       " 'D07-1080',\n",
       " 'P05-1015',\n",
       " 'N07-1051',\n",
       " 'P03-1051',\n",
       " 'P92-1032',\n",
       " 'W02-1028',\n",
       " 'P93-1032',\n",
       " 'H92-1045',\n",
       " 'E09-1005',\n",
       " 'D09-1026',\n",
       " 'P08-1090',\n",
       " 'E06-1043',\n",
       " 'P03-1010',\n",
       " 'H93-1052',\n",
       " 'J95-2002',\n",
       " 'W04-3205',\n",
       " 'D09-1120',\n",
       " 'E89-1009',\n",
       " 'P05-1012',\n",
       " 'N04-1001',\n",
       " 'D08-1020',\n",
       " 'A00-2004',\n",
       " 'P09-1026',\n",
       " 'P01-1017',\n",
       " 'P07-1125',\n",
       " 'P02-1031',\n",
       " 'W02-2026',\n",
       " 'N04-1033',\n",
       " 'P06-1072',\n",
       " 'D09-1098',\n",
       " 'J97-1005',\n",
       " 'P07-1123',\n",
       " 'C88-2121',\n",
       " 'J04-3002',\n",
       " 'H91-1060',\n",
       " 'P04-1035',\n",
       " 'N04-1019',\n",
       " 'P07-1096',\n",
       " 'D07-1031',\n",
       " 'H05-1011',\n",
       " 'D11-1129',\n",
       " 'P99-1042',\n",
       " 'C04-1073',\n",
       " 'C04-1146',\n",
       " 'P05-1073',\n",
       " 'P02-1050',\n",
       " 'P03-1002',\n",
       " 'W02-0109',\n",
       " 'N10-1013',\n",
       " 'P04-1014',\n",
       " 'C00-1072',\n",
       " 'P96-1042',\n",
       " 'P10-2041',\n",
       " 'P83-1019',\n",
       " 'P11-2008',\n",
       " 'J08-2002',\n",
       " 'A92-1018',\n",
       " 'C92-1019',\n",
       " 'H05-1045',\n",
       " 'P03-1004',\n",
       " 'N07-1047',\n",
       " 'J97-1003',\n",
       " 'J07-4004',\n",
       " 'P99-1014',\n",
       " 'P95-1050',\n",
       " 'A92-1021',\n",
       " 'W02-1210',\n",
       " 'W04-2407',\n",
       " 'A97-1029',\n",
       " 'C04-1041',\n",
       " 'J02-4002',\n",
       " 'J92-1001',\n",
       " 'P02-1022',\n",
       " 'P08-1114',\n",
       " 'P04-1054',\n",
       " 'E06-1002',\n",
       " 'D07-1071',\n",
       " 'J00-2004',\n",
       " 'W02-0817',\n",
       " 'P89-1031',\n",
       " 'J97-2003',\n",
       " 'D08-1011',\n",
       " 'P00-1056']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"./From-ScisummNet-2019\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac0e9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = int(len(files)*0.7)\n",
    "val_end = int(len(files)*0.1)+train_end\n",
    "train_docs = files[0:train_end]\n",
    "val_docs = files[train_end:val_end]\n",
    "test_docs = files[val_end:len(files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e713843c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f P83-1020\n",
      "f W02-1039\n",
      "f P96-1021\n",
      "f N06-1020\n",
      "f P03-1069\n",
      "f P06-1010\n",
      "f J93-3003\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example_sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(example_sent.lower())\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w.isalpha()]\n",
    "#     print(filtered_sentence)\n",
    "    new = \" \" \n",
    "    a = new.join(filtered_sentence)\n",
    "    return a\n",
    "\n",
    "    \n",
    "def get_dataset(files):\n",
    "    data_lis = []\n",
    "    label_lis = []\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    new_corpus_prev = None\n",
    "    for z,f in enumerate(files):\n",
    "        print(\"f\",f)\n",
    "        try:\n",
    "            citants = pd.read_json(\"From-ScisummNet-2019/\"+str(f)+\"/citing_sentences.json\")\n",
    "            citants = citants[['citance_No','clean_text']]\n",
    "            queries = list(citants['clean_text'])\n",
    "            cite_no = list(citants.citance_No)\n",
    "            tree = ET.parse(\"From-ScisummNet-2019/\"+f+\"/Reference_XML/\"+f+\".xml\")\n",
    "            root = tree.getroot()\n",
    "            final1=[]\n",
    "            final2=[]\n",
    "            i = 0\n",
    "            total = len(root)\n",
    "            for a in root:\n",
    "                for b in a:\n",
    "                    final1.append(b.text)\n",
    "                    if i == 0:\n",
    "                        final2.append(\"Abstract\")\n",
    "                    if i == 1:\n",
    "                        final2.append(\"Introduction\")\n",
    "                    elif i < total-2:\n",
    "                        final2.append(\"Experiment/Discussion\")\n",
    "                    if i == total-2 or i == total-1:\n",
    "                        final2.append(\"Results/Conclusion\")\n",
    "                    if i == total:\n",
    "                        final2.append(\"Acknowledgment\")\n",
    "                i = i+1\n",
    "\n",
    "            d={'col1':final1,'col2':final2}\n",
    "\n",
    "            rp = pd.DataFrame(data=d)\n",
    "            data_lis.extend(list(rp.col1))\n",
    "            label_lis.extend(list(rp.col2))\n",
    "            \n",
    "        except Exception as e: print(e)\n",
    "    return data_lis, label_lis\n",
    "            \n",
    "    \n",
    "train_data, train_labels = get_dataset(train_docs[:4])\n",
    "val_data,val_labels = get_dataset(val_docs[:3])\n",
    "# test_data = get_dataset(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb89272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "val_labels = le.transform(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c06cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmpsmxhsa9h\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52fd4b604204cc2ad443123f92fe750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /home/avani.gupta/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmpxexut4wl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556c758e2d441acbcd060befaa45826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /home/avani.gupta/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmp2n5i_b40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148296ed66f4417d82f4251bbb2f0995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /home/avani.gupta/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/avani.gupta/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/avani.gupta/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmpuxgq0v57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e2ddd17de748a9b2fe1efde39fc220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa2269c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_data,return_tensors=\"pt\",max_length=512,padding='max_length')\n",
    "val_encodings = tokenizer(val_data,return_tensors=\"pt\",max_length=512,padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6961aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# train_encodings = tokenizer(train_data,return_tensors=\"pt\",padding=True)\n",
    "# val_encodings = tokenizer(val_data,return_tensors=\"pt\",padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4afcf6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, torch.from_numpy(train_labels))\n",
    "val_dataset = IMDbDataset(val_encodings, torch.from_numpy(val_labels))\n",
    "# test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9c3fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b12090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(train_labels), max(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8be55f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/avani.gupta/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/avani.gupta/.cache/huggingface/transformers/tmp9wvt3upx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7829bdab064e9cafdc26c16c9dc8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/avani.gupta/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 794\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2382\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2382' max='2382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2382/2382 06:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2382, training_loss=1.2230342989904754, metrics={'train_runtime': 360.9461, 'train_samples_per_second': 6.599, 'train_steps_per_second': 6.599, 'total_flos': 626736161028096.0, 'train_loss': 1.2230342989904754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1500,\n",
    ")\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b26bde54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 1\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='832' max='832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [832/832 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4331277012825012,\n",
       " 'eval_runtime': 22.2682,\n",
       " 'eval_samples_per_second': 37.363,\n",
       " 'eval_steps_per_second': 37.363,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5253896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d48f7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"task1b_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b27cea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/avani.gupta/.cache/huggingface/modules/datasets_modules/metrics/accuracy/3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Mon Nov 29 14:50:52 2021) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9158653846153846}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "acc = load_metric(\"accuracy\")\n",
    "\n",
    "# f1_score = load_metric(\"f1\")\n",
    "model.eval()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
