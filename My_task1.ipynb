{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b17845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ecf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:56:00.983286: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:/usr/local/apps/cuDNN/7.6.5-cuda-10.2/lib64\n",
      "2021-11-27 12:56:00.983325: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ebf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1971d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e713843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "citants = pd.read_json('From-ScisummNet-2019/A00-1031/citing_sentences.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "citants = citants[['citance_No','clean_text']]\n",
    "citants\n",
    "queries = list(citants['clean_text'])\n",
    "# cit_no = list(citants['citance_No'])\n",
    "# query_sent = citants['raw_text']\n",
    "# query_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262db9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_no = list(citants.citance_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7af5c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "tree = ET.parse(\"From-ScisummNet-2019/A00-1031\"+\"/Reference_XML/\"+\"A00-1031\"+\".xml\")\n",
    "root = tree.getroot()\n",
    "final1=[]\n",
    "final2=[]\n",
    "i = 0\n",
    "total = len(root)\n",
    "# print(total)\n",
    "for a in root:\n",
    "  #  print(a.attrib)\n",
    "   # print(i)\n",
    "    for b in a:\n",
    "\n",
    "        final1.append(b.text)\n",
    "        if i == 0:\n",
    "            final2.append(\"Abstract\")\n",
    "        if i == 1:\n",
    "            final2.append(\"Introduction\")\n",
    "        elif i < total-2:\n",
    "            final2.append(\"Experiment/Discussion\")\n",
    "        if i == total-2 or i == total-1:\n",
    "            final2.append(\"Results/Conclusion\")\n",
    "        if i == total:\n",
    "            final2.append(\"Acknowledgment\")\n",
    "    i = i+1\n",
    "\n",
    "\n",
    "d={'col1':final1,'col2':final2}\n",
    "rp = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a853b495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   col1                col2\n",
       "0    Trigrams'n'Tags (TnT) is an efficient statisti...        Introduction\n",
       "1    Contrary to claims found elsewhere in the lite...        Introduction\n",
       "2    A recent comparison has even shown that TnT pe...        Introduction\n",
       "3    We describe the basic model of TnT, the techni...        Introduction\n",
       "4    Furthermore, we present evaluations on two cor...        Introduction\n",
       "..                                                 ...                 ...\n",
       "173  Many thanks go to Hans Uszkoreit for his suppo...  Results/Conclusion\n",
       "174  Most of the work on TnT was carried out while ...  Results/Conclusion\n",
       "175  Large annotated corpora are the pre-requisite ...  Results/Conclusion\n",
       "176  Therefore, I would like to thank all the peopl...  Results/Conclusion\n",
       "177  And, last but not least, I would like to thank...  Results/Conclusion\n",
       "\n",
       "[178 rows x 2 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5daffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Trigrams'n'Tags (TnT) is an efficient statisti...\n",
       "1      Contrary to claims found elsewhere in the lite...\n",
       "2      A recent comparison has even shown that TnT pe...\n",
       "3      We describe the basic model of TnT, the techni...\n",
       "4      Furthermore, we present evaluations on two cor...\n",
       "                             ...                        \n",
       "173    Many thanks go to Hans Uszkoreit for his suppo...\n",
       "174    Most of the work on TnT was carried out while ...\n",
       "175    Large annotated corpora are the pre-requisite ...\n",
       "176    Therefore, I would like to thank all the peopl...\n",
       "177    And, last but not least, I would like to thank...\n",
       "Name: col1, Length: 178, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c0fce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = \"From-ScisummNet-2019/A00-1031/annotation/A00-1031.ann.txt\"\n",
    "with open(ann,\"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f135ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = {'Citance Number':0,'Reference Article':1,'Citing Article':2,'Citation Marker Offset':3,'Citation Marker':4,'Citation Offset':5,'Reference Offset':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73438a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = {}\n",
    "lines = data.split('\\n')\n",
    "for line in lines:\n",
    "    lis = line.split('|')\n",
    "#     print(lis, len(lis))\n",
    "    if len(lis)==11:\n",
    "            cit_no = lis[0].split(':')[1]\n",
    "            ref_off = lis[7].split(':')[1].strip()\n",
    "            exec(\"ref_off=\"+ref_off)\n",
    "            ref_off = list(map(int, ref_off))\n",
    "            gt[int(cit_no)] = ref_off #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393e4df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0, 36],\n",
       " 2: [44, 178],\n",
       " 3: [15, 173],\n",
       " 4: [0, 1],\n",
       " 5: [27, 165],\n",
       " 6: [27, 141],\n",
       " 7: [115, 154],\n",
       " 8: [24, 165],\n",
       " 9: [36, 46],\n",
       " 10: [2, 75],\n",
       " 11: [0, 1],\n",
       " 12: [18, 40],\n",
       " 13: [36, 118],\n",
       " 14: [24, 173],\n",
       " 15: [79, 80],\n",
       " 16: [0, 113],\n",
       " 17: [12, 19],\n",
       " 18: [0, 1],\n",
       " 19: [28, 157],\n",
       " 20: [36, 164]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba3c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(line):\n",
    "#     line = line.split(\"|\")\n",
    "#     cit_2 , ref_2 = line[6].split(\"Citation Text:\")[1].strip() , line[8].split(\"Reference Text:\")[1].strip()\n",
    "#     cit_2 = cit_2.split(\" >\")[1].split(\"</S>\")[0]\n",
    "#     ref_2 = ref_2.split(\" >\")[1:]\n",
    "#     ref_2 = [ i.split(\"</S>\")[0] for i in ref_2]\n",
    "#     ans=\"\"\n",
    "#     for i in ref_2:\n",
    "#         ans=ans + cit_2 + \" $$$$$ \" + i +\"\\n\"\n",
    "#    # ref_2 = \" \".join(ref_2)\n",
    "    \n",
    "#     return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a452d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./From-ScisummNet-2019/A00-1031/annotation/A00-1031.ann.txt') as f:\n",
    "#     data = f.read()\n",
    "# data = [ clean(i) for i in data.split(\"\\n\") if i ]\n",
    "\n",
    "# # with open(\"cleaned.txt\",\"w+\") as f:\n",
    "# #     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ddd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rp.col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9fa591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4bbd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59dd9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_sentences(model):\n",
    "    # Get a vector for each headline (sentence) in the corpus\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "    # Define search queries and embed them to vectors as well\n",
    "\n",
    "    query_embeddings = model.encode(queries)\n",
    "    # For each search term return 5 closest sentences\n",
    "    closest_n = 5\n",
    "\n",
    "    for i in range(len(queries)):\n",
    "        query, query_embedding  = queries[i], query_embeddings[i]\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "        results = zip(range(len(distances)), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "    #     results = dict(enumerate(distances,1))\n",
    "    #     results = dict(sorted(results.items(), key=lambda item: item[1]))\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query)\n",
    "        print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "        indexes = results[0:closest_n]\n",
    "        top_n = []\n",
    "        for l,k in indexes:\n",
    "            top_n.append(l)\n",
    "        print(np.intersect1d(top_n,gt[cite_no[i]]))\n",
    "        print(\"indexes{}, top_n{}, i{}, cite_no[i]{} ,gt[cite_no[i]]{}\".format(indexes, top_n,i,cite_no[i],gt[cite_no[i]]))\n",
    "\n",
    "    #     for idx, distance in results[0:closest_n]:\n",
    "    #         print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be5fd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# #Define your train examples. You need more than just two examples...\n",
    "\n",
    "# train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "#     InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "# #Define your train dataset, the dataloader and the train loss\n",
    "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# #Tune the model\n",
    "# model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d024b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for i in range(len(queries)):\n",
    "#     for j in gt[cite_no[i]]:\n",
    "#         if j < len(corpus):\n",
    "# #             print(queries[i],gt[cite_no[i]],corpus[j])\n",
    "#             data.append(InputExample(texts=[queries[i],corpus[j]],label=0.8))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daf27f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define your train dataset, the dataloader and the train loss\n",
    "# train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# #Tune the model\n",
    "# model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10000, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e643bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The sentences in the DSO collection were tagged with parts of speech using TnT (Brants, 2000) trained on the Brown Corpus itself.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(16, 0.27199719780161213), (3, 0.2751179495764202), (26, 0.3061739845249595), (33, 0.32632695556253266), (174, 0.33747301984757716)], top_n[16, 3, 26, 33, 174], i0, cite_no[i]1 ,gt[cite_no[i]][0, 36]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: \n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(34, 0.20756660002426197), (70, 0.28087194268629), (65, 0.3711071328983985), (57, 0.38959269651066464), (61, 0.4191898053080748)], top_n[34, 70, 65, 57, 61], i1, cite_no[i]2 ,gt[cite_no[i]][44, 178]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The English POS-tagging has been carried out using freely available TNT tagger (Brants, 2000).\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(3, 0.31457500053583565), (16, 0.3492840588830657), (172, 0.37751118188939703), (0, 0.381270112530814), (26, 0.4047158733168057)], top_n[3, 16, 172, 0, 26], i2, cite_no[i]3 ,gt[cite_no[i]][15, 173]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: This proposition is quite viable as statistical POS taggers like TnT (Brants, 2000) are available.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[0]\n",
      "indexes[(0, 0.20597648302860694), (2, 0.259223794982938), (173, 0.283247963603659), (12, 0.3208004324772019), (44, 0.3216908061743433)], top_n[0, 2, 173, 12, 44], i3, cite_no[i]4 ,gt[cite_no[i]][0, 1]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We use TnT (Brants, 2000), a second order Markov Model tagger.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(26, 0.11117472581860877), (3, 0.26697471076774126), (16, 0.30514685913964623), (68, 0.3683256789944094), (33, 0.3751609540184164)], top_n[26, 3, 16, 68, 33], i4, cite_no[i]5 ,gt[cite_no[i]][27, 165]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: For PoS tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant to kenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default models trained on financial news from the Penn Tree bank.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(26, 0.24676326816331207), (23, 0.3059138638370166), (3, 0.31838468052675406), (174, 0.32037157507009184), (16, 0.32188404909691326)], top_n[26, 23, 3, 174, 16], i5, cite_no[i]6 ,gt[cite_no[i]][27, 141]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000).\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(6, 0.24614148279469072), (23, 0.26084521377442416), (158, 0.2614154946347327), (25, 0.30745305244541565), (33, 0.33029828073168077)], top_n[6, 23, 158, 25, 33], i6, cite_no[i]7 ,gt[cite_no[i]][115, 154]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: For example, Petrov et al (2012) build supervised POS taggers for 22 languages using the TNT tagger (Brants, 2000), with an average accuracy of 95.2%.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(94, 0.22604410387277218), (115, 0.2930293473593837), (62, 0.29947053242971977), (151, 0.3234411966329508), (150, 0.3604471978939141)], top_n[94, 115, 62, 151, 150], i7, cite_no[i]8 ,gt[cite_no[i]][24, 165]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Forun aligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates. In Step 6 we build a tagger by feeding the es ti mated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(74, 0.3569085154642472), (141, 0.35791415366717516), (90, 0.37896651636024703), (132, 0.4178370899704805), (0, 0.41986015904322205)], top_n[74, 141, 90, 132, 0], i8, cite_no[i]9 ,gt[cite_no[i]][36, 46]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(7, 0.36509781203273695), (60, 0.3791145089251601), (136, 0.390371887581331), (175, 0.39288869532583337), (6, 0.3953834620887162)], top_n[7, 60, 136, 175, 6], i9, cite_no[i]10 ,gt[cite_no[i]][2, 75]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We also incorporated part-of speech tagging, using the TnT tagger (Brants, 2000) retrained on the GENIA corpus gold standard part of-speech tagging.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[0]\n",
      "indexes[(33, 0.28025529230198176), (26, 0.309438153263225), (3, 0.31226343405992063), (0, 0.32833646931084215), (16, 0.34103272816891494)], top_n[33, 26, 3, 0, 16], i10, cite_no[i]11 ,gt[cite_no[i]][0, 1]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: POS Majority lexical type noun count-noun-le c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000).\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(68, 0.28001304821842454), (15, 0.3217170004949388), (136, 0.32912215879210416), (30, 0.33247758295580165), (23, 0.3354748660769834)], top_n[68, 15, 136, 30, 23], i11, cite_no[i]12 ,gt[cite_no[i]][18, 40]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The texts were POS-tagged using TnT (Brants,2000).\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(16, 0.24141975724580889), (3, 0.2547750787304337), (26, 0.3176923261961939), (33, 0.3329464949994383), (22, 0.3573516149079605)], top_n[16, 3, 26, 33, 22], i12, cite_no[i]13 ,gt[cite_no[i]][36, 118]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The freely-available POS lexicon from Sharoff et al (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(62, 0.30124004010825545), (94, 0.30807407663889785), (115, 0.32714458764671017), (107, 0.37131081037633107), (136, 0.38459917049335746)], top_n[62, 94, 115, 107, 136], i13, cite_no[i]14 ,gt[cite_no[i]][24, 173]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We use a corpus of 5 million words automatically tagged by TnT (Brants, 2000) and freely available online (Sharoff et al, 2008). Because we want to make linguistically-informed corruptions, we corrupt only the words we have information for, identifying the words in the corpus which are found in the lexicon with the appropriate POS tag. We also select only words which have inflectional morphology: nouns, verbs, adjectives, pronouns, and numerals.7 4.2.1 Determining word properties (step 1) We use the POS tag to restrict the properties of a word, regardless of how exactly we corrupt it.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(94, 0.3168022786142255), (104, 0.36213451938241514), (115, 0.36399683754940404), (99, 0.3901020560746752), (68, 0.4119662891317285)], top_n[94, 104, 115, 99, 68], i14, cite_no[i]15 ,gt[cite_no[i]][79, 80]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: To POS tag, we use the HMM tagger TnT (Brants, 2000) with the model from http: //corpus.leeds.ac.uk/mocky/.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(16, 0.30219309888223134), (26, 0.31171576803086654), (23, 0.3266815236553179), (3, 0.3276814340382378), (33, 0.3715378508074261)], top_n[16, 26, 23, 3, 33], i15, cite_no[i]16 ,gt[cite_no[i]][0, 113]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: After finishing the corrections, we experimented with training and testing the TnT tagger (Brants,2000) on the& quot; old& quot; and on the& quot; corrected& quot; version of NEGRA?.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(15, 0.32658059309631915), (3, 0.3285726293568072), (177, 0.33101261189434583), (16, 0.3335905959486619), (26, 0.35644386234027736)], top_n[15, 3, 177, 16, 26], i16, cite_no[i]17 ,gt[cite_no[i]][12, 19]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(175, 0.2295675097442118), (157, 0.28361698180710637), (6, 0.2888474398910228), (22, 0.29396252072994566), (3, 0.302711548449362)], top_n[175, 157, 6, 22, 3], i17, cite_no[i]18 ,gt[cite_no[i]][0, 1]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: POS tags, on the other, represent more of a challenge with only 91.6% NORM LEMMA POS Agreed tokens (out of 57,845) 56,052 55,217 52,959 Accuracy (%) 96.9% 95.5% 91.6% Table 3: Inter-annotator agreement agreement between two annotators, which is cons id erably lower than the agreement level reported for annotating a corpus of modern German using STTS, at 98.6% (Brants, 2000a).\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(154, 0.27581062479431817), (164, 0.2934563118785669), (150, 0.33854472590095963), (10, 0.34706472661886467), (156, 0.35197988196011665)], top_n[154, 164, 150, 10, 156], i18, cite_no[i]19 ,gt[cite_no[i]][28, 157]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: We further plan to retrain state-of the-art POS taggers such as the TreeTagger and TnT Tagger (Brants, 2000b) on our data. Finally, we plan to investigate how linguistic annotations can be automatically integrated in the TEI annotated version of the corpus to produce TEI con formant output.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(175, 0.2635992057982871), (6, 0.297660901984126), (15, 0.3074534593592093), (162, 0.3203113210006743), (7, 0.33314788668527684)], top_n[175, 6, 15, 162, 7], i19, cite_no[i]20 ,gt[cite_no[i]][36, 164]\n"
     ]
    }
   ],
   "source": [
    "get_matching_sentences(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcadbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "271ad995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1743e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "012c8c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from pytorch-ignite) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from torch<2,>=1.3->pytorch-ignite) (3.10.0.2)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bda5a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: The sentences in the DSO collection were tagged with parts of speech using TnT (Brants, 2000) trained on the Brown Corpus itself.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "[]\n",
      "indexes[(16, 0.27199719780161213), (3, 0.2751179495764202), (26, 0.3061739845249595), (33, 0.32632695556253266), (174, 0.33747301984757716)], top_n[16, 3, 26, 33, 174], i0, cite_no[i]1 ,gt[cite_no[i]][0, 36]\n",
      "{'rouge1': Score(precision=0.22727272727272727, recall=0.35714285714285715, fmeasure=0.2777777777777778), 'rougeL': Score(precision=0.18181818181818182, recall=0.2857142857142857, fmeasure=0.2222222222222222)}\n",
      "This paper describes the models and techniques used by TnT together with the implementation. (Score: 0.7280)\n",
      "{'rouge1': Score(precision=0.22727272727272727, recall=0.29411764705882354, fmeasure=0.25641025641025644), 'rougeL': Score(precision=0.18181818181818182, recall=0.23529411764705882, fmeasure=0.20512820512820512)}\n",
      "We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. (Score: 0.7249)\n",
      "{'rouge1': Score(precision=0.18181818181818182, recall=0.4, fmeasure=0.25000000000000006), 'rougeL': Score(precision=0.045454545454545456, recall=0.1, fmeasure=0.06250000000000001)}\n",
      "TnT uses second order Markov models for part-ofspeech tagging. (Score: 0.6938)\n",
      "{'rouge1': Score(precision=0.2727272727272727, recall=0.3333333333333333, fmeasure=0.3), 'rougeL': Score(precision=0.22727272727272727, recall=0.2777777777777778, fmeasure=0.25)}\n",
      "If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!? (Score: 0.6737)\n",
      "{'rouge1': Score(precision=0.3181818181818182, recall=0.2916666666666667, fmeasure=0.30434782608695654), 'rougeL': Score(precision=0.18181818181818182, recall=0.16666666666666666, fmeasure=0.17391304347826086)}\n",
      "Most of the work on TnT was carried out while the author received a grant of the Deutsche Forschungsgemeinschaft in the Graduiertenkolleg Kognitionswissenschaft Saarbriicken. (Score: 0.6625)\n"
     ]
    }
   ],
   "source": [
    "# def get_matching_sentences(model):\n",
    "# Get a vector for each headline (sentence) in the corpus\n",
    "# from ignite.metrics import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# m = Rouge(variants=[\"L\", 2], multiref=\"best\")\n",
    "corpus_embeddings = model.encode(corpus)\n",
    "# Define search queries and embed them to vectors as well\n",
    "\n",
    "query_embeddings = model.encode(queries)\n",
    "# For each search term return 5 closest sentences\n",
    "closest_n = 5\n",
    "\n",
    "# for i in range(len(queries)):\n",
    "i = 0\n",
    "query, query_embedding  = queries[i], query_embeddings[i]\n",
    "distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "results = zip(range(len(distances)), distances)\n",
    "results = sorted(results, key=lambda x: x[1])\n",
    "#     results = dict(enumerate(distances,1))\n",
    "#     results = dict(sorted(results.items(), key=lambda item: item[1]))\n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "indexes = results[0:closest_n]\n",
    "top_n = []\n",
    "for l,k in indexes:\n",
    "    top_n.append(l)\n",
    "print(np.intersect1d(top_n,gt[cite_no[i]]))\n",
    "\n",
    "print(\"indexes{}, top_n{}, i{}, cite_no[i]{} ,gt[cite_no[i]]{}\".format(indexes, top_n,i,cite_no[i],gt[cite_no[i]]))\n",
    "\n",
    "for idx, distance in results[0:closest_n]:\n",
    "#     m.update(([query.split()],[corpus[idx].strip().split()]))\n",
    "#     print(m.compute())\n",
    "    scores = scorer.score(corpus[idx].strip(), query)\n",
    "    print(scores)\n",
    "    print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af737f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in os.listdir(\"./From-ScisummNet-2019\"):\n",
    "    try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd94ebf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rouge-L-P': 0.2,\n",
       " 'Rouge-L-R': 0.16666666666666666,\n",
       " 'Rouge-L-F': 0.16666666666666666,\n",
       " 'Rouge-2-P': 0.0,\n",
       " 'Rouge-2-R': 0.0,\n",
       " 'Rouge-2-F': 0.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Rouge(variants=[\"L\", 2], multiref=\"best\")\n",
    "\n",
    "candidate = \"the cat is not there\".split()\n",
    "references = [\n",
    "    \"the dogs are on the mat\".split()\n",
    "]\n",
    "\n",
    "m.update(([candidate], [references]))\n",
    "\n",
    "m.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c896037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: absl-py in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from rouge-score) (1.21.2)\n",
      "Requirement already satisfied: nltk in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from rouge-score) (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from nltk->rouge-score) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: click in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from nltk->rouge-score) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from nltk->rouge-score) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from click->nltk->rouge-score) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/avani.gupta/miniconda3/envs/new/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge-score) (3.6.0)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13028f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a592db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fecbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
